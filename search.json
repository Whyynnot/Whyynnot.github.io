[{"title":"Survey","url":"/2022/07/29/Survey/","content":"<h1 id=\"A-Survey\"><a href=\"#A-Survey\" class=\"headerlink\" title=\"A Survey\"></a>A Survey</h1><blockquote>\n<p>目前的seq2seq类的模型在NLP领域大行其道。但是，它将各种复合句都编码为一个向量来处理。这样会出现组合爆炸问题，即简单句的组合会导致数据量剧增。因此，分析句子的结构，对复合句进行拆分，并将句子的结构知识运用到模型中，有望解决数据获取难等问题。针对该直觉，我进行了相关工作的调研，现将一些调研结果总结如下。<br><span id=\"more\"></span></p>\n</blockquote>\n<h2 id=\"句子的拆分和改写：Sentence-Split-and-Rephrase\"><a href=\"#句子的拆分和改写：Sentence-Split-and-Rephrase\" class=\"headerlink\" title=\"句子的拆分和改写：Sentence Split and Rephrase\"></a>句子的拆分和改写：Sentence Split and Rephrase</h2><ul>\n<li>Fact-Aware Sentence Split and Rephrase with Permutation Invariant Training<ul>\n<li>训练过程中对事实的遗失：Fact-aware Sentence Encoding</li>\n<li>简单句的顺序的影响：PIT(Permutation Invariant Training)</li>\n<li><img src=\"/.com//07/29/Survey/img-20220730153106.png\" class title=\"图片引用方法一\">\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"语义解析-Semantic-Parsing\"><a href=\"#语义解析-Semantic-Parsing\" class=\"headerlink\" title=\"语义解析:Semantic Parsing\"></a>语义解析:Semantic Parsing</h2><ul>\n<li><p>Iterative Utterance Segmentation for Neural Semantic Parsing</p>\n<ul>\n<li>神经语义解析器通常无法将长时间和复杂的话语解析为正确的含义表示，因为缺乏利用组成的原则。为了解决这个问题，我们提出了一个新颖的框架，用于通过迭代话语细分来促进神经语义解析器。给定输入话语，我们的框架在两个神经模块之间迭代：用于从说话分割跨度的细分器，以及将跨度映射到部分含义表示的解析器。然后，这些中间解析结果被组成到最终含义表示形式中。一个关键优势是，该框架不需要任何手工艺模板或其他标记的数据进行分割：我们通过提出一种新颖的培训方法来实现这一目标，在这种方法中，解析器为细分器提供伪监督</li>\n<li><img src=\"/.com//07/29/Survey/img-20220730153231.png\" class title=\"图片引用方法一\">\n</li>\n</ul>\n</li>\n<li><p>SEQZERO: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models</p>\n<ul>\n<li>Seqzero将问题分解为一系列子问题，这些序列与形式语言的子语言相对应。基于分解，LMS只需要使用预测子段的提示来生成简短的答案。因此，seqzero避免了一次产生长长的规范话语。此外，Seqzero不仅采用了几个射击模型，而且还采用了零拍模型来减轻过度拟合。特别是，Seqzero通过配备了我们建议的约束重新制定的合奏来阐明两种模型的优点。 Seqzero在GeoQuery和EcommerceQuery上实现了基于BART的模型的SOTA性能，它们是两个具有组成数据分配的少量数据集。</li>\n<li>SQL拆分示例<img src=\"/.com//07/29/Survey/img-20220730151705.png\" class title=\"图片引用方法一\"></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"组合泛化：Compositional-Generalization\"><a href=\"#组合泛化：Compositional-Generalization\" class=\"headerlink\" title=\"组合泛化：Compositional Generalization\"></a>组合泛化：Compositional Generalization</h2><ul>\n<li>Hierarchical Poset Decoding for Compositional Generalization in Language<ul>\n<li>我们将人类语言理解形式化为结构化的预测任务，其中输出为部分有序集（POSET）。当前的编码器架构不能正确考虑语义的POSET结构，因此遭受了不良的组成概括能力。在本文中，我们提出了一种新型的层次poset解码范式，用于语言中的组成概括。直觉：（1）拟议的范式在语义中执行部分置换不变性，从而避免过拟合bias ordering信息； （2）分层机制允许捕获POSET的高级结构。我们评估了建议的decoder关于CFQ的表现。这是一个庞大而现实的自然语言问题，回答数据集，专门设计用于衡量组成概括。结果表明，它的表现优于当前解码器。</li>\n<li><img src=\"/.com//07/29/Survey/img-20220730153205.png\" class title=\"图片引用方法一\">\n</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li><p>SUBS: Subtree Substitution for Compositional Semantic Parsing</p>\n<ul>\n<li>将子树替换用于组成数据增强，在此我们认为具有类似语义函数的子树是可交换的。</li>\n<li>子树替换示例：<img src=\"/.com//07/29/Survey/img-20220730151835.png\" class title=\"图片引用方法一\"></li>\n</ul>\n</li>\n<li><p>Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations</p>\n<ul>\n<li>序列到序列（SEQ2SEQ）模型在语义解析中很普遍，但已被发现在分布外的组成概括方面挣扎。尽管已经提出了专门的模型体系结构和SEQ2SEQ模型的预培训来解决此问题，但前者通常以一般性为代价，而后者仅显示有限的成功。在本文中，我们研究了中间表示对预训练的SEQ2SEQ模型中组成概括的影响，而无需更改模型体系结构，并确定设计有效表示的关键方面。我们没有将自然语言直接映射到可执行形式的训练，而是将其映射到具有更强的与自然语言的结构对应的可逆或有损中间表示形式。我们提出的中间表示和预培训模型的组合非常有效，最佳组合获得了CFQ上的最新最新作品（+14.8精度点）以及三个文本到 - 到 - SQL数据集（+15.0至+19.4精度点）。这项工作强调了中间表示提供了一种重要且可能被忽视的自由度，以提高预训练的SEQ2SEQ模型的组成概括能力。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"语义角色标注：Semantic-role-labeling\"><a href=\"#语义角色标注：Semantic-role-labeling\" class=\"headerlink\" title=\"语义角色标注：Semantic role labeling\"></a>语义角色标注：Semantic role labeling</h2><ul>\n<li>Syntax-aware Neural Semantic Role Labeling∗<ul>\n<li>传统的基于离散功能的SRL方法是由句法和语义结构之间的紧密相关性的激励，从而大大利用了句法特征。相反，基于深神经网络的方法通常将输入句子编码为单词序列，而无需考虑句法结构。在这项工作中，我们研究了以前的几种编码句法树的方法，并对额外的语法感知表示是否对神经SRL模型有益。基准CONLL-2005数据集的实验表明，语法感知的SRL方法可以通过Elmo的外部单词表示有效地改善强大基线的性能。借助额外的语法意识表示，我们的方法在测试数据上实现了新的最新的85.6 F1（单个模型）和86.6 F1（集合），分别优于0.8和1.0的Elmo的相应强基础。进行了详细的错误分析，以获得有关研究方法的更多见解。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"自动化合规检查：ACC（Automated-Compliance-Checking）\"><a href=\"#自动化合规检查：ACC（Automated-Compliance-Checking）\" class=\"headerlink\" title=\"自动化合规检查：ACC（Automated Compliance Checking）\"></a>自动化合规检查：ACC（Automated Compliance Checking）</h2><ul>\n<li>SPAR.txt, a cheap Shallow Parsing approach for Regulatory texts<ul>\n<li>自动化合规检查（ACC）系统旨在将语义解析为一组规则。但是，已知语义解析很难，需要大量的培训数据。创建此类培训数据的复杂性导致了研究的研究，该研究重点是小型子任务，例如浅解析或提取有限的规则子集。这项研究介绍了一项浅解析任务，培训数据相对便宜，目的是学习ACC的词典。我们注释了200个句子Spar.txt1的小域特异性数据集，并训练一个序列标记器，该序列标记器在测试集上达到79,93 F1分数。</li>\n<li><img src=\"/.com//07/29/Survey/img-20220730152243.png\" class title=\"图片引用方法一\">\n</li>\n</ul>\n</li>\n</ul>\n"},{"title":"EM","url":"/2023/01/15/EM/","content":"<h1 id=\"算法\"><a href=\"#算法\" class=\"headerlink\" title=\"算法\"></a>算法</h1><blockquote>\n<p>最大期望演算法（Expectation-maximization algorithm，又譯期望最大化算法）在统计中被用于寻找，依赖于不可观察的隐性变量的概率模型中，参数的最大似然估计。本文将从EM算法的背景、原理和应用三个层面对该算法进行细致的讲解。<br><span id=\"more\"></span></p>\n</blockquote>\n<h2 id=\"引入——高斯混合模型（GMM）\"><a href=\"#引入——高斯混合模型（GMM）\" class=\"headerlink\" title=\"引入——高斯混合模型（GMM）\"></a>引入——高斯混合模型（GMM）</h2><p><strong>定义</strong>：高斯混合模型是指具有$P(y|\\theta) = \\sum \\limits<em>{k=1}^K \\alpha_k\\phi(y|\\theta_k)$形式的概率分布模型。其中$\\alpha_k \\geq 0$且$\\sum \\limits</em>{k=1}^K \\alpha_k= 1$。$\\phi(y|\\theta_k)$是高斯分布密度，$\\theta_k=(\\mu_k,\\sigma_k^2)$，</p>\n<script type=\"math/tex; mode=display\">\\begin{aligned} \\phi(y|\\theta_k)=\\frac{1}{\\sqrt{2\\pi}\\sigma_k}exp(-\\frac{(y-\\mu_k)^2}{2\\sigma_k^2}) \\end{aligned}</script><p>称为第k个分模型。<br><strong>GMM参数估计的EM算法</strong><br>输入：观测数据$y_1,y_2,…,y_N$，高斯混合模型；<br>输出：高斯混合模型的参数</p>\n<ul>\n<li><p>取参数初始值开始迭代</p>\n</li>\n<li><p>E步：根据当前模型参数，计算分模型k对观测数据$y<em>j$的响应度<br>$\\begin{aligned}\\hat\\gamma</em>{jk} = \\frac{\\alpha<em>k\\phi(y_j|\\theta_k)}{\\sum \\limits</em>{k=1}^K\\alpha_k\\phi(y_j|\\theta_k)}\\end{aligned}$</p>\n</li>\n<li><p>M步：计算新一轮的模型参数<br>$n<em>k = \\sum \\limits</em>{j=1}^NE<em>{\\gamma</em>{jk}}$</p>\n<p>$\\begin{aligned}\\hat\\mu<em>k = \\frac{\\sum \\limits</em>{j=1}^N\\hat\\gamma<em>{jk}y_j}{\\sum \\limits</em>{j=1}^N\\hat\\gamma_{jk}}\\end{aligned}$</p>\n<p>$\\begin{aligned}\\hat\\sigma<em>k^2 = \\frac{\\sum \\limits</em>{j=1}^N\\hat\\gamma<em>{jk}(y_j-\\mu_k)^2}{\\sum \\limits</em>{j=1}^N\\hat\\gamma_{jk}}\\end{aligned}$</p>\n</li>\n<li><p>重复以上两步，直到收敛。</p>\n</li>\n</ul>\n<p>根据这个算法，我们使用python实现GMM中参数估计的EM算法，并三次迭代过程的运行结果依次进行展示。其中，实线部分代表真实的概率分布。虚线部分代表当前参数对应的概率分布。</p>\n<p><strong>例子解析</strong><br>通过这个例子，我们可以总结出EM算法的基本使用步骤：<br>(1)选取参数初始值<br>(2)E步：根据当前参数值，计算一个函数值<br>(3)M步：根据计算出的函数值，更新参数估计值<br>(4)重复(2)(3)直到收敛<br>但是，我们仍然存在很多疑问：</p>\n<ul>\n<li>为什么要使用EM算法？</li>\n<li>EM算法为什么可行？</li>\n<li>EM算法中的参数应当如何进行初始化？</li>\n<li>E步中需要根据当前参数值去求哪个函数的值？</li>\n<li>M步中参数应该如何根据E步求出的函数值进行估计？</li>\n<li>收敛的判断标准是什么？</li>\n</ul>\n<p>带着这些疑问，我们进行进一步的EM算法的产生背景和原理的学习。解决完这些疑问后，本文将进一步展示EM算法的应用实例，帮助读者真正掌握EM算法的使用流程。</p>\n<h2 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h2><p><strong>基本概念</strong></p>\n<ul>\n<li>概率模型：是用来描述不同随机变量之间关系的数学模型，通常情况下刻画了一个或多个随机变量之间的相互非确定性的概率关系。从数学上讲，该模型通常被表达为(Y,P)，其中Y是观测集合用来描述可能的观测结果，P是Y对应的概率分布函数或密度函数集合。最常见的概率分布函数是参数模型，它是由有限维参数构成的分布集合。</li>\n<li>观测变量是指在试验中可以通过观测直接得到的随机变量。</li>\n<li>隐变量指的是不可观测的随机变量。隐变量可以通过使用数学模型依据观测得的数据被推断出来。</li>\n</ul>\n<p>如果使用概率模型来对观测变量进行描述，在选定概率模型后，需要解决的问题就变成了对概率模型的参数的估计。其中，最常见并且也是应用最广泛的方法是极大似然估计法（英語：Maximum Likelihood Estimation，簡作MLE）。</p>\n<ul>\n<li>MLE算法<ul>\n<li>基本思想<ul>\n<li>选定一组参数，使得在该参数条件下，观测变量存在的概率最大</li>\n</ul>\n</li>\n<li>算法流程<ul>\n<li>写出模型的对数似然函数</li>\n<li>求参数的梯度，并使梯度为0，求解极大值点，作为参数的估计值</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>对于只存在观测变量的概率模型，使用极大似然估计法往往就可以很好地估计参数。但是，当概率模型中引入隐变量的时候，MLE就难以按照上述流程解决问题了。</p>\n<p><strong>典型问题</strong><br>抛硬币A,B,C，正面的概率分别为π,p,q。先抛A，当A的结果是正面时抛B，反之抛C。记录第二次抛硬币的结果，独立重复n次实验。<br>根据上述流程，写出其似然函数如下：<br>$P(Y|\\theta)=\\prod \\limits_{j=1}^n[\\pi p^{y_j}(1-p)^{1-y_j} + (1-\\pi)q^{y_j}(1-q)^{1-y_j}]$<br>由于隐变量的存在，导致对该似然函数求极值点，无解析解。针对MLE中存在的这种问题，EM算法产生，并有效地解决了这类问题。<br><strong>EM算法</strong><br>输入：观测变量数据Y，隐变量数据Z，联合分布P(Y,Z|θ)，条件分布P(Z|Y,θ)；<br>输出：模型参数估计值θ<br>(1)选择初始参数<br>(2)E：根据当前参数θi的估计值，计算Q(θ,θi)<br>(3)M：求使Q(θ,θi)最大的估计值θ，作为新一轮的参数估计值θi+1<br>(4)重复(2)和(3)，直至收敛（参数估计值变化量小于预设阈值或者Q函数值变化量小于预设阈值）</p>\n<p>由于在隐变量存在时，概率模型的参数估计问题没有解析解。因此，EM算法产生。它试图通过一种迭代的方式去逼近最佳估计。<br>至此，我们已经成功解决了EM算法为什么会产生的疑问。引入中的其他疑问，将会在原理中进一步解释。</p>\n<h2 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h2><p><strong>Q函数的选取</strong><br>在刚刚提到的EM算法的定义中，我们看到在E步和M步都涉及到一个函数:$Q(\\theta,\\theta<em>i)$。那么这个函数是如何得到的？<br>我们知道，EM算法的产生是为了解决MLE的局限性。但是其思想是一致的，最大化似然概率。<br>$P(Y|\\theta) = \\sum \\limits_ZP(Y,Z|\\theta)$<br>最大化似然概率又等价于最大化对数似然概率<br>$L(\\theta) = log(\\sum \\limits_ZP(Y,Z|\\theta))$<br>而EM算法通过迭代逐步逼近最佳参数估计，因此我们考量两次相邻迭代之间的差：<br>$\\begin{aligned}L(\\theta)-L(\\theta^{(i)}) &amp;= log(\\sum \\limits</em>{Z}P(Y|Z,\\theta)P(Z|\\theta)) - logP(Y|\\theta^{(i)}) \\<br>&amp;= log(\\sum \\limits_{Z}P(Z|Y,\\theta^{(i)})\\frac{P(Y|Z,\\theta)P(Z|\\theta))}{P(Z|Y,\\theta^{(i)})} - logP(Y|\\theta^{(i)})\\end{aligned}$<br>这里，我们需要利用Jessen不等式进行进一步的缩放。</p>\n<blockquote>\n<p>Jessen不等式证明：<br><strong>凸函数</strong><br>凸函数是一个定义在某个向量空间的凸子集 C（区间）上的实值函数 f，如果在其定义域 C 上的任意两点$x<em>1,x_2,0 \\leq t \\leq 1$，有<br>$tf(x_1)+(1-t)f(x_2) \\geq f(tx_1+(1-t)x_2) \\quad \\quad (1)$<br>也就是说凸函数任意两点的割线位于函数图形上方， 这也是Jensen不等式的两点形式。<br><strong>Jensen不等式</strong><br>若对于任意点集${x_i}$，若$\\lambda_i \\geq 0$且$\\sum \\limits_i\\lambda_i = 1$，使用数学归纳法，可以<a href=\"(https://zhuanlan.zhihu.com/p/39315786\">证明</a>)凸函数 f (x) 满足：<br>$f(\\sum \\limits</em>{i=1}^M\\lambda<em>ix_i) \\leq \\sum \\limits</em>{i=1}^M\\lambda_if(x_i) \\quad \\quad (2)$<br>公式(2)被称为 Jensen 不等式，它是式(1)的泛化形式。</p>\n</blockquote>\n<p>对log函数取负号即变为凸函数，然后进行缩放，然后左右再同时取负号，可以得到：</p>\n<script type=\"math/tex; mode=display\">\\begin{aligned} L(\\theta)-L(\\theta^{i})&=log(\\sum_{Z}P(Z|Y,\\theta^i)\\frac{ P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^i)})-\\log P(Y|\\theta^i)\\\\ &\\ge \\sum_{Z} P(Z|Y,\\theta^i)log(\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^i)})-\\log P(Y|\\theta^i)\\\\ &=\\sum_{Z} P(Z|Y,\\theta^i)log(\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^i)})-\\sum_{Z} P(Z|Y,\\theta^i)\\log P(Y|\\theta^i)\\\\ &= \\sum_{Z} P(Z|Y,\\theta^i)[log\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^i)}-logP(Y|\\theta^{i})] \\\\ &= \\sum_{Z} P(Z|Y,\\theta^i)log\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^i) P(Y|\\theta^{i})} \\end{aligned}</script><p>我们将缩放后的差值和上一轮迭代的对数似然函数的结果相加，定义如下：<br>$\\begin{aligned}B(\\theta,\\theta<em>i) = L(\\theta_i) + \\sum</em>{Z} P(Z|Y,\\theta^i)log\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^i) P(Y|\\theta^{i})}\\end{aligned}$<br>我们根据定义，可以得知如下两个结论：<br>(1)$B(\\theta<em>i,\\theta_i) = L(\\theta_i)$<br>(2)$L(\\theta) \\geq B(\\theta,\\theta_i)$<br>更形象地看，两者的关系可以用下图表示：<br><img src=\"/.com//01/15/EM/BL.png\" class title=\"图片引用方法一\"><br>其中，对于每次迭代的增值。B函数的增值幅度永远不超过对数似然函数的增值幅度。因此，我们只需要保证迭代每次都可以增加B函数，即可达到通过迭代逐步增加对数似然函数的目的。<br>因此，第i+1轮的$\\theta$的估计值可以表示为：<br>$\\begin{aligned}\\theta</em>{i+1} &amp;= argmax<em>{\\theta} B(\\theta,\\theta_i) \\<br>&amp;= argmax</em>\\theta (L(\\theta<em>i) + \\sum</em>{Z} P(Z|Y,\\theta^i)log\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^i) P(Y|\\theta^{i})})\\<br>&amp;= argmax<em>\\theta(L(\\theta_i) -\\sum</em>{Z} P(Z|Y,\\theta^i)logP(Z|Y,\\theta^i) P(Y|\\theta^{i})  + \\sum<em>{Z} P(Z|Y,\\theta^i)logP(Y|Z,\\theta)P(Z|\\theta))\\<br>&amp;=argmax</em>\\theta(\\sum<em>{Z} P(Z|Y,\\theta^i)logP(Y|Z,\\theta)P(Z|\\theta))\\<br>&amp;=argmax</em>\\theta(\\sum\\limits<em>ZP(Z|Y,\\theta_i)logP(Y,Z|\\theta))\\<br>&amp;=argmax</em>\\theta(Q(\\theta,\\theta^i))<br>\\end{aligned}$<br>最后一行就是EM算法定义中提到的Q函数。它的期望形式如下：</p>\n<script type=\"math/tex; mode=display\">\\begin{aligned}Q(\\theta,\\theta^i)&=\\sum\\limits_{Z} P(Z|Y,\\theta^i)logP(Y,Z|\\theta)\\\\ &=E_Z[logP(Y,Z|\\theta)|Y,\\theta^i]\\\\ &=E_{Z|Y,\\theta^i}logP(Y,Z|\\theta) \\end{aligned}</script><p>至此，Q函数的来源问题我们也已经解决。下面，我们来进一步证明EM算法可以通过迭代的方式逐步增大对数似然函数。<br><strong>证明：</strong>$P(Y|\\theta_{i+1}) \\geq P(Y|\\theta_i)$</p>\n<h2 id=\"应用\"><a href=\"#应用\" class=\"headerlink\" title=\"应用\"></a>应用</h2><ul>\n<li><p>隐马尔可夫模型无监督学习</p>\n</li>\n<li><p>PLSA</p>\n</li>\n<li>LDA的变分EM算法</li>\n</ul>\n<h2 id=\"推广\"><a href=\"#推广\" class=\"headerlink\" title=\"推广\"></a>推广</h2><p><strong>F函数</strong>：假设隐变量数据$Z$的概率分布为$\\widetilde{P}(Z)$,定义分布$ \\widetilde{P}$与参数$\\theta$的函数:</p>\n<script type=\"math/tex; mode=display\">\nF( \\widetilde{P},\\theta) = E_{ \\widetilde{P}}[\\log P(Y,Z|\\theta)] + H( \\widetilde{P})</script><p>称为$F$函数。式中$H(\\widetilde{P}) = -E_{\\widetilde{P}}\\log\\widetilde{P}(Z)$是分布$\\widetilde{P}(Z)$的熵。</p>\n<h3 id=\"推论：EM算法的一次迭代可由F函数的极大-极大算法实现。\"><a href=\"#推论：EM算法的一次迭代可由F函数的极大-极大算法实现。\" class=\"headerlink\" title=\"推论：EM算法的一次迭代可由F函数的极大-极大算法实现。\"></a>推论：EM算法的一次迭代可由F函数的极大-极大算法实现。</h3><ul>\n<li>GEM算法1：直接对F函数中的$\\widetilde{P}$和$\\theta$进行迭代<ul>\n<li>缺点：很难直接找到$\\theta$对用的极大值点</li>\n</ul>\n</li>\n<li>GME算法2：对Q函数进行迭代<ul>\n<li>优点：只需找到$\\theta^{i+1}$使Q函数变大即可</li>\n</ul>\n</li>\n<li>GME算法3：对Q函数中的参数，一次进行一个分量的迭代</li>\n</ul>\n"}]