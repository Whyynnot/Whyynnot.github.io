[{"url":"/2022/09/14/COMPLEXWEBQUESTIONS/","content":""},{"url":"/2022/09/19/CoNLL-2000-Chunking/","content":""},{"url":"/2022/09/16/HotpotQA/","content":""},{"url":"/2022/09/15/Q_D_R/","content":""},{"url":"/2022/09/14/ReinforcementLearning/","content":"<h1 id=\"Reinforcement-Learning\"><a href=\"#Reinforcement-Learning\" class=\"headerlink\" title=\"Reinforcement Learning\"></a>Reinforcement Learning</h1><h2 id><a href=\"#\" class=\"headerlink\" title=\" \"></a> </h2>"},{"url":"/2022/09/14/HMM/","content":"<h1 id=\"Hidden-Markov-model\"><a href=\"#Hidden-Markov-model\" class=\"headerlink\" title=\"Hidden Markov model\"></a>Hidden Markov model</h1><h2 id=\"基本概念\"><a href=\"#基本概念\" class=\"headerlink\" title=\"基本概念\"></a>基本概念</h2><h3 id=\"构成\"><a href=\"#构成\" class=\"headerlink\" title=\"构成\"></a>构成</h3><ul>\n<li>$\\pi$:出事状态概率向量</li>\n<li>$A$:状态转移概率矩阵</li>\n<li>$B$:观测概率矩阵</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\lambda = (A,B,\\pi)</script><h3 id=\"基本假设\"><a href=\"#基本假设\" class=\"headerlink\" title=\"基本假设\"></a>基本假设</h3><ul>\n<li>齐次马尔可夫假设：t时刻的状态只依赖前一时刻的状态</li>\n<li>观测独立性假设：任意时刻的观测只依赖该时刻马尔可夫链的状态</li>\n</ul>\n<h3 id=\"三个基本问题\"><a href=\"#三个基本问题\" class=\"headerlink\" title=\"三个基本问题\"></a>三个基本问题</h3><ul>\n<li>概率计算问题：给定模型$\\lambda = (A,B,\\pi)$和观测序列$O=(o_1,o_2,…,o_T)$,计算在模型$\\lambda$下序列$O$出现的概率</li>\n<li>学习问题：已知观测序列，估计模型$\\lambda = (A,B,\\pi)$的参数，使得该模型下观测序列出现的概率$P(O|\\lambda)$最大，即极大似然估计方法估计参数</li>\n<li>预测问题（解码问题）：已知模型和观测序列，求解最有可能的对应的状态序列$P(I|O)$</li>\n</ul>\n<h3 id=\"基本问题解法\"><a href=\"#基本问题解法\" class=\"headerlink\" title=\"基本问题解法\"></a>基本问题解法</h3><h4 id=\"概率计算\"><a href=\"#概率计算\" class=\"headerlink\" title=\"概率计算\"></a>概率计算</h4><ul>\n<li><p>直接计算</p>\n<script type=\"math/tex; mode=display\">\nP(O|\\lambda) = \\sum_{I}P(O|I,\\lambda)P(I|\\lambda)</script></li>\n<li><p>前向算法</p>\n</li>\n<li>后向算法</li>\n</ul>\n<h4 id=\"学习算法\"><a href=\"#学习算法\" class=\"headerlink\" title=\"学习算法\"></a>学习算法</h4><ul>\n<li>监督学习方法：直接大数定律，极大似然估计</li>\n<li>Baum-Welch算法（EM算法）<ul>\n<li>写出Q函数</li>\n<li>利用拉格朗日乘子法求解极值点</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"预测算法\"><a href=\"#预测算法\" class=\"headerlink\" title=\"预测算法\"></a>预测算法</h4><ul>\n<li><p>近似算法</p>\n<ul>\n<li>每个时刻t选择最可能出现的状态，然后得到状态序列，作为预测结果</li>\n<li>缺点：不能保证预测的状态序列整体是最优的</li>\n</ul>\n</li>\n<li><p>维特比算法</p>\n<ul>\n<li>动态规划求解最优路径</li>\n</ul>\n<blockquote>\n<p><a href=\"https://www.zhihu.com/question/20136144\">https://www.zhihu.com/question/20136144</a></p>\n</blockquote>\n</li>\n</ul>\n"},{"title":"Survey","url":"/2022/07/29/Survey/","content":"<h1 id=\"A-Survey\"><a href=\"#A-Survey\" class=\"headerlink\" title=\"A Survey\"></a>A Survey</h1><blockquote>\n<p>目前的seq2seq类的模型在NLP领域大行其道。但是，它将各种复合句都编码为一个向量来处理。这样会出现组合爆炸问题，即简单句的组合会导致数据量剧增。因此，分析句子的结构，对复合句进行拆分，并将句子的结构知识运用到模型中，有望解决数据获取难等问题。针对该直觉，我进行了相关工作的调研，现将一些调研结果总结如下。<br><span id=\"more\"></span></p>\n</blockquote>\n<h2 id=\"句子的拆分和改写：Sentence-Split-and-Rephrase\"><a href=\"#句子的拆分和改写：Sentence-Split-and-Rephrase\" class=\"headerlink\" title=\"句子的拆分和改写：Sentence Split and Rephrase\"></a>句子的拆分和改写：Sentence Split and Rephrase</h2><ul>\n<li>Fact-Aware Sentence Split and Rephrase with Permutation Invariant Training<ul>\n<li>训练过程中对事实的遗失：Fact-aware Sentence Encoding</li>\n<li>简单句的顺序的影响：PIT(Permutation Invariant Training)</li>\n<li><img src=\"/.com//07/29/Survey/img-20220730153106.png\" class title=\"图片引用方法一\">\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"语义解析-Semantic-Parsing\"><a href=\"#语义解析-Semantic-Parsing\" class=\"headerlink\" title=\"语义解析:Semantic Parsing\"></a>语义解析:Semantic Parsing</h2><ul>\n<li><p>Iterative Utterance Segmentation for Neural Semantic Parsing</p>\n<ul>\n<li>神经语义解析器通常无法将长时间和复杂的话语解析为正确的含义表示，因为缺乏利用组成的原则。为了解决这个问题，我们提出了一个新颖的框架，用于通过迭代话语细分来促进神经语义解析器。给定输入话语，我们的框架在两个神经模块之间迭代：用于从说话分割跨度的细分器，以及将跨度映射到部分含义表示的解析器。然后，这些中间解析结果被组成到最终含义表示形式中。一个关键优势是，该框架不需要任何手工艺模板或其他标记的数据进行分割：我们通过提出一种新颖的培训方法来实现这一目标，在这种方法中，解析器为细分器提供伪监督</li>\n<li><img src=\"/.com//07/29/Survey/img-20220730153231.png\" class title=\"图片引用方法一\">\n</li>\n</ul>\n</li>\n<li><p>SEQZERO: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models</p>\n<ul>\n<li>Seqzero将问题分解为一系列子问题，这些序列与形式语言的子语言相对应。基于分解，LMS只需要使用预测子段的提示来生成简短的答案。因此，seqzero避免了一次产生长长的规范话语。此外，Seqzero不仅采用了几个射击模型，而且还采用了零拍模型来减轻过度拟合。特别是，Seqzero通过配备了我们建议的约束重新制定的合奏来阐明两种模型的优点。 Seqzero在GeoQuery和EcommerceQuery上实现了基于BART的模型的SOTA性能，它们是两个具有组成数据分配的少量数据集。</li>\n<li>SQL拆分示例<img src=\"/.com//07/29/Survey/img-20220730151705.png\" class title=\"图片引用方法一\"></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"组合泛化：Compositional-Generalization\"><a href=\"#组合泛化：Compositional-Generalization\" class=\"headerlink\" title=\"组合泛化：Compositional Generalization\"></a>组合泛化：Compositional Generalization</h2><ul>\n<li>Hierarchical Poset Decoding for Compositional Generalization in Language<ul>\n<li>我们将人类语言理解形式化为结构化的预测任务，其中输出为部分有序集（POSET）。当前的编码器架构不能正确考虑语义的POSET结构，因此遭受了不良的组成概括能力。在本文中，我们提出了一种新型的层次poset解码范式，用于语言中的组成概括。直觉：（1）拟议的范式在语义中执行部分置换不变性，从而避免过拟合bias ordering信息； （2）分层机制允许捕获POSET的高级结构。我们评估了建议的decoder关于CFQ的表现。这是一个庞大而现实的自然语言问题，回答数据集，专门设计用于衡量组成概括。结果表明，它的表现优于当前解码器。</li>\n<li><img src=\"/.com//07/29/Survey/img-20220730153205.png\" class title=\"图片引用方法一\">\n</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li><p>SUBS: Subtree Substitution for Compositional Semantic Parsing</p>\n<ul>\n<li>将子树替换用于组成数据增强，在此我们认为具有类似语义函数的子树是可交换的。</li>\n<li>子树替换示例：<img src=\"/.com//07/29/Survey/img-20220730151835.png\" class title=\"图片引用方法一\"></li>\n</ul>\n</li>\n<li><p>Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations</p>\n<ul>\n<li>序列到序列（SEQ2SEQ）模型在语义解析中很普遍，但已被发现在分布外的组成概括方面挣扎。尽管已经提出了专门的模型体系结构和SEQ2SEQ模型的预培训来解决此问题，但前者通常以一般性为代价，而后者仅显示有限的成功。在本文中，我们研究了中间表示对预训练的SEQ2SEQ模型中组成概括的影响，而无需更改模型体系结构，并确定设计有效表示的关键方面。我们没有将自然语言直接映射到可执行形式的训练，而是将其映射到具有更强的与自然语言的结构对应的可逆或有损中间表示形式。我们提出的中间表示和预培训模型的组合非常有效，最佳组合获得了CFQ上的最新最新作品（+14.8精度点）以及三个文本到 - 到 - SQL数据集（+15.0至+19.4精度点）。这项工作强调了中间表示提供了一种重要且可能被忽视的自由度，以提高预训练的SEQ2SEQ模型的组成概括能力。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"语义角色标注：Semantic-role-labeling\"><a href=\"#语义角色标注：Semantic-role-labeling\" class=\"headerlink\" title=\"语义角色标注：Semantic role labeling\"></a>语义角色标注：Semantic role labeling</h2><ul>\n<li>Syntax-aware Neural Semantic Role Labeling∗<ul>\n<li>传统的基于离散功能的SRL方法是由句法和语义结构之间的紧密相关性的激励，从而大大利用了句法特征。相反，基于深神经网络的方法通常将输入句子编码为单词序列，而无需考虑句法结构。在这项工作中，我们研究了以前的几种编码句法树的方法，并对额外的语法感知表示是否对神经SRL模型有益。基准CONLL-2005数据集的实验表明，语法感知的SRL方法可以通过Elmo的外部单词表示有效地改善强大基线的性能。借助额外的语法意识表示，我们的方法在测试数据上实现了新的最新的85.6 F1（单个模型）和86.6 F1（集合），分别优于0.8和1.0的Elmo的相应强基础。进行了详细的错误分析，以获得有关研究方法的更多见解。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"自动化合规检查：ACC（Automated-Compliance-Checking）\"><a href=\"#自动化合规检查：ACC（Automated-Compliance-Checking）\" class=\"headerlink\" title=\"自动化合规检查：ACC（Automated Compliance Checking）\"></a>自动化合规检查：ACC（Automated Compliance Checking）</h2><ul>\n<li>SPAR.txt, a cheap Shallow Parsing approach for Regulatory texts<ul>\n<li>自动化合规检查（ACC）系统旨在将语义解析为一组规则。但是，已知语义解析很难，需要大量的培训数据。创建此类培训数据的复杂性导致了研究的研究，该研究重点是小型子任务，例如浅解析或提取有限的规则子集。这项研究介绍了一项浅解析任务，培训数据相对便宜，目的是学习ACC的词典。我们注释了200个句子Spar.txt1的小域特异性数据集，并训练一个序列标记器，该序列标记器在测试集上达到79,93 F1分数。</li>\n<li><img src=\"/.com//07/29/Survey/img-20220730152243.png\" class title=\"图片引用方法一\">\n</li>\n</ul>\n</li>\n</ul>\n"}]