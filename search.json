[{"title":"Survey","url":"/2022/07/29/Survey/","content":"<h1 id=\"a-survey\">A Survey</h1>\r\n<blockquote>\r\n<p>目前的seq2seq类的模型在NLP领域大行其道。但是，它将各种复合句都编码为一个向量来处理。这样会出现组合爆炸问题，即简单句的组合会导致数据量剧增。因此，分析句子的结构，对复合句进行拆分，并将句子的结构知识运用到模型中，有望解决数据获取难等问题。针对该直觉，我进行了相关工作的调研，现将一些调研结果总结如下。\r\n<span id=\"more\"></span></p>\r\n</blockquote>\r\n<h2 id=\"句子的拆分和改写sentence-split-and-rephrase\">句子的拆分和改写：Sentence\r\nSplit and Rephrase</h2>\r\n<ul>\r\n<li>Fact-Aware Sentence Split and Rephrase with Permutation Invariant\r\nTraining\r\n<ul>\r\n<li>训练过程中对事实的遗失：Fact-aware Sentence Encoding</li>\r\n<li>简单句的顺序的影响：PIT(Permutation Invariant Training)</li>\r\n<li><img src=\"/2022/07/29/Survey/07/29/Survey/img-20220730153106.png\" class title=\"图片引用方法一\"></li>\r\n</ul></li>\r\n</ul>\r\n<h2 id=\"语义解析semantic-parsing\">语义解析:Semantic Parsing</h2>\r\n<ul>\r\n<li>Iterative Utterance Segmentation for Neural Semantic Parsing\r\n<ul>\r\n<li>神经语义解析器通常无法将长时间和复杂的话语解析为正确的含义表示，因为缺乏利用组成的原则。为了解决这个问题，我们提出了一个新颖的框架，用于通过迭代话语细分来促进神经语义解析器。给定输入话语，我们的框架在两个神经模块之间迭代：用于从说话分割跨度的细分器，以及将跨度映射到部分含义表示的解析器。然后，这些中间解析结果被组成到最终含义表示形式中。一个关键优势是，该框架不需要任何手工艺模板或其他标记的数据进行分割：我们通过提出一种新颖的培训方法来实现这一目标，在这种方法中，解析器为细分器提供伪监督</li>\r\n<li><img src=\"/2022/07/29/Survey/07/29/Survey/img-20220730153231.png\" class title=\"图片引用方法一\"></li>\r\n</ul></li>\r\n<li>SEQZERO: Few-shot Compositional Semantic Parsing with Sequential\r\nPrompts and Zero-shot Models\r\n<ul>\r\n<li>Seqzero将问题分解为一系列子问题，这些序列与形式语言的子语言相对应。基于分解，LMS只需要使用预测子段的提示来生成简短的答案。因此，seqzero避免了一次产生长长的规范话语。此外，Seqzero不仅采用了几个射击模型，而且还采用了零拍模型来减轻过度拟合。特别是，Seqzero通过配备了我们建议的约束重新制定的合奏来阐明两种模型的优点。\r\nSeqzero在GeoQuery和EcommerceQuery上实现了基于BART的模型的SOTA性能，它们是两个具有组成数据分配的少量数据集。</li>\r\n<li>SQL拆分示例<img src=\"/2022/07/29/Survey/07/29/Survey/img-20220730151705.png\" class title=\"图片引用方法一\"></li>\r\n</ul></li>\r\n</ul>\r\n<h2 id=\"组合泛化compositional-generalization\">组合泛化：Compositional\r\nGeneralization</h2>\r\n<ul>\r\n<li>Hierarchical Poset Decoding for Compositional Generalization in\r\nLanguage\r\n<ul>\r\n<li>我们将人类语言理解形式化为结构化的预测任务，其中输出为部分有序集（POSET）。当前的编码器架构不能正确考虑语义的POSET结构，因此遭受了不良的组成概括能力。在本文中，我们提出了一种新型的层次poset解码范式，用于语言中的组成概括。直觉：（1）拟议的范式在语义中执行部分置换不变性，从而避免过拟合bias\r\nordering信息；\r\n（2）分层机制允许捕获POSET的高级结构。我们评估了建议的decoder关于CFQ的表现。这是一个庞大而现实的自然语言问题，回答数据集，专门设计用于衡量组成概括。结果表明，它的表现优于当前解码器。</li>\r\n<li><img src=\"/2022/07/29/Survey/07/29/Survey/img-20220730153205.png\" class title=\"图片引用方法一\"></li>\r\n</ul></li>\r\n<li>SUBS: Subtree Substitution for Compositional Semantic Parsing\r\n<ul>\r\n<li>将子树替换用于组成数据增强，在此我们认为具有类似语义函数的子树是可交换的。</li>\r\n<li>子树替换示例：<img src=\"/2022/07/29/Survey/07/29/Survey/img-20220730151835.png\" class title=\"图片引用方法一\"></li>\r\n</ul></li>\r\n<li>Unlocking Compositional Generalization in Pre-trained Models Using\r\nIntermediate Representations\r\n<ul>\r\n<li>序列到序列（SEQ2SEQ）模型在语义解析中很普遍，但已被发现在分布外的组成概括方面挣扎。尽管已经提出了专门的模型体系结构和SEQ2SEQ模型的预培训来解决此问题，但前者通常以一般性为代价，而后者仅显示有限的成功。在本文中，我们研究了中间表示对预训练的SEQ2SEQ模型中组成概括的影响，而无需更改模型体系结构，并确定设计有效表示的关键方面。我们没有将自然语言直接映射到可执行形式的训练，而是将其映射到具有更强的与自然语言的结构对应的可逆或有损中间表示形式。我们提出的中间表示和预培训模型的组合非常有效，最佳组合获得了CFQ上的最新最新作品（+14.8精度点）以及三个文本到\r\n- 到 -\r\nSQL数据集（+15.0至+19.4精度点）。这项工作强调了中间表示提供了一种重要且可能被忽视的自由度，以提高预训练的SEQ2SEQ模型的组成概括能力。</li>\r\n</ul></li>\r\n</ul>\r\n<h2 id=\"语义角色标注semantic-role-labeling\">语义角色标注：Semantic role\r\nlabeling</h2>\r\n<ul>\r\n<li>Syntax-aware Neural Semantic Role Labeling∗\r\n<ul>\r\n<li>传统的基于离散功能的SRL方法是由句法和语义结构之间的紧密相关性的激励，从而大大利用了句法特征。相反，基于深神经网络的方法通常将输入句子编码为单词序列，而无需考虑句法结构。在这项工作中，我们研究了以前的几种编码句法树的方法，并对额外的语法感知表示是否对神经SRL模型有益。基准CONLL-2005数据集的实验表明，语法感知的SRL方法可以通过Elmo的外部单词表示有效地改善强大基线的性能。借助额外的语法意识表示，我们的方法在测试数据上实现了新的最新的85.6\r\nF1（单个模型）和86.6\r\nF1（集合），分别优于0.8和1.0的Elmo的相应强基础。进行了详细的错误分析，以获得有关研究方法的更多见解。</li>\r\n</ul></li>\r\n</ul>\r\n<h2 id=\"自动化合规检查accautomated-compliance-checking\">自动化合规检查：ACC（Automated\r\nCompliance Checking）</h2>\r\n<ul>\r\n<li>SPAR.txt, a cheap Shallow Parsing approach for Regulatory texts\r\n<ul>\r\n<li>自动化合规检查（ACC）系统旨在将语义解析为一组规则。但是，已知语义解析很难，需要大量的培训数据。创建此类培训数据的复杂性导致了研究的研究，该研究重点是小型子任务，例如浅解析或提取有限的规则子集。这项研究介绍了一项浅解析任务，培训数据相对便宜，目的是学习ACC的词典。我们注释了200个句子Spar.txt1的小域特异性数据集，并训练一个序列标记器，该序列标记器在测试集上达到79,93\r\nF1分数。</li>\r\n<li><img src=\"/2022/07/29/Survey/07/29/Survey/img-20220730152243.png\" class title=\"图片引用方法一\"></li>\r\n</ul></li>\r\n</ul>\r\n"},{"title":"EM","url":"/2023/01/15/EM/","content":"<h1 id=\"em算法\">EM算法</h1>\r\n<blockquote>\r\n<p>最大期望演算法（Expectation-maximization\r\nalgorithm，又譯期望最大化算法）在统计中被用于寻找，依赖于不可观察的隐性变量的概率模型中，参数的最大似然估计。本文将从EM算法的背景、原理和应用三个层面对该算法进行细致的讲解。\r\n<span id=\"more\"></span></p>\r\n</blockquote>\r\n<h2 id=\"引入高斯混合模型gmm\">引入——高斯混合模型（GMM）</h2>\r\n<p><strong>定义</strong>：高斯混合模型是指具有<span class=\"math inline\">\\(P(y|\\theta) = \\sum \\limits_{k=1}^K\r\n\\alpha_k\\phi(y|\\theta_k)\\)</span>形式的概率分布模型。其中<span class=\"math inline\">\\(\\alpha_k \\geq 0\\)</span>且<span class=\"math inline\">\\(\\sum \\limits_{k=1}^K \\alpha_k= 1\\)</span>。<span class=\"math inline\">\\(\\phi(y|\\theta_k)\\)</span>是高斯分布密度，<span class=\"math inline\">\\(\\theta_k=(\\mu_k,\\sigma_k^2)\\)</span>，</p>\r\n<p><span class=\"math display\">\\[\\begin{aligned}\r\n\\phi(y|\\theta_k)=\\frac{1}{\\sqrt{2\\pi}\\sigma_k}exp(-\\frac{(y-\\mu_k)^2}{2\\sigma_k^2})\r\n\\end{aligned}\\]</span> 称为第k个分模型。\r\n<strong>GMM参数估计的EM算法</strong> 输入：观测数据<span class=\"math inline\">\\(y_1,y_2,...,y_N\\)</span>，高斯混合模型；\r\n输出：高斯混合模型的参数</p>\r\n<ul>\r\n<li><p>取参数初始值开始迭代</p></li>\r\n<li><p>E步：根据当前模型参数，计算分模型k对观测数据<span class=\"math inline\">\\(y_j\\)</span>的响应度 <span class=\"math inline\">\\(\\begin{aligned}\\hat\\gamma_{jk} =\r\n\\frac{\\alpha_k\\phi(y_j|\\theta_k)}{\\sum\r\n\\limits_{k=1}^K\\alpha_k\\phi(y_j|\\theta_k)}\\end{aligned}\\)</span></p></li>\r\n<li><p>M步：计算新一轮的模型参数 <span class=\"math inline\">\\(n_k = \\sum\r\n\\limits_{j=1}^NE_{\\gamma_{jk}}\\)</span></p>\r\n<p><span class=\"math inline\">\\(\\begin{aligned}\\hat\\mu_k = \\frac{\\sum\r\n\\limits_{j=1}^N\\hat\\gamma_{jk}y_j}{\\sum\r\n\\limits_{j=1}^N\\hat\\gamma_{jk}}\\end{aligned}\\)</span></p>\r\n<p><span class=\"math inline\">\\(\\begin{aligned}\\hat\\sigma_k^2 =\r\n\\frac{\\sum \\limits_{j=1}^N\\hat\\gamma_{jk}(y_j-\\mu_k)^2}{\\sum\r\n\\limits_{j=1}^N\\hat\\gamma_{jk}}\\end{aligned}\\)</span></p></li>\r\n<li><p>重复以上两步，直到收敛。</p></li>\r\n</ul>\r\n<p>根据这个算法，我们使用python实现GMM中参数估计的EM算法，并三次迭代过程的运行结果依次进行展示。其中，实线部分代表真实的概率分布。虚线部分代表当前参数对应的概率分布。</p>\r\n<p><strong>例子解析</strong>\r\n通过这个例子，我们可以总结出EM算法的基本使用步骤： (1)选取参数初始值\r\n(2)E步：根据当前参数值，计算一个函数值\r\n(3)M步：根据计算出的函数值，更新参数估计值 (4)重复(2)(3)直到收敛\r\n但是，我们仍然存在很多疑问：</p>\r\n<ul>\r\n<li>为什么要使用EM算法？</li>\r\n<li>EM算法为什么可行？</li>\r\n<li>EM算法中的参数应当如何进行初始化？</li>\r\n<li>E步中需要根据当前参数值去求哪个函数的值？</li>\r\n<li>M步中参数应该如何根据E步求出的函数值进行估计？</li>\r\n<li>收敛的判断标准是什么？</li>\r\n</ul>\r\n<p>带着这些疑问，我们进行进一步的EM算法的产生背景和原理的学习。解决完这些疑问后，本文将进一步展示EM算法的应用实例，帮助读者真正掌握EM算法的使用流程。</p>\r\n<h2 id=\"背景\">背景</h2>\r\n<p><strong>基本概念</strong></p>\r\n<ul>\r\n<li>概率模型：是用来描述不同随机变量之间关系的数学模型，通常情况下刻画了一个或多个随机变量之间的相互非确定性的概率关系。从数学上讲，该模型通常被表达为(Y,P)，其中Y是观测集合用来描述可能的观测结果，P是Y对应的概率分布函数或密度函数集合。最常见的概率分布函数是参数模型，它是由有限维参数构成的分布集合。</li>\r\n<li>观测变量是指在试验中可以通过观测直接得到的随机变量。</li>\r\n<li>隐变量指的是不可观测的随机变量。隐变量可以通过使用数学模型依据观测得的数据被推断出来。</li>\r\n</ul>\r\n<p>如果使用概率模型来对观测变量进行描述，在选定概率模型后，需要解决的问题就变成了对概率模型的参数的估计。其中，最常见并且也是应用最广泛的方法是极大似然估计法（英語：Maximum\r\nLikelihood Estimation，簡作MLE）。</p>\r\n<ul>\r\n<li>MLE算法\r\n<ul>\r\n<li>基本思想\r\n<ul>\r\n<li>选定一组参数，使得在该参数条件下，观测变量存在的概率最大</li>\r\n</ul></li>\r\n<li>算法流程\r\n<ul>\r\n<li>写出模型的对数似然函数</li>\r\n<li>求参数的梯度，并使梯度为0，求解极大值点，作为参数的估计值</li>\r\n</ul></li>\r\n</ul></li>\r\n</ul>\r\n<p>对于只存在观测变量的概率模型，使用极大似然估计法往往就可以很好地估计参数。但是，当概率模型中引入隐变量的时候，MLE就难以按照上述流程解决问题了。</p>\r\n<p><strong>典型问题</strong>\r\n抛硬币A,B,C，正面的概率分别为π,p,q。先抛A，当A的结果是正面时抛B，反之抛C。记录第二次抛硬币的结果，独立重复n次实验。\r\n根据上述流程，写出其似然函数如下： <span class=\"math inline\">\\(P(Y|\\theta)=\\prod \\limits_{j=1}^n[\\pi\r\np^{y_j}(1-p)^{1-y_j} + (1-\\pi)q^{y_j}(1-q)^{1-y_j}]\\)</span>\r\n由于隐变量的存在，导致对该似然函数求极值点，无解析解。针对MLE中存在的这种问题，EM算法产生，并有效地解决了这类问题。\r\n<strong>EM算法</strong>\r\n输入：观测变量数据Y，隐变量数据Z，联合分布P(Y,Z|θ)，条件分布P(Z|Y,θ)；\r\n输出：模型参数估计值θ (1)选择初始参数\r\n(2)E：根据当前参数θi的估计值，计算Q(θ,θi)\r\n(3)M：求使Q(θ,θi)最大的估计值θ，作为新一轮的参数估计值θi+1\r\n(4)重复(2)和(3)，直至收敛（参数估计值变化量小于预设阈值或者Q函数值变化量小于预设阈值）</p>\r\n<p>由于在隐变量存在时，概率模型的参数估计问题没有解析解。因此，EM算法产生。它试图通过一种迭代的方式去逼近最佳估计。\r\n至此，我们已经成功解决了EM算法为什么会产生的疑问。引入中的其他疑问，将会在原理中进一步解释。</p>\r\n<h2 id=\"原理\">原理</h2>\r\n<p><strong>Q函数的选取</strong>\r\n在刚刚提到的EM算法的定义中，我们看到在E步和M步都涉及到一个函数:<span class=\"math inline\">\\(Q(\\theta,\\theta_i)\\)</span>。那么这个函数是如何得到的？\r\n我们知道，EM算法的产生是为了解决MLE的局限性。但是其思想是一致的，最大化似然概率。\r\n<span class=\"math inline\">\\(P(Y|\\theta) = \\sum\r\n\\limits_ZP(Y,Z|\\theta)\\)</span> 最大化似然概率又等价于最大化对数似然概率\r\n<span class=\"math inline\">\\(L(\\theta) = log(\\sum\r\n\\limits_ZP(Y,Z|\\theta))\\)</span>\r\n而EM算法通过迭代逐步逼近最佳参数估计，因此我们考量两次相邻迭代之间的差：\r\n<span class=\"math inline\">\\(\\begin{aligned}L(\\theta)-L(\\theta^{(i)})\r\n&amp;= log(\\sum \\limits_{Z}P(Y|Z,\\theta)P(Z|\\theta)) -\r\nlogP(Y|\\theta^{(i)}) \\\\ &amp;= log(\\sum\r\n\\limits_{Z}P(Z|Y,\\theta^{(i)})\\frac{P(Y|Z,\\theta)P(Z|\\theta))}{P(Z|Y,\\theta^{(i)})}\r\n- logP(Y|\\theta^{(i)})\\end{aligned}\\)</span>\r\n这里，我们需要利用Jessen不等式进行进一步的缩放。</p>\r\n<blockquote>\r\n<p>Jessen不等式证明： <strong>凸函数</strong>\r\n凸函数是一个定义在某个向量空间的凸子集 C（区间）上的实值函数\r\nf，如果在其定义域 C 上的任意两点<span class=\"math inline\">\\(x_1,x_2,0\r\n\\leq t \\leq 1\\)</span>，有 <span class=\"math inline\">\\(tf(x_1)+(1-t)f(x_2) \\geq f(tx_1+(1-t)x_2) \\quad\r\n\\quad (1)\\)</span> 也就是说凸函数任意两点的割线位于函数图形上方，\r\n这也是Jensen不等式的两点形式。 <strong>Jensen不等式</strong>\r\n若对于任意点集<span class=\"math inline\">\\({x_i}\\)</span>，若<span class=\"math inline\">\\(\\lambda_i \\geq 0\\)</span>且<span class=\"math inline\">\\(\\sum \\limits_i\\lambda_i =\r\n1\\)</span>，使用数学归纳法，可以<a href=\"(https://zhuanlan.zhihu.com/p/39315786)\">证明</a>凸函数 f (x)\r\n满足： <span class=\"math inline\">\\(f(\\sum \\limits_{i=1}^M\\lambda_ix_i)\r\n\\leq \\sum \\limits_{i=1}^M\\lambda_if(x_i) \\quad \\quad (2)\\)</span>\r\n公式(2)被称为 Jensen 不等式，它是式(1)的泛化形式。</p>\r\n</blockquote>\r\n<p>对log函数取负号即变为凸函数，然后进行缩放，然后左右再同时取负号，可以得到：\r\n<span class=\"math display\">\\[\\begin{aligned}\r\nL(\\theta)-L(\\theta^{i})&amp;=log(\\sum_{Z}P(Z|Y,\\theta^i)\\frac{\r\nP(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^i)})-\\log P(Y|\\theta^i)\\\\\r\n&amp;\\ge \\sum_{Z}\r\nP(Z|Y,\\theta^i)log(\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^i)})-\\log\r\nP(Y|\\theta^i)\\\\ &amp;=\\sum_{Z}\r\nP(Z|Y,\\theta^i)log(\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^i)})-\\sum_{Z}\r\nP(Z|Y,\\theta^i)\\log P(Y|\\theta^i)\\\\ &amp;= \\sum_{Z}\r\nP(Z|Y,\\theta^i)[log\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^i)}-logP(Y|\\theta^{i})]\r\n\\\\ &amp;= \\sum_{Z}\r\nP(Z|Y,\\theta^i)log\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^i)\r\nP(Y|\\theta^{i})} \\end{aligned}\\]</span></p>\r\n<p>我们将缩放后的差值和上一轮迭代的对数似然函数的结果相加，定义如下：\r\n<span class=\"math inline\">\\(\\begin{aligned}B(\\theta,\\theta_i) =\r\nL(\\theta_i) + \\sum_{Z}\r\nP(Z|Y,\\theta^i)log\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^i)\r\nP(Y|\\theta^{i})}\\end{aligned}\\)</span>\r\n我们根据定义，可以得知如下两个结论： (1)<span class=\"math inline\">\\(B(\\theta_i,\\theta_i) = L(\\theta_i)\\)</span>\r\n(2)<span class=\"math inline\">\\(L(\\theta) \\geq\r\nB(\\theta,\\theta_i)\\)</span> 更形象地看，两者的关系可以用下图表示：\r\n<img src=\"/2023/01/15/EM/01/15/EM/BL.png\" class title=\"图片引用方法一\">\r\n其中，对于每次迭代的增值。B函数的增值幅度永远不超过对数似然函数的增值幅度。因此，我们只需要保证迭代每次都可以增加B函数，即可达到通过迭代逐步增加对数似然函数的目的。\r\n因此，第i+1轮的<span class=\"math inline\">\\(\\theta\\)</span>的估计值可以表示为： <span class=\"math inline\">\\(\\begin{aligned}\\theta_{i+1} &amp;= argmax_{\\theta}\r\nB(\\theta,\\theta_i) \\\\ &amp;= argmax_\\theta (L(\\theta_i) + \\sum_{Z}\r\nP(Z|Y,\\theta^i)log\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^i)\r\nP(Y|\\theta^{i})})\\\\ &amp;= argmax_\\theta(L(\\theta_i) -\\sum_{Z}\r\nP(Z|Y,\\theta^i)logP(Z|Y,\\theta^i) P(Y|\\theta^{i}) + \\sum_{Z}\r\nP(Z|Y,\\theta^i)logP(Y|Z,\\theta)P(Z|\\theta))\\\\\r\n&amp;=argmax_\\theta(\\sum_{Z}\r\nP(Z|Y,\\theta^i)logP(Y|Z,\\theta)P(Z|\\theta))\\\\\r\n&amp;=argmax_\\theta(\\sum\\limits_ZP(Z|Y,\\theta_i)logP(Y,Z|\\theta))\\\\\r\n&amp;=argmax_\\theta(Q(\\theta,\\theta^i)) \\end{aligned}\\)</span>\r\n最后一行就是EM算法定义中提到的Q函数。它的期望形式如下： <span class=\"math display\">\\[\\begin{aligned}Q(\\theta,\\theta^i)&amp;=\\sum\\limits_{Z}\r\nP(Z|Y,\\theta^i)logP(Y,Z|\\theta)\\\\\r\n&amp;=E_Z[logP(Y,Z|\\theta)|Y,\\theta^i]\\\\\r\n&amp;=E_{Z|Y,\\theta^i}logP(Y,Z|\\theta) \\end{aligned}\\]</span>\r\n至此，Q函数的来源问题我们也已经解决。下面，我们来进一步证明EM算法可以通过迭代的方式逐步增大对数似然函数。\r\n<strong>证明：</strong><span class=\"math inline\">\\(P(Y|\\theta_{i+1})\r\n\\geq P(Y|\\theta_i)\\)</span></p>\r\n<h2 id=\"应用\">应用</h2>\r\n<ul>\r\n<li><p>隐马尔可夫模型无监督学习</p></li>\r\n<li><p>PLSA</p></li>\r\n<li><p>LDA的变分EM算法</p></li>\r\n</ul>\r\n<h2 id=\"推广\">推广</h2>\r\n<p><strong>F函数</strong>：假设隐变量数据<span class=\"math inline\">\\(Z\\)</span>的概率分布为<span class=\"math inline\">\\(\\widetilde{P}(Z)\\)</span>,定义分布$ <span class=\"math inline\">\\(与参数\\)</span>$的函数: <span class=\"math display\">\\[\r\nF( \\widetilde{P},\\theta) = E_{ \\widetilde{P}}[\\log P(Y,Z|\\theta)] + H(\r\n\\widetilde{P})\r\n\\]</span> 称为<span class=\"math inline\">\\(F\\)</span>函数。式中<span class=\"math inline\">\\(H(\\widetilde{P}) =\r\n-E_{\\widetilde{P}}\\log\\widetilde{P}(Z)\\)</span>是分布<span class=\"math inline\">\\(\\widetilde{P}(Z)\\)</span>的熵。</p>\r\n<h3 id=\"推论em算法的一次迭代可由f函数的极大-极大算法实现\">推论：EM算法的一次迭代可由F函数的极大-极大算法实现。</h3>\r\n<ul>\r\n<li>GEM算法1：直接对F函数中的<span class=\"math inline\">\\(\\widetilde{P}\\)</span>和<span class=\"math inline\">\\(\\theta\\)</span>进行迭代\r\n<ul>\r\n<li>缺点：很难直接找到<span class=\"math inline\">\\(\\theta\\)</span>对用的极大值点</li>\r\n</ul></li>\r\n<li>GME算法2：对Q函数进行迭代\r\n<ul>\r\n<li>优点：只需找到<span class=\"math inline\">\\(\\theta^{i+1}\\)</span>使Q函数变大即可</li>\r\n</ul></li>\r\n<li>GME算法3：对Q函数中的参数，一次进行一个分量的迭代</li>\r\n</ul>\r\n<h2 id=\"理解em算法的几重境界\">理解EM算法的几重境界</h2>\r\n<h3 id=\"em-e-m\">EM = E + M</h3>\r\n<ul>\r\n<li>使用E+M的形式，突破了MLE的限制</li>\r\n</ul>\r\n<h3 id=\"em算法是一种局部下限构造\">EM算法是一种局部下限构造</h3>\r\n<ul>\r\n<li><p>根据EM算法证明，我们可以看出，他是通过构造下限，让下限逐步提高，从而保证似然函数也是逐步提升的</p>\r\n<figure>\r\n<img src=\"https://picx.zhimg.com/50/v2-a38b6748f36f0cb9bcd43b5ca435e5c6_720w.jpg?source=1940ef5c\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure></li>\r\n<li><p>如下图所示，每次都构造一个下限（绿色），然后通过下限的不断提升，保证似然函数的不断提升</p>\r\n<figure>\r\n<img src=\"/2023/01/15/EM/image-20230115193948771.png\" alt=\"image-20230115193948771\">\r\n<figcaption aria-hidden=\"true\">image-20230115193948771</figcaption>\r\n</figure></li>\r\n</ul>\r\n<h3 id=\"k-means是一种hard-em算法\">K-means是一种Hard EM算法</h3>\r\n<figure>\r\n<img src=\"/2023/01/15/EM/image-20220913224805897.png\" alt=\"image-20220913224805897\">\r\n<figcaption aria-hidden=\"true\">image-20220913224805897</figcaption>\r\n</figure>\r\n<ul>\r\n<li><p>K-Means算法对数据点的聚类进行了“硬分配”，即每个数据点只属于唯一的聚类；而GMM的EM解法则基于后验概率分布，对数据点进行“软分配”，即每个单独的高斯模型对数据聚类都有贡献，不过贡献值有大有小。</p></li>\r\n<li><p>而其实，我们可以将K-Means算法归类为GMM的EM解法的一个特例。</p>\r\n<figure>\r\n<img src=\"https://pic2.zhimg.com/50/v2-1c1d5d2b658b4e9580a051f955df7b48_720w.jpg?source=1940ef5c\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure></li>\r\n</ul>\r\n<h4 id=\"关于初始化\">关于初始化</h4>\r\n<ul>\r\n<li>随机选取一个点作为第一个聚类中心。</li>\r\n<li>计算所有样本与第一个聚类中心的距离。</li>\r\n<li>选择出上一步中距离最大的点作为第二个聚类中心。</li>\r\n<li>迭代：计算所有点到与之最近的聚类中心的距离，选取最大距离的点作为新的聚类中心。</li>\r\n<li>终止条件：直到选出了这k个中心。</li>\r\n</ul>\r\n<h4 id=\"类比k-meansem算法初始化\">类比K-means,EM算法初始化</h4>\r\n<ul>\r\n<li><p>采用一种基于网格的聚类算法来初始化EM算法</p>\r\n<figure>\r\n<img src=\"https://images2015.cnblogs.com/blog/797505/201604/797505-20160401131828238-1041936013.png\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure>\r\n<figure>\r\n<img src=\"https://images2015.cnblogs.com/blog/797505/201604/797505-20160401131856191-574167681.png\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure></li>\r\n<li><p>我觉得这篇论文的主要思想应该是这样的：就拿身高举例。它就是首先做一个预处理，将身高在一个范围内（例如1.71至1.74）的分成一个网格，再看这个网格占全部数据的多少，以此判断出该网格为高密度还是低密度，然后循环算出所有网格的，再使用EM算法计算哪些高密度网格，这样会使整个算法收敛的快一些。还有一些其他的论文也是讲的这个。</p></li>\r\n</ul>\r\n<h4 id=\"进一步理解隐变量是存在一种分布的\">进一步理解：隐变量是存在一种分布的</h4>\r\n<figure>\r\n<img src=\"https://picx.zhimg.com/50/v2-1cb36ffe3e6b918080b16627389395a5_720w.jpg?source=1940ef5c\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure>\r\n<h3 id=\"em-是广义em的特例从隐变量到隐分布\">EM\r\n是广义EM的特例：从隐变量到隐分布</h3>\r\n<ul>\r\n<li><p>引入隐分布来再次证明EM算法</p>\r\n<figure>\r\n<img src=\"https://pica.zhimg.com/50/v2-866e11172dc0fba6daefa9f370411b11_720w.jpg?source=1940ef5c\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure></li>\r\n<li><p>我们把Jensen不等收右边的部分定义为<strong>自由能</strong>，那么<strong>E步骤是固定参数优化隐分布，\r\nM步骤是固定隐分布优化参数，这就是广义EM算法了</strong>。</p>\r\n<figure>\r\n<img src=\"https://pica.zhimg.com/50/v2-4240a9b9e33693ff67024bd96821e2f7_720w.jpg?source=1940ef5c\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure></li>\r\n<li><p>有了广义EM算法之后， 我们对自由能深入挖掘，\r\n发现自由能和似然度和KL距离之间的关系：</p>\r\n<figure>\r\n<img src=\"https://pica.zhimg.com/50/v2-47d2d736c98ab6bf95e56f66620f3fc7_720w.jpg?source=1940ef5c\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure>\r\n<blockquote>\r\n<p><strong>KL距离</strong></p>\r\n<ul>\r\n<li><p>全称：Kullback-Leibler差异（Kullback-Leibler\r\ndivergence）</p></li>\r\n<li><p>又称：相对熵（relative entropy）</p></li>\r\n<li><p>数学本质：衡量<strong>相同事件空间</strong>里<strong>两个概率分布</strong>相对<strong>差距</strong>的<strong>测度</strong></p></li>\r\n<li><p>定义; <span class=\"math display\">\\[\r\nD(p||q) = \\sum_{x \\in X}p(x)\\log \\frac{p(x)}{q(x)}\r\n\\]</span></p></li>\r\n<li><p>约定：$0log(0/q)=0、p l o g ( p / 0 ) = $</p></li>\r\n<li><p>等价形式： <span class=\"math display\">\\[\r\nD(p||q) = E_p[\\log \\frac{p(X)}{q(X)}]\r\n\\]</span></p></li>\r\n<li><p>说明：</p>\r\n<ul>\r\n<li>两个概率分布差距越大，KL距离越大</li>\r\n<li>两个概率分布差距越小，KL距离为0</li>\r\n</ul></li>\r\n</ul>\r\n</blockquote></li>\r\n<li><p>所以固定参数的情况下， 那么只能最优化KL距离了，\r\n那么隐分布只能取如下分布：</p>\r\n<figure>\r\n<img src=\"https://pic2.zhimg.com/50/v2-28aa54c91428fa32a93fe7243034e70f_720w.jpg?source=1940ef5c\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure></li>\r\n<li><p>而这个<strong>在EM算法里面是直接给出的</strong>。\r\n所以EM算法是广义EM算法的天然最优的隐分布情况。\r\n<strong>但是很多时候隐分布不是那么容易计算的！</strong></p>\r\n<p>前面的推理虽然很简单， 但是要理解到位真心不容易，\r\n首先要<strong>深入理解KL距离是如何被引入的？</strong></p>\r\n<figure>\r\n<img src=\"https://pic3.zhimg.com/50/v2-5fb1549c245298846063a9742cb11e1a_720w.jpg?source=1940ef5c\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure></li>\r\n<li><p>其次要理解， 为什么传统的EM算法，\r\n<strong>不存在第一个最优化</strong>？因为在<strong>没有限制的隐分布（天然情况下）</strong>情况下，\r\n第一个最优就是要求：</p>\r\n<figure>\r\n<img src=\"https://pic3.zhimg.com/50/v2-227eccdac2131d6e0538d83515444624_720w.jpg?source=1940ef5c\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure></li>\r\n<li><p>而这个隐分布， EM算法里面是直接给出的，而不是让你证明得到的。</p>\r\n<figure>\r\n<img src=\"https://pic1.zhimg.com/50/v2-2004ef50d0796d32e6cd61ac7a6cdf2d_720w.jpg?source=1940ef5c\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure></li>\r\n</ul>\r\n","tags":["EM算法"]}]