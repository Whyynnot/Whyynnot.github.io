[{"title":"EM","url":"/2023/01/15/EM/","content":"<h1 id=\"em算法\">EM算法</h1>\r\n<blockquote>\r\n<p>最大期望演算法（Expectation-maximization\r\nalgorithm，又譯期望最大化算法）在统计中被用于寻找，依赖于不可观察的隐性变量的概率模型中，参数的最大似然估计。本文将从EM算法的背景、原理和应用三个层面对该算法进行细致的讲解。\r\n<span id=\"more\"></span></p>\r\n</blockquote>\r\n<h2 id=\"引入高斯混合模型gmm\">引入——高斯混合模型（GMM）</h2>\r\n<p><strong>定义</strong>：高斯混合模型是指具有<span class=\"math inline\">\\(P(y|\\theta) = \\sum \\limits_{k=1}^K\r\n\\alpha_k\\phi(y|\\theta_k)\\)</span>形式的概率分布模型。其中<span class=\"math inline\">\\(\\alpha_k \\geq 0\\)</span>且<span class=\"math inline\">\\(\\sum \\limits_{k=1}^K \\alpha_k= 1\\)</span>。<span class=\"math inline\">\\(\\phi(y|\\theta_k)\\)</span>是高斯分布密度，<span class=\"math inline\">\\(\\theta_k=(\\mu_k,\\sigma_k^2)\\)</span>，</p>\r\n<p><span class=\"math display\">\\[\\begin{aligned}\r\n\\phi(y|\\theta_k)=\\frac{1}{\\sqrt{2\\pi}\\sigma_k}exp(-\\frac{(y-\\mu_k)^2}{2\\sigma_k^2})\r\n\\end{aligned}\\]</span> 称为第k个分模型。\r\n<strong>GMM参数估计的EM算法</strong> 输入：观测数据<span class=\"math inline\">\\(y_1,y_2,...,y_N\\)</span>，高斯混合模型；\r\n输出：高斯混合模型的参数</p>\r\n<ul>\r\n<li><p>取参数初始值开始迭代</p></li>\r\n<li><p>E步：根据当前模型参数，计算分模型k对观测数据<span class=\"math inline\">\\(y_j\\)</span>的响应度 <span class=\"math inline\">\\(\\begin{aligned}\\hat\\gamma_{jk} =\r\n\\frac{\\alpha_k\\phi(y_j|\\theta_k)}{\\sum\r\n\\limits_{k=1}^K\\alpha_k\\phi(y_j|\\theta_k)}\\end{aligned}\\)</span></p></li>\r\n<li><p>M步：计算新一轮的模型参数 <span class=\"math inline\">\\(n_k = \\sum\r\n\\limits_{j=1}^NE_{\\gamma_{jk}}\\)</span></p>\r\n<p><span class=\"math inline\">\\(\\begin{aligned}\\hat\\mu_k = \\frac{\\sum\r\n\\limits_{j=1}^N\\hat\\gamma_{jk}y_j}{\\sum\r\n\\limits_{j=1}^N\\hat\\gamma_{jk}}\\end{aligned}\\)</span></p>\r\n<p><span class=\"math inline\">\\(\\begin{aligned}\\hat\\sigma_k^2 =\r\n\\frac{\\sum \\limits_{j=1}^N\\hat\\gamma_{jk}(y_j-\\mu_k)^2}{\\sum\r\n\\limits_{j=1}^N\\hat\\gamma_{jk}}\\end{aligned}\\)</span></p></li>\r\n<li><p>重复以上两步，直到收敛。</p></li>\r\n</ul>\r\n<p>根据这个算法，我们使用python实现GMM中参数估计的EM算法，并三次迭代过程的运行结果依次进行展示。其中，实线部分代表真实的概率分布。虚线部分代表当前参数对应的概率分布。</p>\r\n<p><strong>例子解析</strong>\r\n通过这个例子，我们可以总结出EM算法的基本使用步骤： (1)选取参数初始值\r\n(2)E步：根据当前参数值，计算一个函数值\r\n(3)M步：根据计算出的函数值，更新参数估计值 (4)重复(2)(3)直到收敛\r\n但是，我们仍然存在很多疑问：</p>\r\n<ul>\r\n<li>为什么要使用EM算法？</li>\r\n<li>EM算法为什么可行？</li>\r\n<li>EM算法中的参数应当如何进行初始化？</li>\r\n<li>E步中需要根据当前参数值去求哪个函数的值？</li>\r\n<li>M步中参数应该如何根据E步求出的函数值进行估计？</li>\r\n<li>收敛的判断标准是什么？</li>\r\n</ul>\r\n<p>带着这些疑问，我们进行进一步的EM算法的产生背景和原理的学习。解决完这些疑问后，本文将进一步展示EM算法的应用实例，帮助读者真正掌握EM算法的使用流程。</p>\r\n<h2 id=\"背景\">背景</h2>\r\n<p><strong>基本概念</strong></p>\r\n<ul>\r\n<li>概率模型：是用来描述不同随机变量之间关系的数学模型，通常情况下刻画了一个或多个随机变量之间的相互非确定性的概率关系。从数学上讲，该模型通常被表达为(Y,P)，其中Y是观测集合用来描述可能的观测结果，P是Y对应的概率分布函数或密度函数集合。最常见的概率分布函数是参数模型，它是由有限维参数构成的分布集合。</li>\r\n<li>观测变量是指在试验中可以通过观测直接得到的随机变量。</li>\r\n<li>隐变量指的是不可观测的随机变量。隐变量可以通过使用数学模型依据观测得的数据被推断出来。</li>\r\n</ul>\r\n<p>如果使用概率模型来对观测变量进行描述，在选定概率模型后，需要解决的问题就变成了对概率模型的参数的估计。其中，最常见并且也是应用最广泛的方法是极大似然估计法（英語：Maximum\r\nLikelihood Estimation，簡作MLE）。</p>\r\n<ul>\r\n<li>MLE算法\r\n<ul>\r\n<li>基本思想\r\n<ul>\r\n<li>选定一组参数，使得在该参数条件下，观测变量存在的概率最大</li>\r\n</ul></li>\r\n<li>算法流程\r\n<ul>\r\n<li>写出模型的对数似然函数</li>\r\n<li>求参数的梯度，并使梯度为0，求解极大值点，作为参数的估计值</li>\r\n</ul></li>\r\n</ul></li>\r\n</ul>\r\n<p>对于只存在观测变量的概率模型，使用极大似然估计法往往就可以很好地估计参数。但是，当概率模型中引入隐变量的时候，MLE就难以按照上述流程解决问题了。</p>\r\n<p><strong>典型问题</strong>\r\n抛硬币A,B,C，正面的概率分别为π,p,q。先抛A，当A的结果是正面时抛B，反之抛C。记录第二次抛硬币的结果，独立重复n次实验。\r\n根据上述流程，写出其似然函数如下： <span class=\"math inline\">\\(P(Y|\\theta)=\\prod \\limits_{j=1}^n[\\pi\r\np^{y_j}(1-p)^{1-y_j} + (1-\\pi)q^{y_j}(1-q)^{1-y_j}]\\)</span>\r\n由于隐变量的存在，导致对该似然函数求极值点，无解析解。针对MLE中存在的这种问题，EM算法产生，并有效地解决了这类问题。\r\n<strong>EM算法</strong>\r\n输入：观测变量数据Y，隐变量数据Z，联合分布P(Y,Z|θ)，条件分布P(Z|Y,θ)；\r\n输出：模型参数估计值θ (1)选择初始参数\r\n(2)E：根据当前参数θi的估计值，计算Q(θ,θi)\r\n(3)M：求使Q(θ,θi)最大的估计值θ，作为新一轮的参数估计值θi+1\r\n(4)重复(2)和(3)，直至收敛（参数估计值变化量小于预设阈值或者Q函数值变化量小于预设阈值）</p>\r\n<p>由于在隐变量存在时，概率模型的参数估计问题没有解析解。因此，EM算法产生。它试图通过一种迭代的方式去逼近最佳估计。\r\n至此，我们已经成功解决了EM算法为什么会产生的疑问。引入中的其他疑问，将会在原理中进一步解释。</p>\r\n<h2 id=\"原理\">原理</h2>\r\n<p><strong>Q函数的选取</strong>\r\n在刚刚提到的EM算法的定义中，我们看到在E步和M步都涉及到一个函数:<span class=\"math inline\">\\(Q(\\theta,\\theta_i)\\)</span>。那么这个函数是如何得到的？\r\n我们知道，EM算法的产生是为了解决MLE的局限性。但是其思想是一致的，最大化似然概率。\r\n<span class=\"math inline\">\\(P(Y|\\theta) = \\sum\r\n\\limits_ZP(Y,Z|\\theta)\\)</span> 最大化似然概率又等价于最大化对数似然概率\r\n<span class=\"math inline\">\\(L(\\theta) = log(\\sum\r\n\\limits_ZP(Y,Z|\\theta))\\)</span>\r\n而EM算法通过迭代逐步逼近最佳参数估计，因此我们考量两次相邻迭代之间的差：\r\n<span class=\"math inline\">\\(\\begin{aligned}L(\\theta)-L(\\theta^{(i)})\r\n&amp;= log(\\sum \\limits_{Z}P(Y|Z,\\theta)P(Z|\\theta)) -\r\nlogP(Y|\\theta^{(i)}) \\\\ &amp;= log(\\sum\r\n\\limits_{Z}P(Z|Y,\\theta^{(i)})\\frac{P(Y|Z,\\theta)P(Z|\\theta))}{P(Z|Y,\\theta^{(i)})}\r\n- logP(Y|\\theta^{(i)})\\end{aligned}\\)</span>\r\n这里，我们需要利用Jessen不等式进行进一步的缩放。</p>\r\n<blockquote>\r\n<p>Jessen不等式证明： <strong>凸函数</strong>\r\n凸函数是一个定义在某个向量空间的凸子集 C（区间）上的实值函数\r\nf，如果在其定义域 C 上的任意两点<span class=\"math inline\">\\(x_1,x_2,0\r\n\\leq t \\leq 1\\)</span>，有 <span class=\"math inline\">\\(tf(x_1)+(1-t)f(x_2) \\geq f(tx_1+(1-t)x_2) \\quad\r\n\\quad (1)\\)</span> 也就是说凸函数任意两点的割线位于函数图形上方，\r\n这也是Jensen不等式的两点形式。 <strong>Jensen不等式</strong>\r\n若对于任意点集<span class=\"math inline\">\\({x_i}\\)</span>，若<span class=\"math inline\">\\(\\lambda_i \\geq 0\\)</span>且<span class=\"math inline\">\\(\\sum \\limits_i\\lambda_i =\r\n1\\)</span>，使用数学归纳法，可以<a href=\"(https://zhuanlan.zhihu.com/p/39315786)\">证明</a>凸函数 f (x)\r\n满足： <span class=\"math inline\">\\(f(\\sum \\limits_{i=1}^M\\lambda_ix_i)\r\n\\leq \\sum \\limits_{i=1}^M\\lambda_if(x_i) \\quad \\quad (2)\\)</span>\r\n公式(2)被称为 Jensen 不等式，它是式(1)的泛化形式。</p>\r\n</blockquote>\r\n<p>对log函数取负号即变为凸函数，然后进行缩放，然后左右再同时取负号，可以得到：\r\n<span class=\"math display\">\\[\\begin{aligned}\r\nL(\\theta)-L(\\theta^{i})&amp;=log(\\sum_{Z}P(Z|Y,\\theta^i)\\frac{\r\nP(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^i)})-\\log P(Y|\\theta^i)\\\\\r\n&amp;\\ge \\sum_{Z}\r\nP(Z|Y,\\theta^i)log(\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^i)})-\\log\r\nP(Y|\\theta^i)\\\\ &amp;=\\sum_{Z}\r\nP(Z|Y,\\theta^i)log(\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^i)})-\\sum_{Z}\r\nP(Z|Y,\\theta^i)\\log P(Y|\\theta^i)\\\\ &amp;= \\sum_{Z}\r\nP(Z|Y,\\theta^i)[log\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^i)}-logP(Y|\\theta^{i})]\r\n\\\\ &amp;= \\sum_{Z}\r\nP(Z|Y,\\theta^i)log\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^i)\r\nP(Y|\\theta^{i})} \\end{aligned}\\]</span></p>\r\n<p>我们将缩放后的差值和上一轮迭代的对数似然函数的结果相加，定义如下：\r\n<span class=\"math inline\">\\(\\begin{aligned}B(\\theta,\\theta_i) =\r\nL(\\theta_i) + \\sum_{Z}\r\nP(Z|Y,\\theta^i)log\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^i)\r\nP(Y|\\theta^{i})}\\end{aligned}\\)</span>\r\n我们根据定义，可以得知如下两个结论： (1)<span class=\"math inline\">\\(B(\\theta_i,\\theta_i) = L(\\theta_i)\\)</span>\r\n(2)<span class=\"math inline\">\\(L(\\theta) \\geq\r\nB(\\theta,\\theta_i)\\)</span> 更形象地看，两者的关系可以用下图表示：\r\n<img src=\"/2023/01/15/EM/01/15/EM/BL.png\" class title=\"图片引用方法一\">\r\n其中，对于每次迭代的增值。B函数的增值幅度永远不超过对数似然函数的增值幅度。因此，我们只需要保证迭代每次都可以增加B函数，即可达到通过迭代逐步增加对数似然函数的目的。\r\n因此，第i+1轮的<span class=\"math inline\">\\(\\theta\\)</span>的估计值可以表示为： <span class=\"math inline\">\\(\\begin{aligned}\\theta_{i+1} &amp;= argmax_{\\theta}\r\nB(\\theta,\\theta_i) \\\\ &amp;= argmax_\\theta (L(\\theta_i) + \\sum_{Z}\r\nP(Z|Y,\\theta^i)log\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^i)\r\nP(Y|\\theta^{i})})\\\\ &amp;= argmax_\\theta(L(\\theta_i) -\\sum_{Z}\r\nP(Z|Y,\\theta^i)logP(Z|Y,\\theta^i) P(Y|\\theta^{i}) + \\sum_{Z}\r\nP(Z|Y,\\theta^i)logP(Y|Z,\\theta)P(Z|\\theta))\\\\\r\n&amp;=argmax_\\theta(\\sum_{Z}\r\nP(Z|Y,\\theta^i)logP(Y|Z,\\theta)P(Z|\\theta))\\\\\r\n&amp;=argmax_\\theta(\\sum\\limits_ZP(Z|Y,\\theta_i)logP(Y,Z|\\theta))\\\\\r\n&amp;=argmax_\\theta(Q(\\theta,\\theta^i)) \\end{aligned}\\)</span>\r\n最后一行就是EM算法定义中提到的Q函数。它的期望形式如下： <span class=\"math display\">\\[\\begin{aligned}Q(\\theta,\\theta^i)&amp;=\\sum\\limits_{Z}\r\nP(Z|Y,\\theta^i)logP(Y,Z|\\theta)\\\\\r\n&amp;=E_Z[logP(Y,Z|\\theta)|Y,\\theta^i]\\\\\r\n&amp;=E_{Z|Y,\\theta^i}logP(Y,Z|\\theta) \\end{aligned}\\]</span>\r\n至此，Q函数的来源问题我们也已经解决。下面，我们来进一步证明EM算法可以通过迭代的方式逐步增大对数似然函数。\r\n<strong>证明：</strong><span class=\"math inline\">\\(P(Y|\\theta_{i+1})\r\n\\geq P(Y|\\theta_i)\\)</span></p>\r\n<h2 id=\"应用\">应用</h2>\r\n<ul>\r\n<li><p>隐马尔可夫模型无监督学习</p></li>\r\n<li><p>PLSA</p></li>\r\n<li><p>LDA的变分EM算法</p></li>\r\n</ul>\r\n<h2 id=\"推广\">推广</h2>\r\n<p><strong>F函数</strong>：假设隐变量数据<span class=\"math inline\">\\(Z\\)</span>的概率分布为<span class=\"math inline\">\\(\\widetilde{P}(Z)\\)</span>,定义分布$ <span class=\"math inline\">\\(与参数\\)</span>$的函数: <span class=\"math display\">\\[\r\nF( \\widetilde{P},\\theta) = E_{ \\widetilde{P}}[\\log P(Y,Z|\\theta)] + H(\r\n\\widetilde{P})\r\n\\]</span> 称为<span class=\"math inline\">\\(F\\)</span>函数。式中<span class=\"math inline\">\\(H(\\widetilde{P}) =\r\n-E_{\\widetilde{P}}\\log\\widetilde{P}(Z)\\)</span>是分布<span class=\"math inline\">\\(\\widetilde{P}(Z)\\)</span>的熵。</p>\r\n<h3 id=\"推论em算法的一次迭代可由f函数的极大-极大算法实现\">推论：EM算法的一次迭代可由F函数的极大-极大算法实现。</h3>\r\n<ul>\r\n<li>GEM算法1：直接对F函数中的<span class=\"math inline\">\\(\\widetilde{P}\\)</span>和<span class=\"math inline\">\\(\\theta\\)</span>进行迭代\r\n<ul>\r\n<li>缺点：很难直接找到<span class=\"math inline\">\\(\\theta\\)</span>对用的极大值点</li>\r\n</ul></li>\r\n<li>GME算法2：对Q函数进行迭代\r\n<ul>\r\n<li>优点：只需找到<span class=\"math inline\">\\(\\theta^{i+1}\\)</span>使Q函数变大即可</li>\r\n</ul></li>\r\n<li>GME算法3：对Q函数中的参数，一次进行一个分量的迭代</li>\r\n</ul>\r\n<h2 id=\"理解em算法的几重境界\">理解EM算法的几重境界</h2>\r\n<h3 id=\"em-e-m\">EM = E + M</h3>\r\n<ul>\r\n<li>使用E+M的形式，突破了MLE的限制</li>\r\n</ul>\r\n<h3 id=\"em算法是一种局部下限构造\">EM算法是一种局部下限构造</h3>\r\n<ul>\r\n<li><p>根据EM算法证明，我们可以看出，他是通过构造下限，让下限逐步提高，从而保证似然函数也是逐步提升的</p>\r\n<figure>\r\n<img src=\"https://picx.zhimg.com/50/v2-a38b6748f36f0cb9bcd43b5ca435e5c6_720w.jpg?source=1940ef5c\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure></li>\r\n<li><p>如下图所示，每次都构造一个下限（绿色），然后通过下限的不断提升，保证似然函数的不断提升</p>\r\n<figure>\r\n<img src=\"/2023/01/15/EM/image-20230115193948771.png\" alt=\"image-20230115193948771\">\r\n<figcaption aria-hidden=\"true\">image-20230115193948771</figcaption>\r\n</figure></li>\r\n</ul>\r\n<h3 id=\"k-means是一种hard-em算法\">K-means是一种Hard EM算法</h3>\r\n<figure>\r\n<img src=\"/2023/01/15/EM/image-20220913224805897.png\" alt=\"image-20220913224805897\">\r\n<figcaption aria-hidden=\"true\">image-20220913224805897</figcaption>\r\n</figure>\r\n<ul>\r\n<li><p>K-Means算法对数据点的聚类进行了“硬分配”，即每个数据点只属于唯一的聚类；而GMM的EM解法则基于后验概率分布，对数据点进行“软分配”，即每个单独的高斯模型对数据聚类都有贡献，不过贡献值有大有小。</p></li>\r\n<li><p>而其实，我们可以将K-Means算法归类为GMM的EM解法的一个特例。</p>\r\n<figure>\r\n<img src=\"https://pic2.zhimg.com/50/v2-1c1d5d2b658b4e9580a051f955df7b48_720w.jpg?source=1940ef5c\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure></li>\r\n</ul>\r\n<h4 id=\"关于初始化\">关于初始化</h4>\r\n<ul>\r\n<li>随机选取一个点作为第一个聚类中心。</li>\r\n<li>计算所有样本与第一个聚类中心的距离。</li>\r\n<li>选择出上一步中距离最大的点作为第二个聚类中心。</li>\r\n<li>迭代：计算所有点到与之最近的聚类中心的距离，选取最大距离的点作为新的聚类中心。</li>\r\n<li>终止条件：直到选出了这k个中心。</li>\r\n</ul>\r\n<h4 id=\"类比k-meansem算法初始化\">类比K-means,EM算法初始化</h4>\r\n<ul>\r\n<li><p>采用一种基于网格的聚类算法来初始化EM算法</p>\r\n<figure>\r\n<img src=\"/2023/01/15/EM/797505-20160401131828238-1041936013.png\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure>\r\n<figure>\r\n<img src=\"https://images2015.cnblogs.com/blog/797505/201604/797505-20160401131856191-574167681.png\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure></li>\r\n<li><p>我觉得这篇论文的主要思想应该是这样的：就拿身高举例。它就是首先做一个预处理，将身高在一个范围内（例如1.71至1.74）的分成一个网格，再看这个网格占全部数据的多少，以此判断出该网格为高密度还是低密度，然后循环算出所有网格的，再使用EM算法计算哪些高密度网格，这样会使整个算法收敛的快一些。还有一些其他的论文也是讲的这个。</p></li>\r\n</ul>\r\n<h4 id=\"进一步理解隐变量是存在一种分布的\">进一步理解：隐变量是存在一种分布的</h4>\r\n<figure>\r\n<img src=\"https://picx.zhimg.com/50/v2-1cb36ffe3e6b918080b16627389395a5_720w.jpg?source=1940ef5c\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure>\r\n<h3 id=\"em-是广义em的特例从隐变量到隐分布\">EM\r\n是广义EM的特例：从隐变量到隐分布</h3>\r\n<ul>\r\n<li><p>引入隐分布来再次证明EM算法</p>\r\n<figure>\r\n<img src=\"https://pica.zhimg.com/50/v2-866e11172dc0fba6daefa9f370411b11_720w.jpg?source=1940ef5c\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure></li>\r\n<li><p>我们把Jensen不等收右边的部分定义为<strong>自由能</strong>，那么<strong>E步骤是固定参数优化隐分布，\r\nM步骤是固定隐分布优化参数，这就是广义EM算法了</strong>。</p>\r\n<figure>\r\n<img src=\"https://pica.zhimg.com/50/v2-4240a9b9e33693ff67024bd96821e2f7_720w.jpg?source=1940ef5c\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure></li>\r\n<li><p>有了广义EM算法之后， 我们对自由能深入挖掘，\r\n发现自由能和似然度和KL距离之间的关系：</p>\r\n<figure>\r\n<img src=\"https://pica.zhimg.com/50/v2-47d2d736c98ab6bf95e56f66620f3fc7_720w.jpg?source=1940ef5c\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure>\r\n<blockquote>\r\n<p><strong>KL距离</strong></p>\r\n<ul>\r\n<li><p>全称：Kullback-Leibler差异（Kullback-Leibler\r\ndivergence）</p></li>\r\n<li><p>又称：相对熵（relative entropy）</p></li>\r\n<li><p>数学本质：衡量<strong>相同事件空间</strong>里<strong>两个概率分布</strong>相对<strong>差距</strong>的<strong>测度</strong></p></li>\r\n<li><p>定义; <span class=\"math display\">\\[\r\nD(p||q) = \\sum_{x \\in X}p(x)\\log \\frac{p(x)}{q(x)}\r\n\\]</span></p></li>\r\n<li><p>约定：$0log(0/q)=0、p l o g ( p / 0 ) = $</p></li>\r\n<li><p>等价形式： <span class=\"math display\">\\[\r\nD(p||q) = E_p[\\log \\frac{p(X)}{q(X)}]\r\n\\]</span></p></li>\r\n<li><p>说明：</p>\r\n<ul>\r\n<li>两个概率分布差距越大，KL距离越大</li>\r\n<li>两个概率分布差距越小，KL距离为0</li>\r\n</ul></li>\r\n</ul>\r\n</blockquote></li>\r\n<li><p>所以固定参数的情况下， 那么只能最优化KL距离了，\r\n那么隐分布只能取如下分布：</p>\r\n<figure>\r\n<img src=\"https://pic2.zhimg.com/50/v2-28aa54c91428fa32a93fe7243034e70f_720w.jpg?source=1940ef5c\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure></li>\r\n<li><p>而这个<strong>在EM算法里面是直接给出的</strong>。\r\n所以EM算法是广义EM算法的天然最优的隐分布情况。\r\n<strong>但是很多时候隐分布不是那么容易计算的！</strong></p>\r\n<p>前面的推理虽然很简单， 但是要理解到位真心不容易，\r\n首先要<strong>深入理解KL距离是如何被引入的？</strong></p>\r\n<figure>\r\n<img src=\"https://pic3.zhimg.com/50/v2-5fb1549c245298846063a9742cb11e1a_720w.jpg?source=1940ef5c\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure></li>\r\n<li><p>其次要理解， 为什么传统的EM算法，\r\n<strong>不存在第一个最优化</strong>？因为在<strong>没有限制的隐分布（天然情况下）</strong>情况下，\r\n第一个最优就是要求：</p>\r\n<figure>\r\n<img src=\"https://pic3.zhimg.com/50/v2-227eccdac2131d6e0538d83515444624_720w.jpg?source=1940ef5c\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure></li>\r\n<li><p>而这个隐分布， EM算法里面是直接给出的，而不是让你证明得到的。</p>\r\n<figure>\r\n<img src=\"https://pic1.zhimg.com/50/v2-2004ef50d0796d32e6cd61ac7a6cdf2d_720w.jpg?source=1940ef5c\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure></li>\r\n</ul>\r\n","tags":["EM算法"]},{"title":"GraphConstructionMethods4NLP","url":"/2023/01/16/GraphConstructionMethods4NLP/","content":"<h1 id=\"gnn-in-nlp-2\">GNN in NLP 2</h1>\r\n<blockquote>\r\n<p>GNN in\r\nNLP介绍的第二部分，延续上一部分内容。继续介绍三个子部分内容：</p>\r\n<ul>\r\n<li>Graph Construction Methods</li>\r\n<li>Graph Representation Learning</li>\r\n<li>Graph Based Encoder-Decoder Model</li>\r\n</ul>\r\n<span id=\"more\"></span>\r\n</blockquote>\r\n<h2 id=\"graph-construction-methods-for-natural-language-processing\">Graph\r\nConstruction Methods for Natural Language Processing</h2>\r\n<p>对绝大多数的NLP任务，典型的输入是一个文本序列，而不是图。因此，为了利用\r\nGNN 的强大功能，如何从文本序列构建图形输入成为一个艰巨的步骤。</p>\r\n<h3 id=\"static-graph-construction\">Static Graph Construction</h3>\r\n<p>静态图构建方法旨在通过利用现有的<strong>关系解析工具</strong>（例如，依赖解析）或手动定义的<strong>规则</strong>在预处理期间构建图结构。从概念上讲，静态图结合了隐藏在原始文本序列中的不同<strong>领域知识/外部知识</strong>，从而为原始文本增加了丰富的<strong>结构化信息</strong>。</p>\r\n<p>我们假设输入是一个文档<span class=\"math inline\">\\(doc =\r\n\\{para_1,para_2,...,para_n\\}\\)</span>，它由n个表示为para的段落组成。类似的，一个段落由m个句子组成<span class=\"math inline\">\\(para_i =\r\n\\{sent_1,sent_2,...,sent_m\\}\\)</span>每个句子由l个单词组成<span class=\"math inline\">\\(sent_i = \\{w_1,w_2,...,w_l\\}\\)</span></p>\r\n<p><strong>STATIC GRAPH CONSTRUCTION APPROACHES</strong></p>\r\n<ul>\r\n<li><p>Dependency Graph Construction</p>\r\n<figure>\r\n<img src=\"/2023/01/16/GraphConstructionMethods4NLP/image-20221013115352600.png\" alt=\"image-20221013115352600\">\r\n<figcaption aria-hidden=\"true\">image-20221013115352600</figcaption>\r\n</figure>\r\n<p>省流：</p>\r\n<ul>\r\n<li>节点：单词</li>\r\n<li>边：\r\n<ul>\r\n<li>依赖关系的边</li>\r\n<li>词在初始输入中的顺序的边（相邻则添加无向边）</li>\r\n</ul></li>\r\n</ul></li>\r\n<li><p>Constituency Graph Construction</p>\r\n<figure>\r\n<img src=\"/2023/01/16/GraphConstructionMethods4NLP/image-20221013115412629.png\" alt=\"image-20221013115412629\">\r\n<figcaption aria-hidden=\"true\">image-20221013115412629</figcaption>\r\n</figure>\r\n<p>省流：</p>\r\n<ul>\r\n<li>节点：\r\n<ul>\r\n<li>非终结节点<span class=\"math inline\">\\(V_{nt}\\)</span></li>\r\n<li>终结节点<span class=\"math inline\">\\(V_{words}\\)</span></li>\r\n</ul></li>\r\n<li>边：<span class=\"math inline\">\\(\\textbf{R}_{cons} \\subseteq\r\n\\textbf{V}_{nt} \\times (\\textbf{V}_{nt}+\\textbf{V}_{words})\\)</span>\r\n<ul>\r\n<li>选取关系的边，有向边从非终结节点指向终结节点，可以一对多（短语级别的关系）</li>\r\n<li>初始输入顺序的边仍然添加在相邻单词之间的无向边</li>\r\n</ul></li>\r\n</ul></li>\r\n<li><p>AMR Graph Construction</p>\r\n<figure>\r\n<img src=\"/2023/01/16/GraphConstructionMethods4NLP/image-20221013120751635.png\" alt=\"image-20221013120751635\">\r\n<figcaption aria-hidden=\"true\">image-20221013120751635</figcaption>\r\n</figure>\r\n<blockquote>\r\n<p>这是一个AMR图的示例，原始的句子是：\"Pual’s description of himself: a\r\nfighter\"。</p>\r\n</blockquote>\r\n<p>AMR图是<strong>有根、有标签、有向、无环图</strong>，广泛用于表示非结构化和具体自然文本的抽象概念之间的高级<strong>语义关系</strong>。</p>\r\n<p>AMR图表示的是句子的高级语义抽象。语义相似的不同句子可能会共享同样的AMR图。</p>\r\n<p>省流：AMR 图 G(V, E) 是有根的、标记的、有向的、无环图\r\n(DAG)，由下面讨论的 AMR 节点和 AMR 关系组成。</p>\r\n<ul>\r\n<li>节点：\r\n<ul>\r\n<li><strong>名称（name）</strong>（例如“Paul”）是节点实例的特定值</li>\r\n<li><strong>概念（concept）</strong>可以是英文单词（例如“boy”）、PropBank\r\n框架集（Kingsbury 和 Palmer，2002\r\n年）（例如“want-01”）或特殊关键字。</li>\r\n<li><strong>名称</strong>节点是唯一身份，而<strong>概念</strong>节点由不同的实例共享。</li>\r\n</ul></li>\r\n<li>边:连接节点的边称为关系（例如 :ARG0 和\r\n:name）。可以从带边的节点对中提取这些 AMR 关系，表示为<span class=\"math inline\">\\((n_i,r_{i,j},n_j)\\in R_{amr}\\)</span>\r\n<ul>\r\n<li>对每一个三元组，设置从节点<span class=\"math inline\">\\(n_i\\)</span>指向<span class=\"math inline\">\\(n_j\\)</span>的有向边，其边的类型为<span class=\"math inline\">\\(r_{i,j}\\)</span></li>\r\n</ul></li>\r\n</ul></li>\r\n<li><p>Information Extraction Graph Construction</p>\r\n<figure>\r\n<img src=\"/2023/01/16/GraphConstructionMethods4NLP/image-20221013151356999.png\" alt=\"image-20221013151356999\">\r\n<figcaption aria-hidden=\"true\">image-20221013151356999</figcaption>\r\n</figure>\r\n<blockquote>\r\n<p>一个 IE 图构建的例子，它包含<strong>共同引用过程(Co-reference\r\nprocess)</strong>和<strong>开放信息提取过程(Open Information Extraction\r\nprocess)</strong>。</p>\r\n</blockquote>\r\n<p>信息提取图（IE\r\nGraph）旨在提取结构信息以表示自然句子中的高级信息，例如基于文本的文档。</p>\r\n<p>下面我们将描述如何从给定的段落<span class=\"math inline\">\\(para\\)</span>中生成对应的IE\r\ngraph,这个过程主要分为三个步骤：</p>\r\n<ul>\r\n<li><p>coreference resolution</p>\r\n<p>共指解析是信息提取任务的基本过程，旨在找到引用文本序列中相同<strong>实体</strong>的<strong>表达式(expressions)</strong>。如图\r\n4 所示，名称“Pual”、名词术语“He”和“a renowned computer\r\nscientist”可能指的是同一个对象（人）。</p>\r\n<p>我们将共指簇 C\r\n表示为一组<strong>引用同一对象的短语</strong>。给定一个段落，可以得到从非结构化数据中提取的共指集\r\n<span class=\"math inline\">\\(C = \\{C_1, C_2, ...,\r\nC_n\\}\\)</span>。</p></li>\r\n<li><p>constructing IE relations</p>\r\n<p>为了构建IE图，我们首先需要从段落中提取三元组，这可以通过使用一些注明的信息提取系统实现。</p>\r\n<p>我们称<span class=\"math inline\">\\((subject,predicate,object)\\)</span>为一个关系，可以表示为<span class=\"math inline\">\\((n_i,r_{i,j},n_j)\\in R_{ie}\\)</span>.</p>\r\n<blockquote>\r\n<p>值得注意的是，如果两个三元组只有一个参数不同，而其他参数重叠，我们只保留较长的三元组。</p>\r\n</blockquote></li>\r\n<li><p>graph construction</p>\r\n<ul>\r\n<li>首先，对关系三元组，我们添加节点和对应的有向边，边的类型是对应的谓语类型</li>\r\n<li>然后，对于每个共指簇 <span class=\"math inline\">\\(C_i \\in\r\nC\\)</span>，可以将 <span class=\"math inline\">\\(C_i\\)</span>\r\n中的所有共指短语折叠到一个节点中。这可以通过仅保留一个节点来帮助大大<strong>减少节点数</strong>量并<strong>消除歧义</strong>。</li>\r\n</ul></li>\r\n</ul></li>\r\n<li><p>Discourse Graph Construction</p>\r\n<p>当候选文档太长时，许多 NLP\r\n任务都会遇到长依赖挑战。描述两个句子如何在逻辑上相互连接的<strong>语篇图(Discourse\r\nGraph)</strong>被证明可以有效应对这一挑战。</p>\r\n<ul>\r\n<li><p>Discourse Relation</p>\r\n<p>语篇关系源自语篇分析，旨在识别一组句子上的句子顺序约束。</p>\r\n<p>给定两个句子，我们可以将discourse relation定义为<span class=\"math inline\">\\((sent_i,sent_j)\\)</span>,它代表着\"<span class=\"math inline\">\\(sentence_j\\)</span>可以被放置到<span class=\"math inline\">\\(sentence_i\\)</span>之后\"的discourse relation。</p>\r\n<p>在许多NLP任务中，给定一个文档<span class=\"math inline\">\\(doc\\)</span>,我们首先将它分割成句子集<span class=\"math inline\">\\(V =\r\n\\{sent_1,sent_2,...,sent_m\\}\\)</span>。然后，我们应用discourse\r\nanalysis来得到成对的discourse relation，表示为<span class=\"math inline\">\\(R_{sep}\\subseteq V\\times V\\)</span>。</p></li>\r\n<li><p>Discourse Graph</p>\r\n<p>话语图 G(V, E) 由上面讨论的句子节点和话语关系组成。</p>\r\n<p>给定文档 doc 和话语关系集 <span class=\"math inline\">\\(R_{dis}\\)</span>，对于每个关系 <span class=\"math inline\">\\((sent_i, sent_j) \\in R_{dis}\\)</span>，添加节点\r\n<span class=\"math inline\">\\(v_i\\)</span>（对于句子 <span class=\"math inline\">\\(sent_i\\)</span>）和 <span class=\"math inline\">\\(v_j\\)</span>（对于句子 <span class=\"math inline\">\\(sent_j\\)</span>），并添加从节点 <span class=\"math inline\">\\(v_i\\)</span> 到节点 <span class=\"math inline\">\\(v_j\\)</span>的<strong>有向边</strong>。</p></li>\r\n</ul></li>\r\n<li><p>Knowledge Graph Construction</p>\r\n<figure>\r\n<img src=\"/2023/01/16/GraphConstructionMethods4NLP/image-20221013165928021.png\" alt=\"image-20221013165928021\">\r\n<figcaption aria-hidden=\"true\">image-20221013165928021</figcaption>\r\n</figure>\r\n<blockquote>\r\n<p>知识图构建示例，其中使用的知识库 (KB) 和生成的概念图均来自数据集\r\nMetaQA（Zhang 等人，2018b）。</p>\r\n</blockquote>\r\n<p>捕获实体和关系的知识图 (KG) 可以极大地促进许多 NLP\r\n应用程序中的学习和推理。</p>\r\n<p>一般来说，KG 可以根据它们的图构建方法分为两大类。</p>\r\n<ul>\r\n<li>许多应用程序将 KG\r\n视为<strong>非结构化数据</strong>的紧凑且可解释的中间表示。从概念上讲，它几乎类似于我们之前讨论过的\r\nIE 图。</li>\r\n<li>许多其他作品（Wu et al., 2020b; Ye et al., 2019; Bansal et al.,\r\n2019; Yang et al., 2019）整合了现有的知识库，例如 YAGO (Suchanek et al.,\r\n2008)）和ConceptNet (Speer et al., 2017) 进一步提高下游任务的性能 (Zhao\r\net al., 2020c)。</li>\r\n</ul>\r\n<p>下面我们重点讨论第二种知识图。</p>\r\n<p><span class=\"math inline\">\\(\\mathcal{G}(\\mathcal{V},\\mathcal{E})\\)</span>由知识库中的元素构成，元素三元组<span class=\"math inline\">\\((e_1,rel,e_2)\\)</span>中，前者是source\r\nentity,后者是target\r\nentity，中间的rel表示关系的类型。构建知识图的时候，前后两者为节点，中间的是从前者指向后者的有向边的类型。</p>\r\n<p>在当做数据增广手段的时候，由于知识库过大，且存在许多噪声，因此需要在特定域中抽取出对应的子图。其构建方法：不同应用差距很大，下面只是一种例子:</p>\r\n<ul>\r\n<li>首先需要做的是获取给定<strong>查询</strong>中的<strong>术语实例</strong>。</li>\r\n<li>然后可以通过一些匹配算法（例如最大子字符串匹配）将术语实例链接到 KG\r\n中的概念。这些概念被视为抽取出的子图中的初始节点</li>\r\n<li>下一步是获取 KG 中初始节点的 1 跳邻居。</li>\r\n<li>此外，可以通过应用一些图节点相关性模型来计算邻居与初始节点的相关性，例如个性化\r\nPageRank (PPR) 算法 (Page et al., 1999)。</li>\r\n<li>然后根据结果，可以进一步剪除相关分数低于置信阈值的边缘，并移除孤立的邻居。</li>\r\n<li>剩余的最终子图随后用于提供任何图表示学习模块。</li>\r\n</ul></li>\r\n<li><p>Coreference Graph Construction</p>\r\n<p>当给定段落中的两个或多个术语指代同一个对象时，就会发生<strong>共同指称(coreference或co-reference）</strong></p>\r\n<p>这种现象有助于更好地理解语料库的复杂结构和逻辑，解决歧义</p>\r\n<p>为了有效地利用共指信息，共指图被构造为显式地对隐式共指关系建模。</p>\r\n<p>给定一组短语，共指图可以链接指向文本语料库中相同实体的节点（短语）。在下面的小节中，我们将重点关注由\r\nm 个句子组成的段落 para 的共指图构造。值得注意的是，虽然它与 IE\r\n图的第一步类似，但共指图将通过图<strong>显式地对共指关系进行建模</strong>，而不是<strong>折叠成一个节点</strong>。</p>\r\n<ul>\r\n<li><p>Coreference Relation</p>\r\n<p>共指关系可以很容易地通过共指解析系统获得，如 IE\r\n图构造中所讨论的。类似地，我们可以在给定特定段落的情况下获得共指簇 C。簇\r\n<span class=\"math inline\">\\(C_i \\in C\\)</span>\r\n中的所有短语都指向同一个对象。</p></li>\r\n<li><p>Coreference Graph</p>\r\n<p>共指图建立在共指关系集 <span class=\"math inline\">\\(R_{coref}\\)</span>\r\n之上。根据节点类型，一般可以分为两大类：</p>\r\n<ul>\r\n<li>phrases (or mentions)</li>\r\n<li>words</li>\r\n</ul>\r\n<p>对第一类，coreference图由关系集合<span class=\"math inline\">\\(\\mathcal{R}_{coref}\\)</span>中的所有mentions构成。对于共指簇中的短语对<span class=\"math inline\">\\(p_i,p_j\\)</span>，我们可以在代表两个短语的节点之间添加无向边。</p>\r\n<p>对于第二类，共指图则是由words构成</p>\r\n<p>一个小的区别是，对于每个相关的<strong>短语</strong>，<strong>只链接</strong>每个短语的<strong>第一个单词</strong>。</p></li>\r\n</ul></li>\r\n<li><p>Similarity Graph Construction</p>\r\n<figure>\r\n<img src=\"/2023/01/16/GraphConstructionMethods4NLP/image-20221013174523073.png\" alt=\"image-20221013174523073\">\r\n<figcaption aria-hidden=\"true\">image-20221013174523073</figcaption>\r\n</figure>\r\n<blockquote>\r\n<p>相似图构建的示例。我们使用<strong>句子</strong>作为节点，并使用\r\n<strong>TF-IDF\r\n向量</strong>初始化它们的特征。<strong>较大的相似度值对应于较粗的虚线</strong>。</p>\r\n</blockquote>\r\n<p>由于相似度图通常是面向应用程序的，因此我们只关注为实体、句子和文档等各种类型的元素构建相似度图的基本过程，而忽略了应用程序的具体细节。值得注意的是，相似图的构建是在<strong>预处理过程</strong>中进行的，并<strong>没有</strong>与剩余的学习系统以端到端的方式<strong>联合训练</strong>。</p>\r\n<ul>\r\n<li><p>Similarity Graph</p>\r\n<p>给定一个语料库C，在相似度图G(V,\r\nE)中，图节点可以定义为实体、句子和文档等不同粒度级别。基础的节点集合用<span class=\"math inline\">\\(\\mathcal{V}\\)</span>来表示。</p>\r\n<p>人们可以通过各种机制计算节点特征，例如句子（或文档）的 TF-IDF（Liu\r\n等人，2019a；Yasunaga 等人，2017）和实体的embeddings（Linmei\r\n等人，2019）。</p>\r\n<p>节点对之间的<strong>相似度分数</strong>可以通过余弦相似度等各种度量来计算（Liu\r\net al., 2019a; Linmei et al., 2019; Yasunaga et al.,\r\n2017），用于表示节点对的<strong>边权重</strong>。</p></li>\r\n<li><p>Spare mechanism</p>\r\n<p>初始相似度图通常是密集的，即使某些边缘权重非常小甚至为负。这些值可以被视为噪声，它在相似度图中的作用很小。</p>\r\n<p>因此，提出了各种稀疏技术，通过对图进行稀疏化来进一步提高图的质量。</p>\r\n<p>一种广泛使用的稀疏方法是 <strong>k-NN</strong> (Liu et al.,\r\n2019a)。具体来说，对于节点 <span class=\"math inline\">\\(v_i\\)</span>\r\n和它的邻居集合 <span class=\"math inline\">\\(N\r\n(v_i)\\)</span>，只有通过保留 k 个最大边权重并丢弃剩余边来保留边。</p>\r\n<p>另一种广泛使用的方法是<span class=\"math inline\">\\(\\epsilon\r\n-sparse\\)</span>（Linmei et al., 2019; Yasunaga et al.,\r\n2017）。特别是，将删除权重小于某个阈值<span class=\"math inline\">\\(\\epsilon\\)</span>的边。</p></li>\r\n</ul></li>\r\n<li><p>Co-occurrence Graph Construction</p>\r\n<figure>\r\n<img src=\"/2023/01/16/GraphConstructionMethods4NLP/image-20221013235057395.png\" alt=\"image-20221013235057395\">\r\n<figcaption aria-hidden=\"true\">image-20221013235057395</figcaption>\r\n</figure>\r\n<blockquote>\r\n<p>共现图构造的示例，其中边权重代表单词之间的共现频率。我们将窗口大小设置为\r\n3。</p>\r\n</blockquote>\r\n<p>共现图旨在捕捉文本中单词之间的<strong>共现关系</strong>，广泛用于许多\r\nNLP 任务（Christopoulou 等人，2019；Zhang 和 Qian，2020；Zhang\r\n等人，2020f）。共现关系描述了两个词在固定大小的上下文窗口内同时出现的频率，是捕捉语料库中词之间语义关系的重要特征。在下文中，我们将首先介绍获得同现关系的方法，然后讨论为语料库\r\nC 构建同现图的基本过程。</p>\r\n<ul>\r\n<li><p>Co-occurrence Relation:</p>\r\n<p>共现关系由给定语料库 C 的共现矩阵定义。</p>\r\n<p>可以将矩阵的共现表示为 <span class=\"math inline\">\\(M \\in\r\n\\mathbb{R}^{|V |×|V |}\\)</span>，其中$ |V |$是 <span class=\"math inline\">\\(C\\)</span> 的词汇量。<span class=\"math inline\">\\(M_{w_i,w_j}\\)</span> 描述了词 <span class=\"math inline\">\\(w_i,w_j\\)</span> 在语料库 C\r\n中固定大小的滑动窗口内一起出现的次数。</p>\r\n<p>在得到共现矩阵之后，有两种典型的方法来计算单词之间的权重：</p>\r\n<ul>\r\n<li>共现频率</li>\r\n<li>逐点互信息/point-wise mutual information (PMI)</li>\r\n</ul></li>\r\n<li><p>Co-occurrence Graph</p>\r\n<p>同现图 <span class=\"math inline\">\\(\\mathcal{G}(\\mathcal{V},\\mathcal{E})\\)</span>\r\n由上面讨论的<strong>单词节点</strong>和<strong>同现关系</strong>组成。</p>\r\n<p>给定语料库<span class=\"math inline\">\\(C\\)</span> 和共现关系集 <span class=\"math inline\">\\(R_{co}\\)</span>，对于每个关系 <span class=\"math inline\">\\((w_i, w_j) \\in R_{co}\\)</span>，添加节点 <span class=\"math inline\">\\(v_i\\)</span>（对于单词 <span class=\"math inline\">\\(w_i\\)</span>）和 <span class=\"math inline\">\\(v_j\\)</span>（对于单词 <span class=\"math inline\">\\(w_j\\)</span>）并添加来自节点的无向<strong>边</strong><span class=\"math inline\">\\(v_i\\)</span> 到节点 <span class=\"math inline\">\\(v_j\\)</span>\r\n用上述计算的<strong>边权重</strong>初始化。</p></li>\r\n</ul></li>\r\n<li><p>Topic Graph Construction</p>\r\n<figure>\r\n<img src=\"/2023/01/16/GraphConstructionMethods4NLP/image-20221014000501523.png\" alt=\"image-20221014000501523\">\r\n<figcaption aria-hidden=\"true\">image-20221014000501523</figcaption>\r\n</figure>\r\n<blockquote>\r\n<p>主题图构建示例，其中虚线表示通过利用数据集 AG 新闻上的 LDA\r\n算法进行的主题建模过程</p>\r\n</blockquote>\r\n<p>主题图建立在<strong>多个文档</strong>之上，旨在对<strong>不同主题之间的高级语义关系</strong>进行建模。</p>\r\n<ul>\r\n<li>输入：<span class=\"math inline\">\\(\\mathcal{D} =\r\n\\{doc_1,doc_2,...,doc_m\\}\\)</span></li>\r\n<li>流程：\r\n<ul>\r\n<li>通过LDA等算法学习潜在的主题<span class=\"math inline\">\\(\\mathcal{T}\\)</span></li>\r\n<li>则图为<span class=\"math inline\">\\(\\mathcal{G}(\\mathcal{V},\\mathcal{E})\\)</span>,其中<span class=\"math inline\">\\(\\mathcal{V} =\r\n\\mathcal{D}\\cup\\mathcal{T}\\)</span>,我们添加从文档<span class=\"math inline\">\\(v_i\\)</span>到主题<span class=\"math inline\">\\(v_j\\)</span>之间的无向边。当且仅当该文档含有该主题时添加。</li>\r\n</ul></li>\r\n</ul></li>\r\n<li><p>App-driven Graph Construction</p>\r\n<figure>\r\n<img src=\"/2023/01/16/GraphConstructionMethods4NLP/image-20221014000533458.png\" alt=\"image-20221014000533458\">\r\n<figcaption aria-hidden=\"true\">image-20221014000533458</figcaption>\r\n</figure>\r\n<blockquote>\r\n<p>应用程序驱动的图构建示例，专为 SQL 查询输入而设计。</p>\r\n</blockquote>\r\n<p>应用驱动图是指专门为特定 NLP\r\n任务设计的图，前面讨论的静态图类型无法轻松涵盖。</p>\r\n<p>在一些 NLP\r\n任务中，通常使用特定于应用程序的方法通过结构化形成来表示非结构化数据。例如，SQL\r\n语言可以很自然地用 SQL 解析树来表示。因此它可以转换为 SQL 图（Xu et al.,\r\n2018a; Bogin et al., 2019a; Huo et al., 2019; Bogin et al.,\r\n2019b）。由于这些图基于领域知识过于专业，因此没有统一的模式来总结如何构建应用程序驱动的图。</p></li>\r\n</ul>\r\n<p><strong>HYBRID GRAPH CONSTRUCTION AND DISCUSSION</strong></p>\r\n<p>大多数以前的静态图构造方法只考虑节点之间的<strong>一种</strong>特定关系。尽管获得的图在一定程度上很好地捕捉了结构信息，但它们在利用<strong>不同类型</strong>的图关系方面也受到限制。</p>\r\n<p>为了解决这个限制，人们越来越关注通过将<strong>多个图组合在一起</strong>来构建<strong>混合图</strong>，以丰富图中的语义信息。这些构造混合图的方法往往是根据应用变化的，下面给出一些代表性方法。</p>\r\n<p>为了捕获多个关系，一个常见的策略是构建一个<strong>异构图(heterogeneous\r\ngraph)</strong>，其中包含多种类型的节点和边。在不失一般性的情况下，我们假设可以创建具有两个不同图源\r\n<span class=\"math inline\">\\(\\mathcal{G}_a(\\mathcal{V}_a,\r\n\\mathcal{E}_a)\\)</span> 和 <span class=\"math inline\">\\(\\mathcal{G}_b(\\mathcal{V}_b,\r\n\\mathcal{E}_b)\\)</span>的混合图 Ghybrid。图 a、b\r\n是两种不同的图类型，例如依赖图和选区图。</p>\r\n<ul>\r\n<li><p>给定这些文本输入，如果 <span class=\"math inline\">\\(\\mathcal{G}_a\\)</span> 和 <span class=\"math inline\">\\(\\mathcal{G}_b\\)</span> 共享相同的节点集（即 <span class=\"math inline\">\\(\\mathcal{V}_a =\r\n\\mathcal{V}_b\\)</span>），我们通过<strong>注释特定关系的边类型</strong>来合并边集。</p></li>\r\n<li><p>否则，我们将 $_a <span class=\"math inline\">\\(和\\)</span> _b$\r\n合并得到混合节点集，记为 <span class=\"math inline\">\\(\\mathcal{V} =\r\n\\mathcal{V}_a \\cup\r\n\\mathcal{V}_b\\)</span>。然后我们通过将源节点和目标节点从 $_a <span class=\"math inline\">\\(和\\)</span> _b$ 映射到 <span class=\"math inline\">\\(\\mathcal{V}\\)</span> 来生成 <span class=\"math inline\">\\(\\mathcal{E}_a\\)</span> 和 <span class=\"math inline\">\\(\\mathcal{E}_b\\)</span> 到 <span class=\"math inline\">\\(\\mathcal{E}\\)</span>。</p></li>\r\n</ul>\r\n<h3 id=\"dynamic-graph-construction\">Dynamic Graph Construction</h3>\r\n<p>尽管静态图构造具有将数据的<strong>先验知识</strong>编码到图结构中的优点，但它有几个限制。</p>\r\n<ul>\r\n<li>为了构建合理性能的图拓扑，需要大量的人力和领域专业知识。</li>\r\n<li>手动构建的图结构可能容易出错（例如，嘈杂或不完整）</li>\r\n<li>由于图构建阶段和图表示学习阶段是不相交的，在图构建阶段引入的错误无法纠正，可能会累积到后续阶段，从而导致性能下降。</li>\r\n<li>图构建过程通常仅由机器学习从业者的洞察力提供信息，并且对于下游预测任务可能不是最佳的</li>\r\n</ul>\r\n<p>大多数动态图构建方法旨在<strong>动态学习图结构（即加权邻接矩阵）</strong>，并且图构建模块可以<strong>与后续图表示学习模块联合优化</strong>，以实现<strong>端到端</strong>的下游任务。</p>\r\n<figure>\r\n<img src=\"/2023/01/16/GraphConstructionMethods4NLP/image-20221015000556815.png\" alt=\"image-20221015000556815\">\r\n<figcaption aria-hidden=\"true\">image-20221015000556815</figcaption>\r\n</figure>\r\n<blockquote>\r\n<p>动态图构建方法的总体说明。虚线（在左侧的数据点中）表示可选的内在图拓扑。</p>\r\n</blockquote>\r\n<p>这些动态图构建方法通常包括一个<strong>图相似度度量学习组件</strong>，用于通过考虑嵌入空间中的<strong>成对节点相似性</strong>来学习<strong>邻接矩阵</strong>，以及一个<strong>图稀疏化组件</strong>，用于从学习的全连接图中提取<strong>稀疏图</strong>。</p>\r\n<p>据报道，将<strong>内在图结构</strong>和学习的<strong>隐式图结构</strong>相结合以获得更好的学习性能是有益的。</p>\r\n<p>此外，为了有效地进行联合<strong>图结构和表示</strong>学习，已经提出了各种学习范式。</p>\r\n<p><strong>GRAPH SIMILARITY METRIC LEARNING TECHNIQUES</strong></p>\r\n<p>基于假设<strong>节点属性包含用于学习隐式图结构的信息</strong>，最近的工作将图结构学习问题转换成定义在节点嵌入空间的相似度度量学习问题。</p>\r\n<p>学习到的<strong>相似度度量函数</strong>可以之后被应用到未见过的节点embeddings中去推断一个图结构，从而实现归纳图结构。</p>\r\n<ul>\r\n<li><p>Node Embedding Based Similarity Metric Learning</p>\r\n<p>基于节点嵌入的相似度度量函数旨在通过计算嵌入空间中的成对节点<strong>相似度</strong>来学习<strong>加权邻接矩阵</strong>。</p>\r\n<p>常见的度量函数包括基于<strong>注意力</strong>的度量函数和基于<strong>余弦</strong>的度量函数。</p>\r\n<ul>\r\n<li><p>Attention-based Similarity Metric Functions(most of the\r\nsimilarity metric functions)</p>\r\n<p>为了提高基于<strong>点积</strong>的注意力机制的学习能力，Chen 等人。\r\n（2020d）通过引入可学习参数提出了一种改进的点积，公式如下： <span class=\"math display\">\\[\r\nS_{i,j} = (\\vec{v_i}\\odot\\vec{u})^{T}\\vec{v_j}\r\n\\]</span> 其中<span class=\"math inline\">\\(\\vec{u}\\)</span>是一个非负的权重向量，学习它以便于强调节点embeddings中的不同维度。其中<span class=\"math inline\">\\(\\odot\\)</span>是逐元素的向量乘法。</p>\r\n<p>相似的，通过引入一个可学习的权重矩阵，一个更具表现力的点积版本，公式如下：\r\n<span class=\"math display\">\\[\r\nS_{i,j} = ReLU(\\vec{W}\\vec{v_i})^TReLU(\\vec{W}\\vec{v_j})\r\n\\]</span> 其中<span class=\"math inline\">\\(\\vec{W}\\)</span>是一个<span class=\"math inline\">\\(d\\times d\\)</span>的权重矩阵，而<span class=\"math inline\">\\(ReLU(x) = max(0, x)\\)</span> 是一个整流线性单元\r\n(ReLU)，用于强制相似矩阵的稀疏性。</p></li>\r\n<li><p>Cosine-based Similarity Metric Functions.</p>\r\n<p>陈等人。\r\n（2020e）将基础余弦相似度扩展为<strong>多头加权余弦相似度</strong>，以从多个角度捕获成对节点相似度，公式如下：\r\n<span class=\"math display\">\\[\r\nS_{i,j}^p = cos(\\vec{w}_p\\odot\\vec{v_i},\\vec{w}_p\\odot\\vec{v}_j)\r\n\\newline\r\nS_{i,j} = \\frac{1}{m}\\sum_{p=1}^{m}S_{i,j}^p\r\n\\]</span> 其中的<span class=\"math inline\">\\(\\vec{w}_p\\)</span>是一个和第p个视角相关的权重向量，和节点embeddings有相同的维数。</p>\r\n<p>直观地说，<span class=\"math inline\">\\(S^p_{i,j}\\)</span> 计算第 p\r\n个视角的成对余弦相似度，其中每个视角都考虑嵌入中捕获的语义的一部分。</p>\r\n<p>除了<strong>提高学习能力</strong>外，采用多头学习器还能够<strong>稳定学习过程</strong>。</p></li>\r\n</ul></li>\r\n<li><p>Structure-aware Similarity Metric Learning</p>\r\n<p>受结构感知转换器（structure-aware\r\ntransformers）的启发，最近的方法采用了<strong>结构感知相似度度量函数</strong>，该函数额外考虑了节点信息之外的<strong>内在图的现有边缘信息</strong>。</p>\r\n<p>一种用于学习成对节点相似度的结构感知注意机制，公式如下： <span class=\"math display\">\\[\r\nS_{i,j}^l =\r\nsoftmax(\\vec{u}^Ttanh(\\vec{W}[\\vec{h}_i^l,\\vec{h}_j^l,\\vec{v}_i,\\vec{v}_j,\\vec{e}_{i,j}]))\r\n\\]</span> 其中<span class=\"math inline\">\\(\\vec{v}_i\\)</span>表示的是节点i的embeddings,<span class=\"math inline\">\\(\\vec{e}_{i,j}\\)</span>表示的是连接节点i和j的边的embeddings。<span class=\"math inline\">\\(\\vec{h}_i^l\\)</span>是在第l个GNN层中的节点i的embeddings。并且，<span class=\"math inline\">\\(\\vec{u}\\)</span>和<span class=\"math inline\">\\(\\vec{W}\\)</span>是可训练的权重向量和权重矩阵。</p>\r\n<p>一种结构感知的全局注意力机制，公式如下： <span class=\"math display\">\\[\r\nS_{i,j} =\r\n\\frac{ReLU(\\vec{W}^Q\\vec{v}_i)^T(ReLU(\\vec{W}^K\\vec{v}_j))+ReLU(\\vec{W}^R\\vec{e}_{i,j})}{\\sqrt{d}}\r\n\\]</span>\r\n三个W是将节点embeddings和边embeddings映射到潜在embeddings空间的线性映射。</p></li>\r\n</ul>\r\n<p><strong>GRAPH SPARSIFICATION TECHNIQUES</strong></p>\r\n<p>现实世界场景中的大多数图都是稀疏图。<strong>相似度度量函数</strong>考虑任何节点对之间的关系并返回一个<strong>全连接图</strong>，这不仅<strong>计算量大</strong>，而且可能<strong>引入噪声</strong>，例如不重要的边。因此，明确地对学习的图结构<strong>强制执行</strong>稀疏性可能是有益的。除了在相似性度量函数中<strong>应用\r\nReLU 函数</strong>（Chen et al., 2020f; Liu et al.,\r\n2021b），还采用了各种图稀疏化技术来增强学习图结构的稀疏性。</p>\r\n<ul>\r\n<li><p>一个kNN风格的稀疏化操作，从相似度学习函数计算的节点相似度矩阵中得到一个稀疏邻接矩阵，公式如下：\r\n<span class=\"math display\">\\[\r\n\\vec{A}_{i,:} = topk(\\vec{S}_{i,:})\r\n\\]</span>\r\n对每个节点，只有K个最近的邻居节点(包括它自己)以及对应的相似度分数会被保留下来，剩下的相似度分数会被屏蔽掉。</p></li>\r\n<li><p>通过仅考虑每个节点的 <span class=\"math inline\">\\(ε\\)</span>-邻域来强制执行稀疏邻接矩阵，公式如下：\r\n<span class=\"math display\">\\[\r\nA _ { i , j } = \\{ \\begin{array}  { l l  }  { S _ { i , j } } &amp; { S\r\n_ { i , j } \\gt ε } \\\\ { 0 } &amp; { otherwise } \\end{array}\r\n\\]</span> 其中 S 中小于非负阈值<span class=\"math inline\">\\(ε\\)</span>的那些元素都被屏蔽掉（即设置为零）。</p></li>\r\n<li><p>除了通过应用某种形式的阈值来显式地强制学习图的稀疏性之外，稀疏性还以基于<strong>学习的方式隐式地强制执行</strong>。陈等人。\r\n（2020e）引入了以下正则化术语来鼓励稀疏图。 <span class=\"math display\">\\[\r\n\\frac{1}{n^2}||A||^2_F\r\n\\]</span> 在哪里 <span class=\"math inline\">\\(|| · ||_F\\)</span>\r\n表示矩阵的 <strong>Frobenius 范数</strong>。</p>\r\n<blockquote>\r\n<p>矩阵A的Frobenius范数定义为矩阵A各项<strong>元素的绝对值平方的总和</strong></p>\r\n</blockquote></li>\r\n</ul>\r\n<p><strong>COMBINING INTRINSIC GRAPH STRUCTURES AND IMPLICIT GRAPH\r\nSTRUCTURES</strong></p>\r\n<p>最近的研究表明，如果在进行动态图构建时完全丢弃内在图结构，可能会损害下游任务的性能。这可能是因为内在图通常仍然携带有关下游任务的最佳图结构的丰富且有用的信息。</p>\r\n<p>因此，他们提出将学习到的隐式图结构与内在图结构相结合，<strong>假设</strong>是学习到的隐式图可能是内在图结构的“转移”（例如子结构），内在图结构是对内在图结构的补充。</p>\r\n<p>另一个潜在的好处是结合内在的图结构可能有助于<strong>加速训练过程</strong>并提高<strong>训练稳定性</strong>。(由于没有关于相似度度量的先验知识，并且可训练的参数是随机初始化的，因此通常可能需要很长时间才能收敛。)</p>\r\n<p>提出计算内在图结构的归一化图拉普拉斯算子 $L^{(0)} $和隐式图结构 <span class=\"math inline\">\\(f(A)\\)</span>\r\n的归一化邻接矩阵的线性组合，公式如下： <span class=\"math display\">\\[\r\n\\widetilde{A} = \\lambda L^{(0)} + (1-\\lambda)f(A)\r\n\\]</span> 其中<span class=\"math inline\">\\(f: \\mathbb{R}^{n \\times n}\r\n\\rightarrow \\mathbb{R}^{n \\times\r\nn}\\)</span>可以是任意归一化操作，例如图拉普拉斯运算和行归一化操作。</p>\r\n<p>并没有明确地融合两个图邻接矩阵,而是提出了一种用于 GNN\r\n的<strong>混合消息传递机制</strong>，该机制分别融合从内在图和学习到的隐式图计算的两个聚合节点向量，然后将融合向量馈送到\r\nGRU 以更新节点嵌入。</p>\r\n<p><strong>LEARNING PARADIGMS</strong></p>\r\n<p>大多数现有的 GNN\r\n动态图构建方法由两个关键学习组件组成：图结构学习（即相似性度量学习）和图表示学习（即\r\nGNN\r\n模块），最终目标是学习优化关于某些下游预测任务的<strong>图结构和表示</strong>。</p>\r\n<p>如何优化两个独立的学习组件以实现相同的最终目标成为一个重要的问题。</p>\r\n<ul>\r\n<li>最直接的策略是以端到端的方式共同优化整个学习系统，面向下游（半）监督预测任务。</li>\r\n<li>另一种常见的策略是自适应地学习每个堆叠 GNN\r\n层的输入图结构，以反映中间图表示的变化。这类似于 Transformer\r\n模型如何在每一层中学习不同的加权全连接图。</li>\r\n<li>提出了一种<strong>迭代</strong>图学习框架，通过基于更好的<strong>图表示</strong>学习更好的<strong>图结构</strong>，同时以迭代的方式基于更好的<strong>图结构</strong>学习更好的<strong>图表示</strong>。结果，这种迭代学习范式反复细化图结构和图表示以达到最佳下游性能。</li>\r\n</ul>\r\n<h2 id=\"graph-representation-learning-for-nlp\">Graph Representation\r\nLearning for NLP</h2>\r\n<p>在本节中，我们将讨论各种图表示学习技术，这些技术直接在构建的图上运行，用于各种\r\nNLP 任务。</p>\r\n<p>图表示学习技术</p>\r\n<ul>\r\n<li>目标：找到一种通过<strong>机器学习模型</strong>将<strong>图结构和属性信息</strong>合并到<strong>低维embeddings</strong>中的方法</li>\r\n<li>数学形式化：\r\n<ul>\r\n<li>图： <span class=\"math inline\">\\(\\mathcal{G}(\\mathcal{V},\\mathcal{E},\\mathcal{T},\\mathcal{R})\\)</span>\r\n<ul>\r\n<li>节点集合： <span class=\"math inline\">\\(\\mathcal{V}\\)</span></li>\r\n<li>边集合： <span class=\"math inline\">\\(\\mathcal{E}\\)</span></li>\r\n<li>节点类型 <span class=\"math inline\">\\(\\mathcal{T} =\r\n\\{T_1,T_2,...,T_p\\}\\)</span></li>\r\n<li>边类型： <span class=\"math inline\">\\(\\mathcal{R} =\r\n\\{R_1,...,R_q\\}\\)</span></li>\r\n</ul></li>\r\n<li>元素数目： <span class=\"math inline\">\\(|\\cdot|\\)</span></li>\r\n<li>节点类型指示函数：<span class=\"math inline\">\\(\\tau(\\cdot) \\in\r\n\\mathcal{T}\\)</span>，其中输入元素为节点<span class=\"math inline\">\\(v_i\\)</span></li>\r\n<li>边类型指示函数：<span class=\"math inline\">\\(\\phi(\\cdot) \\in\r\n\\mathcal{R}\\)</span>，其中输入元素是边<span class=\"math inline\">\\(e_{i,j}\\)</span></li>\r\n</ul></li>\r\n</ul>\r\n<h3 id=\"gnns-for-homogeneous-graphs\">GNNs for Homogeneous Graphs</h3>\r\n<p>Homogeneous Graphs(同构图)：<span class=\"math inline\">\\(|\\mathcal{T}|\r\n= |\\mathcal{R}| = 1\\)</span></p>\r\n<h3 id=\"graph-neural-networks-for-multi-relational-graphs\">Graph Neural\r\nNetworks for Multi-relational Graphs</h3>\r\n<p>Multi-relational Graphs(多关系图)：<span class=\"math inline\">\\(|\\mathcal{T}| = 1\\)</span> 并且 <span class=\"math inline\">\\(|\\mathcal{R}| &gt; 1\\)</span></p>\r\n<h3 id=\"graph-neural-networks-for-heterogeneous-graph\">Graph Neural\r\nNetworks for Heterogeneous Graph</h3>\r\n<p>Heterogeneous Graphs(异构图)：<span class=\"math inline\">\\(|\\mathcal{T}| &gt; 1\\)</span> 或者 <span class=\"math inline\">\\(|\\mathcal{R}| &gt; 1\\)</span></p>\r\n<h2 id=\"gnn-based-encoder-decoder-models\">GNN Based Encoder-Decoder\r\nModels</h2>\r\n<figure>\r\n<img src=\"/2023/01/16/GraphConstructionMethods4NLP/image-20221016223847470.png\" alt=\"image-20221016223847470\">\r\n<figcaption aria-hidden=\"true\">image-20221016223847470</figcaption>\r\n</figure>\r\n<blockquote>\r\n<p>基于图的编码器-解码器模型的整体架构，包含 Graph2Seq 和 Graph2Tree\r\n模型。输入和输出来自 JOBS640 数据集 (Luke, 2005)。 S1、S2\r\n等节点代表子树节点，从中生成新的分支。</p>\r\n</blockquote>\r\n<h3 id=\"sequence-to-sequence-models\">Sequence-to-Sequence Models</h3>\r\n<h3 id=\"graph-to-sequence-models\">Graph-to-Sequence Models</h3>\r\n<p>与 Seq2Seq 范式相比，Graph2Seq\r\n范式更善于捕捉输入文本的丰富结构信息，可以应用于任意图结构数据。与\r\nSeq2Seq 模型相比，Graph2Seq 模型在包括神经机器翻译在内的广泛 NLP\r\n任务中表现出卓越的性能</p>\r\n<h3 id=\"graph-to-tree-models\">Graph-to-Tree Models</h3>\r\n<p>与考虑输入端结构信息的 Graph2Seq 模型相比，许多 NLP\r\n任务还包含以复杂结构表示的输出，例如树，输出端也包含丰富的结构信息，例如句法解析（Ji\r\net al., 2019)(Yang and Deng, 2020), 语义解析(Li et al., 2020a)(Xu et\r\nal., 2018c), 数学单词问题求解(Li et al., 2020a)(Zhang et al.,\r\n2020b)。考虑这些输出的结构信息是我们自然的选择。为此，提出了一些\r\nGraph2Tree\r\n模型，在输入端和输出端都包含结构信息，使编码解码过程中的信息流更加完整。</p>\r\n<h3 id=\"graph-to-graph-models\">Graph-to-Graph Models</h3>\r\n<p>通常用于解决图转换问题的图到图模型作为图编码器-解码器模型。</p>\r\n","tags":["EM算法"]},{"title":"GNN4NLP","url":"/2023/01/16/GNN4NLP/","content":"<h1 id=\"gnn-in-natural-language-processing\">GNN in Natural Language\r\nProcessing</h1>\r\n<blockquote>\r\n<p>本篇主要介绍GNN在NLP中的使用情况。聚焦于其理论基础和主流的应用框架。对GNN在NLP中的使用情况有总体的了解。</p>\r\n<span id=\"more\"></span>\r\n</blockquote>\r\n<ol type=\"1\">\r\n<li><p><strong>Conversation Modeling on Reddit using a Graph-Structured\r\nLSTM.</strong> TACL 2018. <a href=\"https://arxiv.org/pdf/1704.02080\">paper</a></p>\r\n<p><em>Vicky Zayats, Mari Ostendorf.</em></p></li>\r\n<li><p><strong>Learning Graphical State Transitions.</strong> ICLR 2017.\r\n<a href=\"https://openreview.net/forum?id=HJ0NvFzxl\">paper</a></p>\r\n<p><em>Daniel D. Johnson.</em></p></li>\r\n<li><p><strong>Multiple Events Extraction via Attention-based Graph\r\nInformation Aggregation.</strong> EMNLP 2018. <a href=\"https://arxiv.org/pdf/1809.09078.pdf\">paper</a></p>\r\n<p><em>Xiao Liu, Zhunchen Luo, Heyan Huang.</em></p></li>\r\n<li><p><strong>Recurrent Relational Networks.</strong> NeurIPS 2018. <a href=\"http://papers.nips.cc/paper/7597-recurrent-relational-networks.pdf\">paper</a></p>\r\n<p><em>Rasmus Palm, Ulrich Paquet, Ole Winther.</em></p></li>\r\n<li><p><strong>Improved Semantic Representations From Tree-Structured\r\nLong Short-Term Memory Networks.</strong> ACL 2015. <a href=\"https://www.aclweb.org/anthology/P15-1150\">paper</a></p>\r\n<p><em>Kai Sheng Tai, Richard Socher, Christopher D.\r\nManning.</em></p></li>\r\n<li><p><strong>Encoding Sentences with Graph Convolutional Networks for\r\nSemantic Role Labeling.</strong> EMNLP 2017. <a href=\"https://arxiv.org/abs/1703.04826\">paper</a></p>\r\n<p><em>Diego Marcheggiani, Ivan Titov.</em></p></li>\r\n<li><p><strong>Graph Convolutional Networks with Argument-Aware Pooling\r\nfor Event Detection.</strong> AAAI 2018. <a href=\"http://ix.cs.uoregon.edu/~thien/pubs/graphConv.pdf\">paper</a></p>\r\n<p><em>Thien Huu Nguyen, Ralph Grishman.</em></p></li>\r\n<li><p><strong>Exploiting Semantics in Neural Machine Translation with\r\nGraph Convolutional Networks.</strong> NAACL 2018. <a href=\"http://www.aclweb.org/anthology/N18-2078\">paper</a></p>\r\n<p><em>Diego Marcheggiani, Joost Bastings, Ivan Titov.</em></p></li>\r\n<li><p><strong>Exploring Graph-structured Passage Representation for\r\nMulti-hop Reading Comprehension with Graph Neural Networks.</strong>\r\n2018. <a href=\"https://arxiv.org/abs/1809.02040\">paper</a></p>\r\n<p><em>Linfeng Song, Zhiguo Wang, Mo Yu, Yue Zhang, Radu Florian, Daniel\r\nGildea.</em></p></li>\r\n<li><p><strong>Graph Convolution over Pruned Dependency Trees Improves\r\nRelation Extraction.</strong> EMNLP 2018. <a href=\"https://arxiv.org/abs/1809.10185\">paper</a></p>\r\n<p><em>Yuhao Zhang, Peng Qi, Christopher D. Manning.</em></p></li>\r\n<li><p><strong>N-ary relation extraction using graph state\r\nLSTM.</strong> EMNLP 18. <a href=\"https://arxiv.org/abs/1808.09101\">paper</a></p>\r\n<p><em>Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel\r\nGildea.</em></p></li>\r\n<li><p><strong>A Graph-to-Sequence Model for AMR-to-Text\r\nGeneration.</strong> ACL 2018. <a href=\"https://arxiv.org/abs/1805.02473\">paper</a></p>\r\n<p><em>Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel\r\nGildea.</em></p></li>\r\n<li><p><strong>Graph-to-Sequence Learning using Gated Graph Neural\r\nNetworks.</strong> ACL 2018. <a href=\"https://arxiv.org/pdf/1806.09835.pdf\">paper</a></p>\r\n<p><em>Daniel Beck, Gholamreza Haffari, Trevor Cohn.</em></p></li>\r\n<li><p><strong>Cross-Sentence N-ary Relation Extraction with Graph\r\nLSTMs.</strong> TACL. <a href=\"https://arxiv.org/abs/1708.03743\">paper</a></p>\r\n<p><em>Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova,\r\nWen-tau Yih.</em></p></li>\r\n<li><p><strong>Sentence-State LSTM for Text Representation.</strong> ACL\r\n2018. <a href=\"https://arxiv.org/abs/1805.02474\">paper</a></p>\r\n<p><em>Yue Zhang, Qi Liu, Linfeng Song.</em></p></li>\r\n<li><p><strong>End-to-End Relation Extraction using LSTMs on Sequences\r\nand Tree Structures.</strong> ACL 2016. <a href=\"https://arxiv.org/abs/1601.00770\">paper</a></p>\r\n<p><em>Makoto Miwa, Mohit Bansal.</em></p></li>\r\n<li><p><strong>Graph Convolutional Encoders for Syntax-aware Neural\r\nMachine Translation.</strong> EMNLP 2017. <a href=\"https://arxiv.org/pdf/1704.04675\">paper</a></p>\r\n<p><em>Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani,\r\nKhalil Sima'an.</em></p></li>\r\n<li><p><strong>Semi-supervised User Geolocation via Graph Convolutional\r\nNetworks.</strong> ACL 2018. <a href=\"https://arxiv.org/pdf/1804.08049.pdf\">paper</a></p>\r\n<p><em>Afshin Rahimi, Trevor Cohn, Timothy Baldwin.</em></p></li>\r\n<li><p><strong>Modeling Semantics with Gated Graph Neural Networks for\r\nKnowledge Base Question Answering.</strong> COLING 2018. <a href=\"https://arxiv.org/pdf/1808.04126.pdf\">paper</a></p>\r\n<p><em>Daniil Sorokin, Iryna Gurevych.</em></p></li>\r\n<li><p><strong>Graph Convolutional Networks for Text\r\nClassification.</strong> AAAI 2019. <a href=\"https://arxiv.org/pdf/1809.05679.pdf\">paper</a></p>\r\n<p><em>Liang Yao, Chengsheng Mao, Yuan Luo.</em></p></li>\r\n</ol>\r\n<details open style=\"box-sizing: border-box; display: block; margin-top: 0px; margin-bottom: 16px;\">\r\n<summary style=\"box-sizing: border-box; display: list-item; cursor: pointer;\">\r\nmore\r\n</summary>\r\n<ol start=\"21\" dir=\"auto\" style=\"box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 16px;\">\r\n<li style=\"box-sizing: border-box;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Constructing\r\nNarrative Event Evolutionary Graph for Script Event\r\nPrediction.</strong><span> </span>IJCAI\r\n2018.<span> </span><a href=\"https://arxiv.org/pdf/1805.05081.pdf\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Zhongyang Li, Xiao Ding, Ting\r\nLiu.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Incorporating\r\nSyntactic and Semantic Information in Word Embeddings using Graph\r\nConvolutional Networks.</strong><span> </span>ACL\r\n2019.<span> </span><a href=\"https://arxiv.org/pdf/1809.04283\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Shikhar Vashishth, Manik Bhandari,\r\nPrateek Yadav, Piyush Rai, Chiranjib Bhattacharyya, Partha Talukdar</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">PaperRobot:\r\nIncremental Draft Generation of Scientific\r\nIdeas.</strong><span> </span>ACL\r\n2019.<span> </span><a href=\"https://arxiv.org/pdf/1905.07870\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Qingyun Wang, Lifu Huang, Zhiying\r\nJiang, Kevin Knight, Heng Ji, Mohit Bansal, Yi Luan.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Inter-sentence\r\nRelation Extraction with Document-level Graph Convolutional Neural\r\nNetwork.</strong><span> </span>ACL\r\n2019.<span> </span><a href=\"https://arxiv.org/pdf/1906.04684\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Sunil Kumar Sahu, Fenia\r\nChristopoulou, Makoto Miwa, Sophia Ananiadou.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Textbook\r\nQuestion Answering with Multi-modal Context Graph Understanding and\r\nSelf-supervised Open-set Comprehension.</strong><span> </span>ACL\r\n2019.<span> </span><a href=\"https://arxiv.org/pdf/1811.00232\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Daesik Kim, Seonhoon Kim, Nojun\r\nKwak.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Multi-hop\r\nReading Comprehension across Multiple Documents by Reasoning over\r\nHeterogeneous Graphs.</strong><span> </span>ACL\r\n2019.<span> </span><a href=\"https://arxiv.org/pdf/1905.07374\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Ming Tu, Guangtao Wang, Jing Huang,\r\nYun Tang, Xiaodong He, Bowen Zhou.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Dynamically\r\nFused Graph Network for Multi-hop Reasoning.</strong><span> </span>ACL\r\n2019.<span> </span><a href=\"https://arxiv.org/pdf/1905.06933\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Yunxuan Xiao, Yanru Qu, Lin Qiu, Hao\r\nZhou, Lei Li, Weinan Zhang, Yong Yu.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Cognitive\r\nGraph for Multi-Hop Reading Comprehension at\r\nScale.</strong><span> </span>ACL\r\n2019.<span> </span><a href=\"https://arxiv.org/pdf/1905.05460\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Ming Ding, Chang Zhou, Qibin Chen,\r\nHongxia Yang, Jie Tang.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Joint Type\r\nInference on Entities and Relations via Graph Convolutional\r\nNetworks.</strong><span> </span>ACL\r\n2019.<span> </span><a href=\"http://www.czsun.site/publications/joint_entrel_gcn.pdf\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Changzhi Sun, Yeyun Gong, Yuanbin\r\nWu, Ming Gong, Daxing Jiang, Man Lan, Shiliang Sun1, Nan Duan.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Attention\r\nGuided Graph Convolutional Networks for Relation\r\nExtraction.</strong><span> </span>ACL\r\n2019.<span> </span><a href=\"http://www.statnlp.org/wp-content/uploads/2019/06/Attention_Guided_Graph_Convolutional_Networks_for_Relation_Extraction.pdf\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Zhijiang Guo, Yan Zhang, Wei\r\nLu.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">GraphRel:\r\nModeling Text as Relational Graphs for Joint Entity and Relation\r\nExtraction.</strong><span> </span>ACL\r\n2019.<span> </span><a href=\"https://tsujuifu.github.io/pubs/acl19_graph-rel.pdf\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Tsu-Jui Fu, Peng-Hsuan Li, Wei-Yun\r\nMa.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Graph Neural\r\nNetworks with Generated Parameters for Relation\r\nExtraction.</strong><span> </span>ACL\r\n2019.<span> </span><a href=\"https://arxiv.org/pdf/1902.00756\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Hao Zhu, Yankai Lin, Zhiyuan Liu,\r\nJie Fu, Tat-seng Chua, Maosong Sun.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Generating\r\nLogical Forms from Graph Representations of Text and\r\nEntities.</strong><span> </span>ACL\r\n2019.<span> </span><a href=\"https://arxiv.org/pdf/1905.08407\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Peter Shaw, Philip Massey, Angelica\r\nChen, Francesco Piccinno, Yasemin Altun.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Matching\r\nArticle Pairs with Graphical Decomposition and\r\nConvolutions.</strong><span> </span>ACL\r\n2019.<span> </span><a href=\"https://arxiv.org/pdf/1802.07459\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Bang Liu, Di Niu, Haojie Wei,\r\nJinghong Lin, Yancheng He, Kunfeng Lai, Yu Xu.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Representing\r\nSchema Structure with Graph Neural Networks for Text-to-SQL\r\nParsing.</strong><span> </span>ACL\r\n2019.<span> </span><a href=\"https://arxiv.org/pdf/1905.06241\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Ben Bogin, Matt Gardner, Jonathan\r\nBerant.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Coherent\r\nComment Generation for Chinese Articles with a Graph-to-Sequence\r\nModel.</strong><span> </span>ACL\r\n2019.<span> </span><a href=\"https://arxiv.org/pdf/1906.01231\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Wei Li, Jingjing Xu, Yancheng He,\r\nShengli Yan, Yunfang Wu, Xu sun.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">GEAR:\r\nGraph-based Evidence Aggregating and Reasoning for Fact\r\nVerification.</strong><span> </span>ACL\r\n2019.<span> </span><a href=\"https://www.aclweb.org/anthology/P19-1085\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Jie Zhou, Xu Han, Cheng Yang,\r\nZhiyuan Liu, Lifeng Wang, Changcheng Li, Maosong Sun.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Look Again at\r\nthe Syntax: Relational Graph Convolutional Network for Gendered\r\nAmbiguous Pronoun Resolution.</strong><span> </span>ACL\r\n2019.<span> </span><a href=\"https://arxiv.org/pdf/1905.08868.pdf\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Yinchuan Xu, Junlin Yang.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Structured\r\nNeural Summarization.</strong><span> </span>ICLR\r\n2019.<span> </span><a href=\"https://openreview.net/pdf?id=H1ersoRqtm\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Patrick Fernandes, Miltiadis\r\nAllamanis, Marc Brockschmidt.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Long-tail\r\nRelation Extraction via Knowledge Graph Embeddings and Graph Convolution\r\nNetworks.</strong><span> </span>NAACL\r\n2019.<span> </span><a href=\"https://arxiv.org/pdf/1903.01306.pdf\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Ningyu Zhang, Shumin Deng, Zhanlin\r\nSun, Guanying Wang, Xi Chen, Wei Zhang, Huajun Chen.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Text\r\nGeneration from Knowledge Graphs with Graph\r\nTransformers.</strong><span> </span>NAACL\r\n2019.<span> </span><a href=\"https://arxiv.org/pdf/1904.02342.pdf\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Rik Koncel-Kedziorski, Dhanush\r\nBekal, Yi Luan, Mirella Lapata, Hannaneh Hajishirzi.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Question\r\nAnswering by Reasoning Across Documents with Graph Convolutional\r\nNetworks.</strong><span> </span>NAACL\r\n2019.<span> </span><a href=\"https://arxiv.org/pdf/1808.09920.pdf\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Nicola De Cao, Wilker Aziz, Ivan\r\nTitov.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">BAG:\r\nBi-directional Attention Entity Graph Convolutional Network for\r\nMulti-hop Reasoning Question Answering.</strong><span> </span>NAACL\r\n2019.<span> </span><a href=\"https://arxiv.org/pdf/1904.04969.pdf\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Yu Cao, Meng Fang, Dacheng Tao.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">GraphIE: A\r\nGraph-Based Framework for Information\r\nExtraction.</strong><span> </span>NAACL\r\n2019.<span> </span><a href=\"https://arxiv.org/pdf/1810.13083.pdf\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Yujie Qian, Enrico Santus, Zhijing\r\nJin, Jiang Guo, Regina Barzilay.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Graph\r\nConvolution for Multimodal Information Extraction from Visually Rich\r\nDocuments.</strong><span> </span>NAACL\r\n2019.<span> </span><a href=\"https://arxiv.org/pdf/1903.11279.pdf\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Xiaojing Liu, Feiyu Gao, Qiong\r\nZhang, Huasha Zhao.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Structural\r\nNeural Encoders for AMR-to-text Generation.</strong><span> </span>NAACL\r\n2019.<span> </span><a href=\"https://arxiv.org/pdf/1903.11410.pdf\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Marco Damonte, Shay B. Cohen.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Abusive\r\nLanguage Detection with Graph Convolutional\r\nNetworks.</strong><span> </span>NAACL\r\n2019.<span> </span><a href=\"https://arxiv.org/pdf/1904.04073.pdf\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Pushkar Mishra, Marco Del Tredici,\r\nHelen Yannakoudakis, Ekaterina Shutova.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Learning Graph\r\nPooling and Hybrid Convolutional Operations for Text\r\nRepresentations.</strong><span> </span>WWW\r\n2019.<span> </span><a href=\"https://arxiv.org/pdf/1901.06965.pdf\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Hongyang Gao, Yongjun Chen, Shuiwang\r\nJi.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Graph­‐based\r\nTransformer with Cross-candidate Verification for Semantic\r\nParsing.</strong><span> </span>AAAI\r\n2020.<span> </span><a href=\"https://github.com/thunlp/GNNPapers/blob/master\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Bo Shao, Yeyun Gong, Weizhen Qi,\r\nGuihong Cao, Jianshu Ji, Xiaola Lin.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Efficient\r\nMulti-Person Pose Estimation with Provable\r\nGuarantees.</strong><span> </span>AAAI\r\n2020.<span> </span><a href=\"https://arxiv.org/abs/1711.07794\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Shaofei Wang, Konrad Paul Kording,\r\nJulian Yarkony.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Graph\r\nTransformer for Graph-to-Sequence Learning.</strong><span> </span>AAAI\r\n2020.<span> </span><a href=\"https://arxiv.org/abs/1911.07470\" rel=\"nofollow\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Deng Cai, Wai Lam.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Multi-­‐label\r\nPatent Categorization with Non-­‐local Attention-­‐based Graph\r\nConvolutional Network.</strong><span> </span>AAAI\r\n2020.<span> </span><a href=\"https://github.com/thunlp/GNNPapers/blob/master\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Pingjie Tang, Meng Jiang, Bryan\r\n(Ning) Xia, Jed Pitera, Jeff Welser, Nitesh Chawla.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Multi-task\r\nLearning for Metaphor Detection with Graph Convolutional Neural Networks\r\nand Word Sense Disambiguation.</strong><span> </span>AAAI\r\n2020.<span> </span><a href=\"https://github.com/thunlp/GNNPapers/blob/master\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Duong Minh Le, My Thai and Thien Huu\r\nNguyen.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">Schema-Guided\r\nMulti-Domain Dialogue State Tracking with Graph Attention Neural\r\nNetworks.</strong><span> </span>AAAI\r\n2020.<span> </span><a href=\"https://github.com/thunlp/GNNPapers/blob/master\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Lu Chen, Boer Lv, Chi Wang, Su Zhu,\r\nBowen Tan, Kai Yu.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">GraphER:\r\nToken-Centric Entity Resolution with Graph Convolutional Neural\r\nNetworks.</strong><span> </span>AAAI\r\n2020.<span> </span><a href=\"https://github.com/thunlp/GNNPapers/blob/master\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Bing Li, Wei Wang, Yifang Sun,\r\nLinhan Zhang, Muhammad Asif Ali, Yi Wang.</em>\r\n</p>\r\n</li>\r\n<li style=\"box-sizing: border-box; margin-top: 0.25em;\">\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<strong style=\"box-sizing: border-box; font-weight: 600;\">CFGNN:Cross\r\nFlow Graph Neural Networks for Question Answering on Complex\r\nTables.</strong><span> </span>AAAI\r\n2020.<span> </span><a href=\"https://github.com/thunlp/GNNPapers/blob/master\" style=\"box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;\">paper</a>\r\n</p>\r\n<p dir=\"auto\" style=\"box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;\">\r\n<em style=\"box-sizing: border-box;\">Xuanyu Zhang.</em>\r\n</p>\r\n</li>\r\n</ol>\r\n</details>\r\n<h1 id=\"graph-neural-networks-for-natural-language-processing-a-survey\">Graph\r\nNeural Networks for Natural Language Processing: A Survey</h1>\r\n<h2 id=\"introduction\">Introduction</h2>\r\n<ul>\r\n<li><p>There is a rich variety of NLP problems that can be best\r\nexpressed with a graph structure.</p>\r\n<ul>\r\n<li>the <strong>sentence structure information</strong> in text\r\nsequence(i.e. syntactic parsing trees like dependency and constituency\r\nparsing trees) can be exploited to augment original sequence data by\r\nincorporating the task-specific knowledge.</li>\r\n<li>the <strong>semantic information</strong> in sequence data (i.e.\r\nsemantic parsing graphs like Abstract Meaning Representation graphs and\r\nInformation Extraction graphs) can be leveraged to enhance original\r\nsequence data as well.</li>\r\n</ul></li>\r\n<li><p>Therefore, these graph-structured data can encode complicated\r\npairwise relationships between entity tokens for learning more\r\ninformative representations.</p></li>\r\n<li><p>Deep learning techniques that were disruptive for Euclidean data\r\n(e.g, images) or sequence data (e.g, text) are not immediately\r\napplicable to graph-structured data, due to the complexity of graph data\r\nsuch as irregular structure and varying size of node neighbors.</p></li>\r\n<li><p>As a result, this gap has driven a tide in research for deep\r\nlearning on graphs, especially in development of graph neural networks\r\n(GNNs)</p>\r\n<ul>\r\n<li>classification tasks</li>\r\n<li>generation tasks</li>\r\n</ul></li>\r\n<li><p>Despite the successes these existing research has achieved, deep\r\nlearning on graphs for NLP still encounters many challenges, namely,</p>\r\n<ul>\r\n<li><p>Automatically transforming original text sequence data into\r\nhighly graph-structured data.</p>\r\n<blockquote>\r\n<p>Automatic graph construction from the text sequence to utilize the\r\nunderlying structural information is a crucial step in utilizing graph\r\nneural networks for NLP problems.</p>\r\n</blockquote></li>\r\n<li><p>Properly determining graph representation learning\r\ntechniques.</p>\r\n<blockquote>\r\n<p>It is critical to come up with specially-designed GNNs to learn the\r\nunique characteristics of different graph-structures data such as\r\nundirected, directed, multi-relational and heterogeneous graphs.</p>\r\n</blockquote></li>\r\n<li><p>Effectively modeling complex data.</p>\r\n<blockquote>\r\n<p>Such challenge is important since many NLP tasks involve learning the\r\nmapping between the graph-based inputs and other highly structured\r\noutput data such as sequences, trees, as well as graph data with\r\nmulti-types in both nodes and edges.</p>\r\n</blockquote></li>\r\n</ul></li>\r\n<li><p>The taxonomy, which systematically organizes GNNs for NLP along\r\nthree axes: graph construction, graph representation learning,\r\nencoder-decoder models, and the applications.<img src=\"/2023/01/16/GNN4NLP/image-20221009143334468-167384297065818.png\" alt=\"image-20221009143334468\"></p></li>\r\n</ul>\r\n<h2 id=\"graph-based-algorithms-for-natural-language-processing\">Graph\r\nBased Algorithms for Natural Language Processing</h2>\r\n<h4 id=\"tips\">Tips</h4>\r\n<h6 id=\"省流\">省流</h6>\r\n<blockquote>\r\n<p>本节我们从图的视角审视NLP的问题，并且简单地介绍了许多相对传统的基于图解决NLP问题的方法</p>\r\n</blockquote>\r\n<h3 id=\"natural-language-processing-a-graph-perspective\">Natural\r\nLanguage Processing: A Graph Perspective</h3>\r\n<h4 id=\"three-ways-to-representing-natural-language\">Three ways to\r\nrepresenting natural language</h4>\r\n<ul>\r\n<li><p>a bag of tokens</p>\r\n<blockquote>\r\n<p>This view of natural language completely ignores the specific\r\n<strong>positions</strong> of tokens appearing in text, and only\r\nconsiders <strong>how many times a unique token appears in\r\ntext</strong>. If one randomly shuffles a given text, the meaning of the\r\ntext does not change at all from this perspective.</p>\r\n</blockquote>\r\n<ul>\r\n<li><p>topic modeling</p>\r\n<blockquote>\r\n<p>It aims to model each input text as a mixture of topics where each\r\ntopic can be further modeled as a mixture of words.</p>\r\n</blockquote></li>\r\n</ul></li>\r\n<li><p>a sequence of tokens</p>\r\n<blockquote>\r\n<p>This is how human beings normally speak and write natural\r\nlanguage.</p>\r\n<p>this view of natural language is able to capture richer information\r\nof text, e.g., which two tokens are <strong>consecutive</strong> and how\r\nmany times a word pair <strong>co-occurs</strong> in local context.</p>\r\n</blockquote>\r\n<ul>\r\n<li>linear-chain CRF</li>\r\n<li>word2vec</li>\r\n</ul></li>\r\n<li><p>a graph</p>\r\n<blockquote>\r\n<p>While it is probably most apparent to regard text as sequential data,\r\nin the NLP community, there is a long history of representing text as\r\nvarious kinds of graphs.</p>\r\n<p>Common graph representations of text or world knowledge include\r\ndependency graphs, constituency graphs, AMR graphs, IE graphs, lexical\r\nnetworks, and knowledge graphs.</p>\r\n<p>Besides, one can also construct a text graph containing\r\n<strong>multiple hierarchies of elements</strong> such as document,\r\npassage, sentence and word.</p>\r\n<p>In comparison with the above two perspectives, this view of natural\r\nlanguage is able to <strong>capture richer relationships</strong> among\r\ntext elements.</p>\r\n</blockquote></li>\r\n</ul>\r\n<h3 id=\"graph-based-methods-for-natural-language-processing\">Graph Based\r\nMethods for Natural Language Processing</h3>\r\n<h6 id=\"省流-1\">省流</h6>\r\n<blockquote>\r\n<p>介绍各种已经成功应用于NLP的经典的基于图的算法</p>\r\n<ul>\r\n<li>首先介绍各种算法</li>\r\n<li>然后讨论它们和GNNs的联系</li>\r\n</ul>\r\n</blockquote>\r\n<ul>\r\n<li><p>Random Walk Algorithms</p>\r\n<h5 id=\"解释\">解释</h5>\r\n<blockquote>\r\n<ul>\r\n<li>是一组基于图的算法，可以在图中产生随机的路径</li>\r\n<li>从任意节点出发，根据特定的转移概率重复地随机选择邻居节点并转移</li>\r\n<li>在随机游走中所有经过的节点就构成了一条随机路径</li>\r\n<li>随机游走收敛后，可以得到一个图中所有节点的平稳分布\r\n<ul>\r\n<li>可以通过概率分数排序来选择图中结构重要性最高的节点，</li>\r\n<li>可以通过计算两个随机游走分布之间的相似性衡量两个图的相关性</li>\r\n</ul></li>\r\n</ul>\r\n</blockquote>\r\n<ul>\r\n<li><p>TextRank: Bringing Order into Texts</p>\r\n<h4 id=\"例子详解\">例子详解</h4>\r\n<blockquote>\r\n<p><strong>基本思想</strong>：利用图形递归绘制全局信息，从而决定顶点的重要程度</p>\r\n<ul>\r\n<li><p>text2graph</p>\r\n<p>为了使基于图的排序算法可以应用于自然语言文本，需要将文本表示成图，并将具有语义关系的单词或其他文本实体用边相连</p></li>\r\n<li><p>pipeline</p>\r\n<ul>\r\n<li>确定最佳定义手头任务的<strong>文本单位</strong>，并将其作为顶点添加到图形中</li>\r\n<li>识别连接这些文本单元的<strong>关系</strong>，并使用这些关系在图中的顶点之间绘制边。\r\n边可以是有向的或无向的，加权的或未加权的</li>\r\n<li>迭代基于图的排序算法，直到<strong>收敛</strong></li>\r\n<li>根据最终得分对顶点进行<strong>排序</strong>。\r\n使用附加到每个顶点的值进行排名 / 选择决策</li>\r\n</ul></li>\r\n<li><p>核心算法</p>\r\n<ul>\r\n<li>通过随机游走来生成随机路径，并按照以下公式更新节点，直到收敛到稳定分布，然后排序即可\r\n<span class=\"math display\">\\[\r\nW S\\left(V_i\\right)=(1-d)+d * \\sum_{V_j \\in\r\n\\operatorname{In}\\left(V_i\\right)} \\frac{w_{j i}}{\\sum_{V_k \\in O u\r\nt\\left(V_j\\right)}{w_{j k}}} W S\\left(V_j\\right)\r\n\\]</span></li>\r\n</ul></li>\r\n<li><p>关键词抽取</p>\r\n<ul>\r\n<li>将通过一定规则提取的一系列<strong>词</strong>加入节点，词之间的关系（任何关系都可以）作为边。</li>\r\n<li>在本任务中，选择<strong>共现关系</strong>作为关系的标准。</li>\r\n<li>具体而言，两个节点如果出现在一个<strong>窗口</strong>内，那么这两个词之间有一条边。窗口从\r\n2 到 N（最大值）</li>\r\n<li>在构建图形（无向未加权图）之后，将与每个顶点相关联的分数设置为初始值\r\n1，并且根据<strong>排序算法</strong>在图上运行多次迭代，直到它<strong>收敛</strong>\r\n- 通常为 20- 30 次迭代，阈值为 0.0001</li>\r\n<li>获得了图中每个顶点的<strong>最终分数</strong>，顶点按其分数的相反顺序进行排序，并保留排名中的前\r\n<strong>top T</strong> 顶点进行后期处理</li>\r\n<li>在后期处理期间，所有词汇单位被选中作为 TextRank\r\n算法的潜在关键词在文本中标记，并显示相邻关键字的序列被折叠成多字关键字</li>\r\n<li>效果：<img src=\"/2023/01/16/GNN4NLP/v2-1e430ba69797cf8242d2885f99b350bd_b-167384301715396.jpg\" alt=\"img\"></li>\r\n</ul></li>\r\n<li><p>句子提取</p>\r\n<ul>\r\n<li><p>节点是text中的每个句子，需要过滤(通过词性和长度来过滤)</p></li>\r\n<li><p>边的权重是句子之间的Similarity(使用单词重叠度来衡量) <span class=\"math display\">\\[\r\n\\operatorname{Similarity}\\left(S_i, S_j\\right)=\\frac{\\left|\\left\\{w_k\r\n\\mid w_k \\in S_i \\&amp; w_k \\in S_j\\right\\}\\right|}{\\log\r\n\\left(\\left|S_i\\right|\\right)+\\log \\left(\\left|S_j\\right|\\right)}\r\n\\]</span></p></li>\r\n<li><p>效果：<img src=\"/2023/01/16/GNN4NLP/image-20221009160825337-167384297065819.png\" alt=\"image-20221009160825337\"></p></li>\r\n</ul></li>\r\n</ul>\r\n</blockquote></li>\r\n</ul></li>\r\n<li><p>Graph Clustering Algorithms</p>\r\n<ul>\r\n<li><p>Spectral clustering(谱聚类)</p>\r\n<h5 id=\"相关知识定义\">相关知识定义</h5>\r\n<blockquote>\r\n<p><strong>无向权重图</strong></p>\r\n<p>图<span class=\"math inline\">\\(G\\)</span>通常表示为<span class=\"math inline\">\\(G(V,E)\\)</span>。其中<span class=\"math inline\">\\(V\\)</span>即为我们数据集里面所有的点<span class=\"math inline\">\\((v_1,v_2,...,v_n)\\)</span>。</p>\r\n<p>对<span class=\"math inline\">\\(V\\)</span>中的任意两个点<span class=\"math inline\">\\(v_i\\)</span>和<span class=\"math inline\">\\(v_j\\)</span>，可以有边连接，也可以没有。我们定义权重<span class=\"math inline\">\\(w_{ij}\\)</span>为点<span class=\"math inline\">\\(v_i,v_j\\)</span>之间的权重。由于我们是无向图，所以<span class=\"math inline\">\\(w_{ij} = w_{ji}\\)</span>。其中，<span class=\"math inline\">\\(w_{ij}&gt;0\\)</span>等价于两个点之间有边。<span class=\"math inline\">\\(w_{ij}=0\\)</span>代表没有边。</p>\r\n<p>对图中任意一个点<span class=\"math inline\">\\(v_i\\)</span>,它的度<span class=\"math inline\">\\(d_i\\)</span>定义为和它连接的所有边的权重之和 <span class=\"math display\">\\[\r\nd_i = \\sum_{j=1}^{n}w_{ij}\r\n\\]</span> 利用每个点度的定义，我们得到一个nxn的矩阵<span class=\"math inline\">\\(D\\)</span>,它是一个对角矩阵，只有主对角线有值，对应第i行的第i个点的度数，定义如下：\r\n<span class=\"math display\">\\[\r\n\\mathbf{D}=\\left(\\begin{array}{ccc}\r\nd_1 &amp; \\ldots &amp; \\ldots \\\\\r\n\\ldots &amp; d_2 &amp; \\ldots \\\\\r\n\\vdots &amp; \\vdots &amp; \\ddots \\\\\r\n\\ldots &amp; \\ldots &amp; d_n\r\n\\end{array}\\right)\r\n\\]</span> 利用所有店的权重值，我们可以得到图的邻接矩阵<span class=\"math inline\">\\(W\\)</span>,它也是一个nxn矩阵，第i行的第j个值对应我们的权重<span class=\"math inline\">\\(w_{ij}\\)</span></p>\r\n<p>此外，对点集<span class=\"math inline\">\\(V\\)</span>的一个子集<span class=\"math inline\">\\(A \\subset V\\)</span>,我们定义： <span class=\"math display\">\\[\r\n|A|:=子集A中点的个数\r\n\\newline\r\nvol(A):=\\sum_{i \\in A}d_i\r\n\\]</span> <strong>相似矩阵</strong></p>\r\n<p>获取邻接矩阵的基本思想：距离远的两个点边权值较低，而距离近的两个点之间边权值较高</p>\r\n<p>构建方法：通过样本点距离度量的相似矩阵S来获得邻接矩阵W</p>\r\n<p>1、<span class=\"math inline\">\\(\\epsilon\\)</span>-临近法</p>\r\n<p>设置了一个阈值<span class=\"math inline\">\\(\\epsilon\\)</span>,使用欧氏距离<span class=\"math inline\">\\(s_ij\\)</span>度量任意两点<span class=\"math inline\">\\(x_i\\)</span>和<span class=\"math inline\">\\(x_j\\)</span>之间的距离。即相似矩阵的<span class=\"math inline\">\\(s_{ij} =\r\n\\left\\|x_i-x_j\\right\\|_2^2\\)</span>,然后定义<span class=\"math inline\">\\(W\\)</span>元素如下： <span class=\"math display\">\\[\r\nw_{i j}= \\begin{cases}0 &amp; s_{i j}&gt;\\epsilon \\\\ \\epsilon &amp; s_{i\r\nj} \\leq \\epsilon\\end{cases}\r\n\\]</span> 距离度量不够精准，一般不使用。</p>\r\n<p>2、K临近法</p>\r\n<p>KNN(K-Nearest\r\nNeighbor)算法:计算当前点和其他点的距离，并找到最近的K个点。这K个点中哪个种类数目最多，则将当前点判定为该种类。</p>\r\n<p>如果使用KNN的思想，我们自然地得到只有样本距离最近的K个点之间的<span class=\"math inline\">\\(w_{ij}&gt;0\\)</span>。但是这样得到的<span class=\"math inline\">\\(W\\)</span>是不对称的，为此，我们修改为如下两种形式：\r\n<span class=\"math display\">\\[\r\nw_{ij}=w_{ji}= \\begin{cases} 0&amp; {x_i \\notin KNN(x_j) \\;and \\;x_j\r\n\\notin KNN(x_i)}\\\\ exp(-\\frac{||x_i-x_j||_2^2}{2\\sigma^2})&amp; {x_i \\in\r\nKNN(x_j)\\; or\\; x_j \\in KNN(x_i}) \\end{cases}\r\n\\newline\r\nor\r\n\\newline\r\nw_{ij}=w_{ji}= \\begin{cases} 0&amp; {x_i \\notin KNN(x_j) \\;or\\;x_j\r\n\\notin KNN(x_i)}\\\\ exp(-\\frac{||x_i-x_j||_2^2}{2\\sigma^2})&amp; {x_i \\in\r\nKNN(x_j)\\; and \\; x_j \\in KNN(x_i}) \\end{cases}\r\n\\]</span> 3、全连接法（最普遍）</p>\r\n<p>所有的点之间的权重都大于0。可以选择不同的核函数来定义边权重，比如多项式核函数，高斯核函数和Sigmoid核函数。最常用的是高斯核函数RBF，此时相似矩阵和邻接矩阵相同：\r\n<span class=\"math display\">\\[\r\nw_{ij}=s_{ij}=exp(-\\frac{||x_i-x_j||_2^2}{2\\sigma^2})\r\n\\]</span> <strong>拉普拉斯矩阵</strong> <span class=\"math display\">\\[\r\nL = D-W\r\n\\]</span> 拉普拉斯矩阵=度矩阵 - 邻接矩阵</p>\r\n<p>好的性质：</p>\r\n<ul>\r\n<li><p>对称矩阵（对称矩阵只差是对称矩阵）</p></li>\r\n<li><p>由于拉普拉斯矩阵是对称矩阵，所以它的所有特征值都是实数</p></li>\r\n<li><p>对任意向量<span class=\"math inline\">\\(f\\)</span>,我们有 <span class=\"math display\">\\[\r\nf^TLf = \\frac{1}{2}\\sum\\limits_{i,j=1}^{n}w_{ij}(f_i-f_j)^2\r\n\\]</span> 这个利用拉普拉斯矩阵的定义很容易得到如下： <span class=\"math display\">\\[\r\n\\begin{align*}\\label{12}\r\n&amp; f^TLf \\\\\r\n&amp; = f^TDf - f^TWf \\\\\r\n&amp; = \\sum\\limits_{i=1}^{n}d_if_i^2 -\r\n\\sum\\limits_{i,j=1}^{n}w_{ij}f_if_j \\\\\r\n&amp; =\\frac{1}{2}( \\sum\\limits_{i=1}^{n}d_if_i^2 - 2\r\n\\sum\\limits_{i,j=1}^{n}w_{ij}f_if_j + \\sum\\limits_{j=1}^{n}d_jf_j^2) \\\\\r\n&amp; = \\frac{1}{2}\\sum\\limits_{i,j=1}^{n}w_{ij}(f_i-f_j)^2 \\\\\r\n\\end{align*}\r\n\\]</span></p></li>\r\n<li><p>拉普拉斯矩阵是半正定的，且对应的n个实数特征值都大于0，即<span class=\"math inline\">\\(0 = \\lambda_1 \\leq \\lambda_2 \\leq···\\leq\r\n\\lambda_n\\)</span>，且最小的特征值是0，这个性质由性质3很容易得出。</p></li>\r\n</ul>\r\n<p>关于拉普拉斯算子与图中的拉普拉斯矩阵：</p>\r\n<ul>\r\n<li><p>拉普拉斯算子是自变量的非混合二阶偏导数之和 <span class=\"math display\">\\[\r\n\\Delta f = \\sum_{i=1}^{n}\\frac{\\partial ^2f}{\\partial   x_i^2}\r\n\\]</span> 我们进行离散化，并且简化为增量为1，则有： <span class=\"math display\">\\[\r\n\\Delta f = \\sum_{(k,l) \\in N(i,j)}(f(x_k,y_l)  - f(x_i,y_j))\r\n\\]</span> 其中<span class=\"math inline\">\\(N(i,j)\\)</span>是<span class=\"math inline\">\\((x_i,y_j)\\)</span>的邻居节点的集合</p>\r\n<figure>\r\n<img src=\"/2023/01/16/GNN4NLP/v2-4d7bf05128e4ba55df16a9fd61913a8e_b-167384304011199.jpg\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure></li>\r\n<li><p>推广到图中 <span class=\"math display\">\\[\r\n\\Delta f_i = \\sum_{j \\in N_i}w_{ij}(f_i-f_j)\r\n\\]</span> <img src=\"/2023/01/16/GNN4NLP/v2-1bfc2035f4d2a95d0af6ea0288cd8680_b-1673843042737102.jpg\" alt=\"img\"></p>\r\n<p>如果j不是i的邻居，则<span class=\"math inline\">\\(w_{ij}=0\\)</span>。因此，上面的式子可以写成：\r\n<span class=\"math display\">\\[\r\n\\Delta f_i = \\sum_{j \\in V}w_{ij}(f_i-f_j) = \\sum_{j\\in\r\nV}w_{ij}f_i-\\sum_{j\\in V}w_{ij}f_j = d_if_i -  \\textbf{w}_i\\textbf{f}\r\n\\]</span> 因此，我们得到 <span class=\"math display\">\\[\r\n\\Delta f = (\\textbf{D}-\\textbf{W})\\textbf{f}\r\n\\]</span> 由此我们可以得到拉普拉斯矩阵的定义及以上几个性质。</p></li>\r\n</ul>\r\n<p><strong>无向图切图</strong></p>\r\n<p>对无向图<span class=\"math inline\">\\(G\\)</span>的切图,每个子图的集合为:<span class=\"math inline\">\\(A_1,A_2,...,A_k\\)</span>,它们满足<span class=\"math inline\">\\(A_i \\cap A_j = \\emptyset\\)</span> , 且有<span class=\"math inline\">\\(A_1 \\cup A_2 \\cup ... \\cup A_k = V\\)</span></p>\r\n<p>对于任意两个子图点的集合<span class=\"math inline\">\\(A,B \\subset V, A\r\n\\cap B = \\emptyset\\)</span> , 我们定义两个子图之间的切图权重为 <span class=\"math display\">\\[\r\nW(A, B) = \\sum\\limits_{i \\in A, j \\in B}w_{ij}\r\n\\]</span> 那么对k个子图点的集合：<span class=\"math inline\">\\(A_1,A_2,...,A_k\\)</span>,我们定义切图cut为： <span class=\"math display\">\\[\r\ncut(A_1,A_2,...A_k) = \\frac{1}{2}\\sum\\limits_{i=1}^{k}W(A_i,\r\n\\overline{A}_i )\r\n\\]</span> 其中<span class=\"math inline\">\\(\\overline{A}_i\\)</span>为<span class=\"math inline\">\\(A_i\\)</span>的补集，意为除<span class=\"math inline\">\\(A_i\\)</span>子集外其他V的子集的并集</p>\r\n<p>那么如何切图可以让子图内的点权重和高，子图间的权重和低呢？一个自然地想法是最小化<span class=\"math inline\">\\(cut(A_1,A_2,...,A_k)\\)</span>,但是可以发现，这种方案存在问题：<img src=\"/2023/01/16/GNN4NLP/1042406-20161227180625461-1385841797-1673843046978105.jpg\" alt=\"img\"></p>\r\n<p>这个最小化方案给出的最优解Smallest cut并不是全局的最优解Best\r\ncut。如何解决？——谱聚类之切图聚类</p>\r\n<p><strong>谱聚类之切图聚类</strong></p>\r\n<ul>\r\n<li><p>RatioCut切图</p>\r\n<p>为了避免上述问题发生，需要额外考虑最大化每个子图点的个数： <span class=\"math display\">\\[\r\nRatioCut(A_1,A_2,...A_k) = \\frac{1}{2}\\sum\\limits_{i=1}^{k}\\frac{W(A_i,\r\n\\overline{A}_i )}{|A_i|}\r\n\\]</span> 我们引入指示向量<span class=\"math inline\">\\(h_j \\in \\{h_1,\r\nh_2,..h_k\\}\\; j =1,2,...k\\)</span>,对任意一个向量<span class=\"math inline\">\\(h_j\\)</span>,它是一个n维向量(n是样本数)，其中\r\n<span class=\"math display\">\\[\r\nh_{ij}= \\begin{cases} 0&amp; { v_i \\notin A_j}\\\\\r\n\\frac{1}{\\sqrt{|A_j|}}&amp; { v_i \\in A_j} \\end{cases}\r\n\\]</span> 则有： <span class=\"math display\">\\[\r\n\\begin{align} h_i^TLh_i &amp; =\r\n\\frac{1}{2}\\sum\\limits_{m=1}\\sum\\limits_{n=1}w_{mn}(h_{im}-h_{in})^2\r\n\\\\&amp; =\\frac{1}{2}(\\sum\\limits_{m \\in A_i, n \\notin\r\nA_i}w_{mn}(\\frac{1}{\\sqrt{|A_i|}} - 0)^2 +  \\sum\\limits_{m \\notin A_i, n\r\n\\in A_i}w_{mn}(0 - \\frac{1}{\\sqrt{|A_i|}} )^2\\\\&amp; =\r\n\\frac{1}{2}(\\sum\\limits_{m \\in A_i, n \\notin A_i}w_{mn}\\frac{1}{|A_i|}\r\n+  \\sum\\limits_{m \\notin A_i, n \\in A_i}w_{mn}\\frac{1}{|A_i|}\\\\&amp; =\r\n\\frac{1}{2}(cut(A_i, \\overline{A}_i) \\frac{1}{|A_i|} +\r\ncut(\\overline{A}_i, A_i) \\frac{1}{|A_i|}) \\\\&amp; =  \\frac{cut(A_i,\r\n\\overline{A}_i)}{|A_i|} \\end{align}\r\n\\]</span> 上述第（1）式用了上面第四节的拉普拉斯矩阵的性质3.\r\n第二式用到了指示向量的定义。可以看出，对于某一个子图i，它的RatioCut对应于<span class=\"math inline\">\\(h_i^TLh_i\\)</span>​,那么我们的k个子图呢？对应的RatioCut函数表达式为：\r\n<span class=\"math display\">\\[\r\nRatioCut(A_1,A_2,...A_k) = \\sum\\limits_{i=1}^{k}h_i^TLh_i =\r\n\\sum\\limits_{i=1}^{k}(H^TLH)_{ii} = tr(H^TLH)\r\n\\]</span> 注意到<span class=\"math inline\">\\(H^TH =\r\nI\\)</span>,则我们的切图优化目标为： <span class=\"math display\">\\[\r\n\\underbrace{arg\\;min}_H\\; tr(H^TLH) \\;\\; s.t.\\;H^TH=I\r\n\\]</span> 注意到我们 H 矩阵里面的每一个指示向量都是 n\r\n维的，向量中每个变量的取值为 0 或者 <span class=\"math inline\">\\(\\frac{1}{\\sqrt{|A_j|}}\\)</span>，就有 <span class=\"math inline\">\\(2^n\\)</span>种取值，有 k 个子图的话就有 k\r\n个指示向量，共有 <span class=\"math inline\">\\(k2^n\\)</span>种\r\nH，因此找到满足上面优化目标的 H 是一个 NP\r\n难的问题。那么是不是就没有办法了呢？</p>\r\n<p>　　　　注意观察 <span class=\"math inline\">\\(tr(H^TLH)\\)</span>中每一个优化子目标 <span class=\"math inline\">\\(h_i^TLh_i\\)</span>, 其中 <span class=\"math inline\">\\(h\\)</span>是单位正交基， L 为对称矩阵，此时 <span class=\"math inline\">\\(h_i^TLh_i\\)</span>的最大值为 L\r\n的最大特征值，最小值是 L 的最小特征值。如果你对主成分分析 PCA\r\n很熟悉的话，这里很好理解。在 PCA\r\n中，我们的目标是找到协方差矩阵（对应此处的拉普拉斯矩阵\r\nL）的最大的特征值，而在我们的谱聚类中，我们的目标是找到目标的最小的特征值，得到对应的特征向量，此时对应二分切图效果最佳。也就是说，我们这里要用到维度规约的思想来近似去解决这个\r\nNP 难的问题。</p>\r\n<p>　　　　对于 <span class=\"math inline\">\\(h_i^TLh_i\\)</span>，我们的目标是找到最小的 L\r\n的特征值，而对于 <span class=\"math inline\">\\(tr(H^TLH) =\r\n\\sum\\limits_{i=1}^{k}h_i^TLh_i\\)</span>，则我们的目标就是找到 k\r\n个最小的特征值，一般来说，k 远远小于\r\nn，也就是说，此时我们进行了维度规约，将维度从 n 降到了\r\nk，从而近似可以解决这个 NP 难的问题。</p>\r\n<p>　　　　通过找到 L 的最小的 k 个特征值，可以得到对应的 k\r\n个特征向量，这 k 个特征向量组成一个 nxk 维度的矩阵，即为我们的\r\nH。一般需要对 H 矩阵按行做标准化，即</p>\r\n<p><span class=\"math display\">\\[h_{ij}^{*}=\r\n\\frac{h_{ij}}{(\\sum\\limits_{t=1}^kh_{it}^{2})^{1/2}}\\]</span></p>\r\n<p>　　　　由于我们在使用维度规约的时候损失了少量信息，导致得到的优化后的指示向量\r\nh 对应的 H 现在不能完全指示各样本的归属，因此一般在得到 nxk 维度的矩阵 H\r\n后还需要对每一行进行一次传统的聚类，比如使用 K-Means 聚类.</p>\r\n<ul>\r\n<li>6.2 Ncut 切图</li>\r\n</ul>\r\n<p>　　　　Ncut 切图和 RatioCut 切图很类似，但是把 Ratiocut 的分母 <span class=\"math inline\">\\(|Ai|\\)</span>换成 <span class=\"math inline\">\\(vol(A_i)\\)</span>.\r\n由于子图样本的个数多并不一定权重就大，我们切图时基于权重也更合我们的目标，因此一般来说\r\nNcut 切图优于 RatioCut 切图。</p>\r\n<p><span class=\"math display\">\\[NCut(A_1,A_2,...A_k) =\r\n\\frac{1}{2}\\sum\\limits_{i=1}^{k}\\frac{W(A_i,\r\n\\overline{A}_i )}{vol(A_i)}\\]</span></p>\r\n<p>　　　　，对应的，Ncut 切图对指示向量 <span class=\"math inline\">\\(h\\)</span>做了改进。注意到 RatioCut\r\n切图的指示向量使用的是 <span class=\"math inline\">\\(\\frac{1}{\\sqrt{|A_j|}}\\)</span>标示样本归属，而\r\nNcut 切图使用了子图权重 <span class=\"math inline\">\\(\\frac{1}{\\sqrt{vol(A_j)}}\\)</span>来标示指示向量\r\nh，定义如下:</p>\r\n<p><span class=\"math display\">\\[h_{ij}= \\begin{cases} 0&amp; { v_i\r\n\\notin A_j}\\\\ \\frac{1}{\\sqrt{vol(A_j)}}&amp; { v_i \\in A_j}\r\n\\end{cases}\\]</span></p>\r\n<p>　　　　那么我们对于 <span class=\"math inline\">\\(h_i^TLh_i\\)</span>,\r\n有：</p>\r\n<p><span class=\"math display\">\\[\\begin{align} h_i^TLh_i &amp;\r\n= \\frac{1}{2}\\sum\\limits_{m=1}\\sum\\limits_{n=1}w_{mn}(h_{im}-h_{in})^2\r\n\\\\&amp; =\\frac{1}{2}(\\sum\\limits_{m \\in A_i, n \\notin\r\nA_i}w_{mn}(\\frac{1}{\\sqrt{vol(A_i)}} - 0)^2 +  \\sum\\limits_{m \\notin\r\nA_i, n \\in A_i}w_{mn}(0 - \\frac{1}{\\sqrt{vol(A_i)}} )^2\\\\&amp; =\r\n\\frac{1}{2}(\\sum\\limits_{m \\in A_i, n \\notin\r\nA_i}w_{mn}\\frac{1}{vol(A_i)} +  \\sum\\limits_{m \\notin A_i, n \\in\r\nA_i}w_{mn}\\frac{1}{vol(A_i)}\\\\&amp; = \\frac{1}{2}(cut(A_i,\r\n\\overline{A}_i) \\frac{1}{vol(A_i)} + cut(\\overline{A}_i, A_i)\r\n\\frac{1}{vol(A_i)}) \\\\&amp; =  \\frac{cut(A_i, \\overline{A}_i)}{vol(A_i)}\r\n\\end{align}\\]</span></p>\r\n<p>　　　　推导方式和 RatioCut\r\n完全一致。也就是说，我们的优化目标仍然是</p>\r\n<p><span class=\"math display\">\\[NCut(A_1,A_2,...A_k) =\r\n\\sum\\limits_{i=1}^{k}h_i^TLh_i = \\sum\\limits_{i=1}^{k}(H^TLH)_{ii} =\r\ntr(H^TLH)\\]</span></p>\r\n<p>　　　　但是此时我们的 <span class=\"math inline\">\\(H^TH \\neq\r\nI\\)</span>, 而是 <span class=\"math inline\">\\(H^TDH =\r\nI\\)</span>。推导如下：</p>\r\n<p><span class=\"math display\">\\[h_i^TDh_i =\r\n\\sum\\limits_{j=1}^{n}h_{ij}^2d_j =\\frac{1}{vol(A_i)}\\sum\\limits_{j \\in\r\nA_i}d_j= \\frac{1}{vol(A_i)}vol(A_i) =1\\]</span></p>\r\n<p>　　　　也就是说，此时我们的优化目标最终为：</p>\r\n<p><span class=\"math display\">\\[\\underbrace{arg\\;min}_H\\; tr(H^TLH) \\;\\;\r\ns.t.\\;H^TDH=I\\]</span></p>\r\n<p>　　　　此时我们的 H 中的指示向量 <span class=\"math inline\">\\(h\\)</span>并不是标准正交基，所以在 RatioCut\r\n里面的降维思想不能直接用。怎么办呢？其实只需要将指示向量矩阵 H\r\n做一个小小的转化即可。</p>\r\n<p>　　　　我们令 <span class=\"math inline\">\\(H = D^{-1/2}F\\)</span>,\r\n则：<span class=\"math inline\">\\(H^TLH =\r\nF^TD^{-1/2}LD^{-1/2}F\\)</span>，<span class=\"math inline\">\\(H^TDH=F^TF =\r\nI\\)</span>, 也就是说优化目标变成了:</p>\r\n<p><span class=\"math display\">\\[\\underbrace{arg\\;min}_F\\;\r\ntr(F^TD^{-1/2}LD^{-1/2}F) \\;\\; s.t.\\;F^TF=I\\]</span></p>\r\n<p>　　　　可以发现这个式子和 RatioCut 基本一致，只是中间的 L 变成了\r\n<span class=\"math inline\">\\(D^{-1/2}LD^{-1/2}\\)</span>。这样我们就可以继续按照\r\nRatioCut 的思想，求出 <span class=\"math inline\">\\(D^{-1/2}LD^{-1/2}\\)</span>的最小的前 k\r\n个特征值，然后求出对应的特征向量，并标准化，得到最后的特征矩阵 <span class=\"math inline\">\\(F\\)</span>, 最后对 <span class=\"math inline\">\\(F\\)</span>进行一次传统的聚类（比如\r\nK-Means）即可。</p>\r\n<p>　　　　一般来说， <span class=\"math inline\">\\(D^{-1/2}LD^{-1/2}\\)</span>相当于对拉普拉斯矩阵\r\n<span class=\"math inline\">\\(L\\)</span>做了一次标准化，即 <span class=\"math inline\">\\(\\frac{L_{ij}}{\\sqrt{d_i*d_j}}\\)</span></p>\r\n<p><strong>谱聚类算法流程</strong></p>\r\n<p>最常用的相似矩阵的生成方式是基于高斯核距离的全连接方式，最常用的切图方式是\r\nNcut。而到最后常用的聚类方法为 K-Means。下面以 Ncut\r\n总结谱聚类算法流程。</p>\r\n<p>输入：样本集 D=<span class=\"math inline\">\\((x_1,x_2,...,x_n)\\)</span>，相似矩阵的生成方式,\r\n降维后的维度 <span class=\"math inline\">\\(k_1\\)</span>,\r\n聚类方法，聚类后的维度 <span class=\"math inline\">\\(k_2\\)</span></p>\r\n<p>输出： 簇划分 <span class=\"math inline\">\\(C(c_1,c_2,...c_{k_2})\\)</span>.</p>\r\n<ol type=\"1\">\r\n<li><p>根据输入的相似矩阵的生成方式构建样本的相似矩阵 S</p></li>\r\n<li><p>根据相似矩阵 S 构建邻接矩阵 W，构建度矩阵 D</p></li>\r\n<li><p>计算出拉普拉斯矩阵 L</p></li>\r\n<li><p>构建标准化后的拉普拉斯矩阵 <span class=\"math inline\">\\(D^{-1/2}LD^{-1/2}\\)</span></p></li>\r\n<li><p>计算 <span class=\"math inline\">\\(D^{-1/2}LD^{-1/2}\\)</span>最小的\r\n<span class=\"math inline\">\\(k_1\\)</span>个特征值所各自对应的特征向量\r\n<span class=\"math inline\">\\(f\\)</span></p></li>\r\n<li><p>将各自对应的特征向量 <span class=\"math inline\">\\(f\\)</span>组成的矩阵按行标准化，最终组成 <span class=\"math inline\">\\(n \\times k_1\\)</span>维的特征矩阵 F</p></li>\r\n<li><p>对 F 中的每一行作为一个 <span class=\"math inline\">\\(k_1\\)</span>维的样本，共 n\r\n个样本，用输入的聚类方法进行聚类，聚类维数为 <span class=\"math inline\">\\(k_2\\)</span>。</p></li>\r\n<li><p>得到簇划分 <span class=\"math inline\">\\(C(c_1,c_2,...c_{k_2})\\)</span>.</p></li>\r\n</ol>\r\n<p><strong>谱聚类总结</strong></p>\r\n<ul>\r\n<li>优点\r\n<ul>\r\n<li>谱聚类只需要数据之间的相似度矩阵，因此对于处理稀疏数据的聚类很有效。这点传统聚类算法比如\r\nK-Means 很难做到</li>\r\n<li>由于使用了降维，因此在处理高维数据聚类时的复杂度比传统聚类算法好。</li>\r\n</ul></li>\r\n<li>缺点\r\n<ul>\r\n<li>如果最终聚类的维度非常高，则由于降维的幅度不够，谱聚类的运行速度和最后的聚类效果均不好。</li>\r\n<li>聚类效果依赖于相似矩阵，不同的相似矩阵得到的最终聚类效果可能很不同。</li>\r\n</ul></li>\r\n</ul></li>\r\n</ul>\r\n</blockquote>\r\n<h4 id=\"例子\">例子</h4>\r\n<blockquote>\r\n<p>得到文档的语义图之后，利用聚类算法就可以对文档进行聚类。</p>\r\n</blockquote></li>\r\n</ul></li>\r\n<li><p>Graph Matching Algorithms</p>\r\n<h5 id=\"定义\">定义</h5>\r\n<blockquote>\r\n<p>计算两个图之间的相似度</p>\r\n</blockquote>\r\n<ul>\r\n<li><p>Graph Edit Distance</p>\r\n<h4 id=\"例子-1\">例子</h4>\r\n<blockquote>\r\n<p><strong>基本思想</strong>：计算两个图之间的相似度</p>\r\n<ul>\r\n<li>它将距离计算为将一个图转换为另一个图所需的更改次数（即添加、删除、替换）。</li>\r\n</ul>\r\n</blockquote></li>\r\n</ul>\r\n<h4 id=\"解释与例子\">解释与例子</h4></li>\r\n<li><p>Label Propagation Algorithms</p>\r\n<h5 id=\"定义-1\">定义</h5>\r\n<blockquote>\r\n<p><strong>基本思想</strong>：标签传播算法 (LPAs)\r\n是一类基于图的半监督算法，可将标签从标记的数据点传播到以前未标记的数据点。</p>\r\n<p><strong>操作实现</strong>：</p>\r\n<ul>\r\n<li>基本上，LPAs 通过在图上迭代地传播和聚合标签来操作。</li>\r\n<li>在每次迭代中，每个节点根据其相邻节点拥有的标签更改其标签。</li>\r\n<li>结果，标签信息在图形中扩散。</li>\r\n</ul>\r\n</blockquote>\r\n<h4 id=\"应用\">应用</h4>\r\n<h4 id=\"实例\">实例</h4>\r\n<blockquote>\r\n<ul>\r\n<li>LPA have been widely used in the network science literature for\r\ndiscovering <strong>community structures</strong> in complex\r\nnetworks.</li>\r\n<li>word-sense disambiguation</li>\r\n<li>sentiment analysis</li>\r\n</ul>\r\n<p>这些应用程序通常专注于标记数据稀缺的<strong>半监督学习</strong>设置，并利用\r\nLPA\r\n算法将标签从有限的标记示例传播到大量类似的未标记示例，并<strong>假设类似示例应该具有相似的标签</strong>。</p>\r\n</blockquote></li>\r\n<li><p>传统方法的限制和与GNNs的联系</p>\r\n<ul>\r\n<li><strong>限制</strong>\r\n<ul>\r\n<li>他们的表达能力有限。他们主要专注于捕获图的结构信息，但没有考虑节点和边的特征，这对于许多\r\nNLP 应用程序也非常重要。</li>\r\n<li>传统的基于图的算法没有统一的学习框架。不同的基于图的算法具有非常不同的属性和设置，并且仅适用于某些特定的用例</li>\r\n</ul></li>\r\n<li><strong>联系</strong>\r\n<ul>\r\n<li>因此需要一个学习能力更强的统一的框架</li>\r\n<li>GNNs 作为一类特殊的神经网络，可以对任意图结构数据进行建模</li>\r\n<li>大多数 GNNs\r\n变体可以被视为基于消息传递的学习框架。与传统的基于消息传递的算法（如\r\nLPAs）通过在图上传播标签进行操作不同，GNNs\r\n通常通过通过多个神经层转换、传播和聚合节点/边的特征来操作，以便学习更好的图表示。</li>\r\n<li>作为一种通用的基于图的学习框架，GNNs\r\n可以应用于各种与图相关的任务，例如节点分类、链接预测和图分类。</li>\r\n</ul></li>\r\n</ul></li>\r\n</ul>\r\n<h2 id=\"graph-neural-networks\">Graph Neural Networks</h2>\r\n<figure>\r\n<img src=\"/2023/01/16/GNN4NLP/image-20221009232432760-167384297065920.png\" alt=\"image-20221009232432760\">\r\n<figcaption aria-hidden=\"true\">image-20221009232432760</figcaption>\r\n</figure>\r\n<figure>\r\n<img src=\"/2023/01/16/GNN4NLP/image-20221009232455005-167384297065921.png\" alt=\"image-20221009232455005\">\r\n<figcaption aria-hidden=\"true\">image-20221009232455005</figcaption>\r\n</figure>\r\n<h3 id=\"foundations\">Foundations</h3>\r\n<ul>\r\n<li><p>本质上是图表示学习的模型，可以被应用到关注节点的任务以及关注图的任务中去。</p></li>\r\n<li><p>它学习图中每个节点的embedding，并综合节点embeddings来产生图embedding</p></li>\r\n<li><p>通常节点embedding的学习需要利用节点embedding和图结构 <span class=\"math display\">\\[\r\nh_i(l) = f_{filter}(A,H^{(l-1)})\r\n\\]</span> 其中<span class=\"math inline\">\\(A\\in\\mathbb{R}^{n\\times\r\nn}\\)</span>是图的邻接矩阵，<span class=\"math inline\">\\(H^{(l-1)} = \\{\r\nh_1^{(l-1)},h_2^{(l-1)},...,h_n^{(l-1)} \\} \\in \\mathbb{R}^{n \\times\r\nd}\\)</span>表示第<span class=\"math inline\">\\(l-1\\)</span>GNN层的节点embeddings输入。d是<span class=\"math inline\">\\(h_i^{(l-1)}\\)</span>的维数。我们将上面式子中描述的过程称为<span class=\"math inline\">\\(graph\\ filtering\\)</span>并且<span class=\"math inline\">\\(f_{filter}(·,·)\\)</span>被称为图滤波器。</p>\r\n<ul>\r\n<li>不同的模型只在图滤波器的选择和参数化上有区别</li>\r\n<li>图滤波不改变图的结构，但是会提炼节点embeddings</li>\r\n</ul></li>\r\n<li><p>由于图滤波不会改变图结构，因此我们受CNNs的影响引入池化操作来产生图级的embeddings</p>\r\n<ul>\r\n<li>图池化将图及其节点嵌入作为输入，然后生成一个具有较少节点的较小图及其相应的新节点嵌入。\r\n<span class=\"math display\">\\[\r\nA&#39;,H&#39; = f_{pool}(A,H)\r\n\\]</span> 其中<span class=\"math inline\">\\(f_{pool}(·,·) \\\r\nA\\in\\mathbb{R}^{n\\times n}\\)</span> 和<span class=\"math inline\">\\(A&#39;\r\n\\in \\mathbb{R}^{n&#39;\\times\r\nn&#39;}\\)</span>是进行池化前后的邻接矩阵。<span class=\"math inline\">\\(H,H&#39;\\)</span>则是池化前后的节点embeddings。在绝大多数情况下，将<span class=\"math inline\">\\(n&#39;\\)</span>设置成1来获取整个图的embedding</li>\r\n</ul></li>\r\n</ul>\r\n<h3 id=\"methodologies\">Methodologies</h3>\r\n<p><strong>Graph Filtering</strong></p>\r\n<h6 id=\"基础数学知识\">基础数学知识</h6>\r\n<blockquote>\r\n<p><strong>谱图理论（Spectral Graph Theory）</strong></p>\r\n<p>通过分析图的拉普拉斯矩阵来学习图的性质。</p>\r\n<ul>\r\n<li><p>Laplacian Matrix</p>\r\n<ul>\r\n<li>对称</li>\r\n<li>半正定</li>\r\n<li>0特征向量的数量等于图中联通分量的数量</li>\r\n</ul></li>\r\n<li><p>Graph Signal Processing</p>\r\n<ul>\r\n<li><p>graph signal</p>\r\n<p>一个graph signal由图和一个定义在图域中的映射函数<span class=\"math inline\">\\(f\\)</span>构成。 <span class=\"math display\">\\[\r\nf: V \\rightarrow \\mathbb{R}^{1\\times d}\r\n\\]</span>\r\n其中d是和每个节点相关的值得维数。不失一般性，我们将d设置为1，从而对所有的节点，我们有$\r\n$</p>\r\n<figure>\r\n<img src=\"/2023/01/16/GNN4NLP/image-20221010154638572-167384297065922.png\" alt=\"image-20221010154638572\">\r\n<figcaption aria-hidden=\"true\">image-20221010154638572</figcaption>\r\n</figure>\r\n<p>一个一维的图信号实例</p>\r\n<p>如果连接节点中的值相似，则图是<strong>平滑</strong>的。一个平滑的图信号的<strong>频率（frequency）</strong>会更低，因为值在图上通过边的变化是缓慢的。</p>\r\n<p>可以用<span class=\"math inline\">\\(\\textbf{f}^TL\\textbf{f}\\)</span>来表征信号<span class=\"math inline\">\\(\\textbf{f}\\)</span>的平滑程度，称为图信号的smoothness/frequency。</p>\r\n<p>图信号和经典的信号一样，都可以在<strong>时域(time\r\ndomain)</strong>和<strong>频域(frequency\r\ndomain)</strong>中表示。</p></li>\r\n<li><p>Graph Fourier Transform <span class=\"math display\">\\[\r\n\\hat{f} ( \\xi ) = \\lt f ( t ) , e x p ( - 2 \\pi i t \\xi ) \\gt = \\int _ {\r\n- \\infty } ^ { \\infty } f ( t ) e x p ( - 2 \\pi i t\\xi ) d t\r\n\\]</span> 经典的傅里叶变换将信号<span class=\"math inline\">\\(f(t)\\)</span>分解为了一系列任意实数<span class=\"math inline\">\\(\\xi\\)</span>的复指数<span class=\"math inline\">\\(exp(-2\\pi i t \\xi)\\)</span>。其中，<span class=\"math inline\">\\(\\xi\\)</span>可以看做是对应的指数项的频率。</p>\r\n<p>这些指数可以看成是<strong>拉普拉斯算子</strong>的<strong>特征函数</strong>。\r\n<span class=\"math display\">\\[\r\n{ \\nabla  }   exp ( - 2 \\pi i t \\xi) = \\frac { \\partial ^ { 2 } } {\r\n\\partial t ^ { 2 } }  }exp ( - 2 \\pi i t \\xi) \\\\\r\n{ = \\frac { \\partial } { \\partial t } ( - 2 \\pi i \\xi ) e x p ( - 2 \\pi\r\ni t\\xi ) } \\\\ { = - ( 2 \\pi i  \\xi ) ^ { 2 } { e x p ( - 2\\pi i t \\xi) }\r\n\\]</span> 类似的，图傅里叶变换可以表示为： <span class=\"math display\">\\[\r\n\\hat{\\textbf{f}}[l] =\r\n&lt;\\textbf{f},\\textbf{u}_l&gt;=\\sum_{i=1}^{N}\\textbf{f}[i]\\textbf{u}_l[i]\r\n\\]</span> 其中<span class=\"math inline\">\\(\\textbf{u}_l\\)</span>是图的拉普拉斯矩阵的第<span class=\"math inline\">\\(l\\)</span>个特征向量。对应的特征值<span class=\"math inline\">\\(\\lambda_l\\)</span>代表特征向量的frequency/smoothness。</p>\r\n<p>计算得到的<span class=\"math inline\">\\(\\hat{\\textbf{f}}\\)</span>就成为了信号<span class=\"math inline\">\\(\\textbf{f}\\)</span>的图傅里叶变换的结果。 <span class=\"math display\">\\[\r\n\\hat{\\textbf{f}} = \\textbf{U}^T\\textbf{f}\r\n\\]</span>\r\n图傅里叶变换可以看做是将输入信号<strong>分解</strong>为<strong>不同频率的图傅里叶基</strong>的过程。得到的<strong>系数</strong><span class=\"math inline\">\\(\\hat{\\textbf{f}}\\)</span>则表示不同傅里叶基对输入信号的贡献程度。是信号在<strong>spectral\r\ndomain</strong>中的表示。</p>\r\n<p>如以下等式所示： <span class=\"math display\">\\[\r\n\\textbf{u}_l^T\\textbf{L}\\textbf{u}_l=\\lambda_l·\\textbf{u}_l^T\\textbf{u}_l=\\lambda_l\r\n\\]</span>\r\n特征向量对应的特征值衡量其smoothness(因为上面的式子本身就是衡量信号在图中的平滑程度).<img src=\"/2023/01/16/GNN4NLP/image-20221010164908386-167384297065923.png\" alt=\"image-20221010164908386\"></p>\r\n<p>同样的，有逆图傅里叶变换过程，会将spectral\r\nrepresentation再转换回spatial representation <span class=\"math display\">\\[\r\n\\textbf{f} = \\sum_{l=1}^N\\hat{f}[l]\\textbf{u}_l[i]\r\n\\]</span> 即<span class=\"math inline\">\\(\\textbf{f} =\r\n\\textbf{U}\\hat{\\textbf{f}}\\)</span>。</p>\r\n<figure>\r\n<img src=\"/2023/01/16/GNN4NLP/image-20221010170329407-167384297066024.png\" alt=\"image-20221010170329407\">\r\n<figcaption aria-hidden=\"true\">image-20221010170329407</figcaption>\r\n</figure>\r\n<p>图 2.5 显示了空间域和频谱域中的图形信号。具体来说，图 2.5a\r\n显示了空间域中的图形信号，图 2.5b 显示了频谱域中的相同图形信号。在图\r\n2.5b 中，x 轴是图傅立叶基，y 轴表示相应的图傅立叶系数。</p></li>\r\n</ul></li>\r\n</ul>\r\n</blockquote>\r\n<ul>\r\n<li><p>Spectral-based Graph Filters</p>\r\n<h5 id=\"定义-2\">定义</h5>\r\n<blockquote>\r\n<ul>\r\n<li><p>Graph Spectral Filtering</p>\r\n<figure>\r\n<img src=\"/2023/01/16/GNN4NLP/image-20221010143237008-167384297066025.png\" alt=\"image-20221010143237008\">\r\n<figcaption aria-hidden=\"true\">image-20221010143237008</figcaption>\r\n</figure>\r\n<p>图谱滤波的思想是调制图信号的频率，使得其中的一些频率组成保持或增强，其他的则被移除或减弱。</p>\r\n<p>因此，给定一个图信号$ ^{N}$,我们首先需要在信号上应用<strong>Graph\r\nFourier Transform(GFT)</strong>来得到它的<strong>graph Fourier\r\ncoefficients(图傅里叶系数)</strong>。然后，在空间域中重建信号前调整这些图傅里叶系数。</p>\r\n<p>首先我们通过图傅里叶变换得到信号在频域中的系数 <span class=\"math display\">\\[\r\n\\hat{\\textbf{f}} = \\textbf{U}^T\\textbf{f}\r\n\\]</span>\r\n为了调节信号的频率(frequencies)，我们对图傅里叶系数进行如下滤波操作：\r\n<span class=\"math display\">\\[\r\n\\hat{\\textbf{f}}&#39;[i] =\r\n\\hat{\\textbf{f}}[i]·\\gamma(\\lambda_i),for\\quad i=1,...,N\r\n\\]</span> 其中<span class=\"math inline\">\\(\\gamma(\\lambda_i)\\)</span>是以<span class=\"math inline\">\\(\\lambda_i\\)</span>为输入的，来决定对应频率成分如何调节的函数。这个过程可以被下面的矩阵形式表述：\r\n<span class=\"math display\">\\[\r\n\\hat{\\textbf{f}} ^ { \\prime } = \\gamma ( \\Lambda ) \\cdot\r\n\\hat{\\textbf{f}} = \\gamma ( \\Lambda ) \\cdot \\textbf{U} ^ { T }\r\n\\textbf{f}\r\n\\]</span> 其中<span class=\"math inline\">\\(\\Lambda\\)</span>是一个由频率构成的对角矩阵(拉普拉斯矩阵的特征值)，并且<span class=\"math inline\">\\(\\gamma(\\Lambda)\\)</span>会将函数<span class=\"math inline\">\\(\\gamma()\\)</span>应用到对角矩阵<span class=\"math inline\">\\(\\Lambda\\)</span>中的每一个元素上。</p>\r\n<figure>\r\n<img src=\"/2023/01/16/GNN4NLP/image-20221011132228704-167384297066126.png\" alt=\"image-20221011132228704\">\r\n<figcaption aria-hidden=\"true\">image-20221011132228704</figcaption>\r\n</figure>\r\n<p>得到过滤后的系数后，我们就可以用逆图傅里叶变换将信号重建会图域中：\r\n<span class=\"math display\">\\[\r\n\\textbf{f}^{\\prime} =\r\n\\textbf{U}\\hat{\\textbf{f}}^{\\prime}=\\textbf{U}\\cdot\\gamma(\\Lambda)\\cdot\\textbf{U}^T\\textbf{f}\r\n\\]</span> 其中<span class=\"math inline\">\\(\\textbf{f}^{\\prime}\\)</span>是得到的过滤后的图信号。这个过滤的过程可以看做是将算子<span class=\"math inline\">\\(\\textbf{U}\\cdot\\gamma(\\Lambda)\\cdot\\textbf{U}^\\top\\)</span>应用于输入的图信号。</p>\r\n<p>便利起见，我们有时将函数<span class=\"math inline\">\\(\\gamma(\\Lambda)\\)</span>称为滤波器，因为它控制了图信号中的频率分量是如何被过滤的。比如，如果<span class=\"math inline\">\\(\\gamma(\\lambda_i)\\)</span>等于0，然后<span class=\"math inline\">\\(\\hat{\\textbf{f}}^{\\prime}[i]=0\\)</span>这就意味着信号<span class=\"math inline\">\\(\\textbf{f}\\)</span>中的频率组分<span class=\"math inline\">\\(\\textbf{u}_i\\)</span>被移除了。</p></li>\r\n<li><p>Spectral-based Graph Filter</p>\r\n<p>如果我们想要图信号更加平滑，我们可以将<span class=\"math inline\">\\(\\gamma(\\Lambda)\\)</span>设置成低通滤波器。但是，在图神经网络中，我们不知道哪些频率是更重要的，因此我们需要学习出滤波器的具体形式。更详细地说，我们选定<span class=\"math inline\">\\(\\gamma(\\Lambda)\\)</span>为特定的函数，然后学习其中的参数。</p>\r\n<p>最只有的方式，是对每一个特征值都有一个对应的函数 <span class=\"math display\">\\[\r\n\\gamma(\\lambda_l) = \\theta_l\r\n\\]</span> 但是，这种滤波器会有很大的局限性：</p>\r\n<ul>\r\n<li>参数过多，等于节点的个数。现实世界中的图的节点会过多</li>\r\n<li>滤波器<span class=\"math inline\">\\(\\textbf{U}\\cdot\\gamma(\\Lambda)\\cdot\\textbf{U}^\\top\\)</span>很可能是一个稠密矩阵，因此输出信号<span class=\"math inline\">\\(\\textbf{f}^{\\prime}\\)</span>的第i个元素会和图中的所有节点有关。即算子并非<strong>空间局部（spatially\r\nlocalized）</strong>的。此外，由于拉普拉斯矩阵的特征分解和稠密矩阵的矩阵乘法使得计算成本很昂贵。</li>\r\n</ul>\r\n<p>为了解决这个问题，一个<strong>多项式滤波算子——Poly-Filter</strong>被提出了。\r\n<span class=\"math display\">\\[\r\n\\gamma(\\lambda_l) = \\sum_{k=0}^{K}\\theta_k\\lambda_l^k\r\n\\newline\r\n\\gamma(\\Lambda) = \\sum_{k=0}^{K}\\theta_k\\Lambda^k\r\n\\]</span> 此时，同阶的是共用同一个参数<span class=\"math inline\">\\(\\theta\\)</span>的，因此参数个数是K+1，并不依赖于图节点的个数。</p>\r\n<p>同时，滤波算子<span class=\"math inline\">\\(\\textbf{U}\\cdot\\gamma({\\Lambda})\\cdot\\textbf{U}^\\top\\)</span>可以表示成拉普拉斯矩阵的多项式。这意味着：</p>\r\n<ul>\r\n<li>不需要再进行拉普拉斯矩阵的特征分解</li>\r\n<li>多项式参数化滤波算子是<strong>空间局部化</strong>的，即输出 f'\r\n的每个元素的计算只涉及图中的少量节点。</li>\r\n</ul>\r\n<p>应用Poly-Filter，我们可以得到过滤后的图信号如下：<img src=\"/2023/01/16/GNN4NLP/image-20221011144220776-167384297066127.png\" alt=\"image-20221011144220776\"></p>\r\n<p>其中<img src=\"/2023/01/16/GNN4NLP/image-20221011144520825-167384297066128.png\" alt=\"image-20221011144520825\"></p>\r\n<p>因此，我们有：<img src=\"/2023/01/16/GNN4NLP/image-20221011144758370-167384297066129.png\" alt=\"image-20221011144758370\"></p>\r\n<p>拉普拉斯矩阵的多项式都是稀疏的。同时，只有当节点<span class=\"math inline\">\\(v_i\\)</span>和节点<span class=\"math inline\">\\(v_j\\)</span>之间的最短路径长度即<span class=\"math inline\">\\(dis(v_i, v_j)\\)</span>小于或等于k时，<span class=\"math inline\">\\(\\textbf{L}^k\\)</span>的第i，j个元素才非零。(可以用归纳法进行证明)</p>\r\n<p>下面我们来关注输出的过滤后的信号的单个元素和哪些节点有关： <span class=\"math display\">\\[\r\n\\textbf{f}^\\prime[i] = \\sum_{v_j\\in\r\nV}(\\sum_{k=0}^K\\theta_k\\textbf{L}_{i,j}^k)\\textbf{f}[j]\r\n\\]</span>\r\n根据上面的结论我们知道，只有在K跳之内的节点才能参与计算，并不是图中的所有节点都参与计算的:<img src=\"/2023/01/16/GNN4NLP/image-20221011150648322-167384297066130.png\" alt=\"image-20221011150648322\"><img src=\"/2023/01/16/GNN4NLP/image-20221011150702333-167384297066131.png\" alt=\"image-20221011150702333\"></p>\r\n<p>其中<span class=\"math inline\">\\(N^K(v_i)\\)</span>代表节点K跳内的邻居节点；<span class=\"math inline\">\\(dis(v_i,v_j)\\)</span>代表节点之间最短路径的长度。</p>\r\n<p>Poly-Filter是在空间局部的，因为只和最邻近的K跳节点有关；也可以看做是空间滤波器，因为可以用空间结构表出（如上面的式子所示）。</p>\r\n<p>尽管Poly-Filter有很多优势，它也有很多局限性：</p>\r\n<ul>\r\n<li>多项式的基(<span class=\"math inline\">\\(1,x,x^2,...\\)</span>)彼此是不正交的。</li>\r\n<li>这就意味着系数彼此是相互依赖的，会导致一个节点的更新可能会导致其他节点的变动</li>\r\n</ul>\r\n<p>为了解决这个问题——<strong>Chebyshev Polynomial and\r\nCheby-Filter</strong></p>\r\n<p>切比雪夫多项式<span class=\"math inline\">\\(T_k(y)\\)</span>可以根据下面的递归关系产生 <span class=\"math display\">\\[\r\nT_k(y) = 2yT_{k-1}(y)-T_{k-2}(y)\r\n\\]</span> 其中，<span class=\"math inline\">\\(T_0(y)\\)</span>=1且<span class=\"math inline\">\\(T_1(y) = y\\)</span>。对于<span class=\"math inline\">\\(y \\in\r\n[-1,1]\\)</span>，这些切比雪夫多项式可以被表示成三角形式 <span class=\"math display\">\\[\r\nT_k(y) = cos(k\\ arccos(y))\r\n\\]</span> 而且这些切比雪夫多项式还满足如下的关系：</p>\r\n<figure>\r\n<img src=\"/2023/01/16/GNN4NLP/image-20221011161701031-167384297066132.png\" alt=\"image-20221011161701031\">\r\n<figcaption aria-hidden=\"true\">image-20221011161701031</figcaption>\r\n</figure>\r\n<p>其中<span class=\"math inline\">\\(\\delta_{l,m}\\)</span>=1当且仅当l=m时成立，否则为0；该式子表明了切比雪夫多项式是相互正交的。因此，切比雪夫多项式组成了希尔伯特空间中平方可积函数的一组正交基，在<span class=\"math inline\">\\(dy/\\sqrt{1-y^2}\\)</span>的度量下。记作<span class=\"math inline\">\\(L^2([-1,1],dy/\\sqrt1-y^2)\\)</span></p>\r\n<p>由于切比雪夫多项式的域是[-1,1],为了用切比雪夫多项式来近似滤波器，我们重新缩放和移动拉普拉斯矩阵的特征值如下：\r\n<span class=\"math display\">\\[\r\n\\widetilde{\\lambda}_l = \\frac{2\\cdot\\lambda_l}{\\lambda_{max}}-1\r\n\\]</span> 其中<span class=\"math inline\">\\(\\lambda_{max}\\)</span>也就是最大的特征值<span class=\"math inline\">\\(\\lambda_N\\)</span>。这样一来，所有的特征值都被改变到[-1,1]范围内了。矩阵层面的表示为：\r\n<span class=\"math display\">\\[\r\n\\widetilde{\\Lambda} = \\frac{2\\cdot\\Lambda}{\\lambda_{max}}-I\r\n\\]</span> 用截断的 切比雪夫(Chebyshev)多项式参数化的 Chevy-Filter\r\n可以表述如下： <span class=\"math display\">\\[\r\n\\gamma(\\Lambda) = \\sum_{k=0}^{K}\\theta_kT_k(\\widetilde{\\Lambda})\r\n\\]</span> 因此，对图信号使用Cheby-Filter的滤波过程可以表示为：<img src=\"/2023/01/16/GNN4NLP/image-20221011195033784-167384297066133.png\" alt=\"image-20221011195033784\"></p>\r\n<p>通过数学归纳法容易证明 <span class=\"math display\">\\[\r\n\\textbf{U}T_k(\\widetilde{\\Lambda})\\textbf{U}^\\top =\r\nT_k(\\widetilde{\\textbf{L}})\r\n\\newline\r\n\\widetilde{\\textbf{L}} = \\frac{2\\textbf{L}}{\\Lambda_{max}} - \\textbf{I}\r\n\\]</span> 此时，Cheby-Filter可以写成：<img src=\"/2023/01/16/GNN4NLP/image-20221011195716309-167384297066134.png\" alt=\"image-20221011195716309\"></p>\r\n<p>因此，Cheby-Filter 仍然享有 Poly-Filter\r\n的优点，同时在扰动下更稳定。</p>\r\n<p><strong>GCN-Filter: Simplified Cheby-Filter Involving 1-hop\r\nNeighbors</strong></p>\r\n<p>通过将切比雪夫多项式中的K设置成1并逼近<span class=\"math inline\">\\(\\lambda_{max} =\r\n2\\)</span>,GCN-Filter可以从Cheby-Filter中简化得到。</p>\r\n<p>在这样的简化和逼近下，可以简化为：<img src=\"/2023/01/16/GNN4NLP/image-20221011200344382-167384297066135.png\" alt=\"image-20221011200344382\"></p>\r\n<p>相应地，将 GCN-Filter 应用于图信号 f，我们可以得到输出信号\r\nf'，如下所示：<img src=\"/2023/01/16/GNN4NLP/image-20221011200414926-167384297066236.png\" alt=\"image-20221011200414926\"></p>\r\n<p>注意这里的拉普拉斯矩阵是规范化的，即<span class=\"math inline\">\\(\\textbf{L} =\r\n\\textbf{I}-\\textbf{D}^{-\\frac{1}{2}}\\textbf{A}\\textbf{D}^{-\\frac{1}{2}}\\)</span></p>\r\n<p>将<span class=\"math inline\">\\(\\theta=\\theta_0=-\\theta_1\\)</span>带入进行进一步的化简得到：<img src=\"/2023/01/16/GNN4NLP/image-20221011200817824-167384297066237.png\" alt=\"image-20221011200817824\"></p>\r\n<p>由于矩阵<span class=\"math inline\">\\(\\textbf{I}+\\textbf{D}^{-\\frac{1}{2}}\\textbf{A}\\textbf{D}^{-\\frac{1}{2}}\\)</span>的特征值在[0,2]上，当在深度学习中反复使用这个算子的时候可能导致数值不稳定，梯度消失/梯度爆炸。为了解决这个问题，我们使用renormalization\r\ntrick <span class=\"math display\">\\[\r\n\\widetilde{\\textbf{A}} = \\textbf{A} + \\textbf{I}\r\n\\newline\r\n\\widetilde{\\textbf{D}}_{ii} = \\sum_{j}\\widetilde{\\textbf{A}}_{i,j\\cdot}\r\n\\newline\r\n\\widetilde{\\textbf{D}}^{-\\frac{1}{2}}\\widetilde{\\textbf{A}}\\widetilde{\\textbf{D}}^{-\\frac{1}{2}}\r\n\\]</span></p>\r\n<blockquote>\r\n<p>这里怎么理解呢？</p>\r\n<p>我们将这个矩阵抽象成<span class=\"math inline\">\\(A = S\\Lambda\r\nS^{-1}\\)</span>,那么<span class=\"math inline\">\\(A^k\\)</span>可以从<span class=\"math inline\">\\(\\Lambda^{k}\\)</span>中得到 <span class=\"math display\">\\[\r\nu_k = A^ku_0=S\\Lambda^kS^{-1}u_0\r\n\\]</span> 我们令<span class=\"math inline\">\\(S^{-1}u_0 =\r\nc\\)</span>则有：<img src=\"/2023/01/16/GNN4NLP/image-20221011221409703-167384297066238.png\" alt=\"image-20221011221409703\"></p>\r\n<p>当层数比较多的时候，这个式子的结果主要由最大的特征值决定。当特征值大于1的时候，会趋向于无穷；特征值小于1的时候，会趋向于0。正好对应着梯度爆炸和梯度消失两种情况。</p>\r\n<p>而新的用来替换的方案，虽然不是恒等的，但是相当于进行了一种新的标准化。新的方案的特征值分布在[-1,1]上，相较于[0,2]发生梯度消失/梯度爆炸的几率更小。</p>\r\n</blockquote>\r\n<p>此时，我们得到GCN-Filter的算子如下： <span class=\"math display\">\\[\r\n\\textbf{f}^{\\prime} =\r\n\\theta\\widetilde{\\textbf{D}}^{-\\frac{1}{2}}\\widetilde{\\textbf{A}}\\widetilde{\\textbf{D}}^{-\\frac{1}{2}}\\textbf{f}\r\n\\]</span> 对于单个节点，这个过程可以看作是从它的 1\r\n跳邻居聚合信息，其中节点本身也被认为是它的 1 跳邻居。因此，GCN-Filter\r\n也可以看作是一个基于空间的过滤器，它在更新节点特征时只涉及直接连接的邻居。</p>\r\n<p><strong>Graph Filters for Multi-channel Graph Signals</strong></p>\r\n<p>上面我们讨论的都是1-channel的图信号，即每个节点的信号是一个标量。然而，实际上典型的图信号大都是multi-channel的。即每个节点都有一个特征向量。</p>\r\n<p>一个<span class=\"math inline\">\\(d_{in}\\)</span>维度的多通道图信号可以表示为<span class=\"math inline\">\\(\\textbf{F} \\in \\mathbb{R}^{N\\times\r\nd_{in}}\\)</span>。为了将图滤波器扩展到多通道信号上，我们利用来自所有输入通道的信号来生成输出信号，如下所示：\r\n<span class=\"math display\">\\[\r\n\\textbf{f}_{out} =\r\n\\sum_{d=1}^{d_{in}}\\textbf{U}\\cdot\\gamma_d(\\Lambda)\\cdot\\textbf{U}^\\top\\textbf{F}_{:,d}\r\n\\]</span> 其中<span class=\"math inline\">\\(\\textbf{f}_{out} \\in\r\n\\mathbb{R}^N\\)</span>是滤波器的单通道输出。因此，该过程可以被视为在每个输入通道中应用图形过滤器，然后计算它们的结果的总和。</p>\r\n<p>就像经典的卷积神经网络一样，在大多数情况下，使用多个滤波器来过滤输入通道，输出也是多通道信号。假如我们使用<span class=\"math inline\">\\(d_out\\)</span>通道的滤波器，则： <span class=\"math display\">\\[\r\n\\textbf{F}^{\\prime}_{:,j} =\r\n\\sum_{d=1}^{d_{in}}\\textbf{U}\\cdot\\gamma_{j,d}(\\Lambda)\\cdot\\textbf{U}^\\top\\textbf{F}_{:,d}\r\n\\quad for \\ j=1,...,d_{out}\r\n\\]</span> 具体来说，在方程式中的 GCN-Filter\r\n的情况下。这个多通道输入和输出的过程可以简单地表示为： <span class=\"math display\">\\[\r\n\\textbf{F}^{\\prime}_{:,j} =\r\n\\sum_{d=1}^{d_{in}}\\theta_{j,d}\\widetilde{\\textbf{D}}^{-\\frac{1}{2}}\\widetilde{\\textbf{A}}\\widetilde{\\textbf{D}}^{-\\frac{1}{2}}\\textbf{F}_{:,d}\r\n\\quad for \\ j=1,...,d_{out}\r\n\\]</span> 可以进一步写成： <span class=\"math display\">\\[\r\n\\textbf{F}^{\\prime} =\r\n\\widetilde{\\textbf{D}}^{-\\frac{1}{2}}\\widetilde{\\textbf{A}}\\widetilde{\\textbf{D}}^{-\\frac{1}{2}}\\textbf{F}\\Theta\r\n\\]</span> 其中<span class=\"math inline\">\\(\\Theta \\in\r\n\\mathbb{R}^{d_{in}\\times d_{out}}\\)</span>并且<span class=\"math inline\">\\(\\Theta[d,j]=\\theta_{j,d}\\)</span>是和第j个输出通道和第d个输入通道相对应的参数。</p>\r\n<p>具体来说，对于单个节点 <span class=\"math inline\">\\(v_i\\)</span>，等式中的过滤过程也可以表述为以下形式：<img src=\"/2023/01/16/GNN4NLP/image-20221011235610664-167384297066239.png\" alt=\"image-20221011235610664\"></p>\r\n<p>其中<span class=\"math inline\">\\(\\widetilde{d_i}=\\widetilde{\\textbf{D}}_{i,i}\\)</span>并且我们使用<span class=\"math inline\">\\(\\textbf{F}_i\\in \\mathbb{R}^{1\\times\r\nd_{out}}\\)</span>代表的是<span class=\"math inline\">\\(\\textbf{F}\\)</span>的第i行，即节点<span class=\"math inline\">\\(v_i\\)</span>的特征。上述方程式中的过程可以看作是从节点\r\nvi 的 1 跳邻居聚合信息。</p></li>\r\n</ul>\r\n</blockquote></li>\r\n<li><p>Spatial-based Graph Filters</p>\r\n<h5 id=\"再次学习\">再次学习</h5>\r\n<blockquote>\r\n<ul>\r\n<li><p>Filter in the very first GNN</p>\r\n<p>对节点<span class=\"math inline\">\\(v_i\\)</span>而言，它对应的标签可以表示为<span class=\"math inline\">\\(l_i\\)</span>。在过滤的过程中，输入的图特征可以被表示为<span class=\"math inline\">\\(\\textbf{F}\\)</span>,其中<span class=\"math inline\">\\(\\textbf{F}_i\\)</span>是它的第i行，是和节点<span class=\"math inline\">\\(v_i\\)</span>相对应的特征。整个过程可以表示为：\r\n<span class=\"math display\">\\[\r\n\\textbf{F}^{\\prime}_i = \\sum_{v_j \\in N(v_i)}g(l_i,\\textbf{F}_j,l_j)\r\n\\]</span> 其中g()是参数化的函数，称为local transition\r\nfunction，是空间局部的。这个过程对每个节点来说只使用其1跳的邻居节点。</p>\r\n<p>注意，节点的标签信息<span class=\"math inline\">\\(l_i\\)</span>可以被视为<strong>初始的输入信息</strong>，在过滤过程中是<strong>固定的</strong>并被利用。</p></li>\r\n<li><p>GraphSAGE-Filter</p>\r\n<p>对一个单独的节点<span class=\"math inline\">\\(v_i\\)</span>，产生它的新的特征的过程可以描绘如下：<img src=\"/2023/01/16/GNN4NLP/image-20221012145843950-167384297066240.png\" alt=\"image-20221012145843950\"></p>\r\n<p>其中 SAMPLE() 是将一个集合作为输入并从输入中随机采样 S\r\n个元素作为输出的函数，AGGREGATE()\r\n是一个组合来自相邻节点的信息的函数，其中 <span class=\"math inline\">\\(f&#39;_{N_S(v_i)}\\)</span> 表示AGGREGATE()\r\n函数的输出，而 [·,·] 是连接操作。</p>\r\n<p>目前，已经有很多AGGREGATE()函数被提出了：</p>\r\n<ul>\r\n<li><p><strong>Mean aggregator</strong>:\r\n直接对向量逐元素取平均。这和GCN-Filter很像，两者都使用（加权）平均，但区别在于：本方案后续对待节点自身的特征时是拼接上去的，而GCN-Filter则是同等对待，也是加权平均的一部分。</p></li>\r\n<li><p><strong>LSTM aggregator</strong>:\r\n将邻居节点视作一个序列，然后使用LSTM结构进行处理，最后一个单元的输出就作为操作的结果。</p>\r\n<p>由于节点本身不存在序列，因此采用随机序列。</p></li>\r\n<li><p><strong>Pooling operator</strong>:\r\n在进行最大池化操作前首先通过一层神经网络： <span class=\"math display\">\\[\r\n\\textbf{f}^{\\prime}_{N_S(v_i)}=max(\\{ \\alpha(\\textbf{F}_j\\Theta_{pool}\r\n),\\forall v_j \\in N_S(v_i)\\})\r\n\\]</span> 其中max()是逐元素的最大化操作；<span class=\"math inline\">\\(\\Theta_{pool}\\)</span>代表转移矩阵，<span class=\"math inline\">\\(\\alpha()\\)</span>是一个非线性激活函数。</p></li>\r\n</ul>\r\n<p>是空间局部性的，且聚合器是节点间共享的。</p></li>\r\n<li><p>MPNN: 基于空间的图滤波器的通用框架</p>\r\n<p>Message Passing Neural Networks (MPNN)\r\n是一个通用的图神经网络框架。很多基于空间的图滤波器都是它的特殊情况。</p>\r\n<p>对一个节点<span class=\"math inline\">\\(v_i\\)</span>而言，MPNN-Filter按照如下流程更新其特征：<img src=\"/2023/01/16/GNN4NLP/image-20221012154439952-167384297066341.png\" alt=\"image-20221012154439952\"></p>\r\n<p>其中<span class=\"math inline\">\\(M()\\)</span>是message函数，<span class=\"math inline\">\\(U()\\)</span>是更新函数，<span class=\"math inline\">\\(e_{(v_i,v_j)}\\)</span>是可能有的边特征。</p>\r\n<p>M()从节点的邻居节点产生需要传递给该节点的messages.</p>\r\n<p>U()然后通过结合<strong>原始特征</strong>和<strong>来自其邻居的聚合消息</strong>来更新节点\r\nvi 的特征。</p>\r\n<p>如果我们将第一个式子中的<strong>求和</strong>操作替换成一个抽象的操作，这个框架可以更加通用。</p></li>\r\n</ul>\r\n</blockquote></li>\r\n<li><p>Attention-based Graph Filter</p>\r\n<h5 id=\"学习\">学习</h5>\r\n<blockquote>\r\n<p>Self-attention机制被引入到graph attention\r\nnetworks(GAT)中来建立空间图滤波器。我们将GAT中的滤波器称为<strong>GAT-Filter</strong>。</p>\r\n<p>和GCN-Filter类似，GAT-Filter也在更新节点特征的时候使用了来自邻居节点的聚合信息。但是，GCN-Filter仅仅是基于图结构，GAT-Filter则是在执行聚合时尝试<strong>区分邻居的重要性</strong>。</p>\r\n<ul>\r\n<li><p>Pipeline</p>\r\n<ul>\r\n<li>在为节点 <span class=\"math inline\">\\(v_i\\)</span>\r\n生成新特征时，它会关注其所有邻居以生成每个邻居的重要性分数。</li>\r\n<li>然后在聚合过程中采用这些重要性分数作为线性系数。</li>\r\n</ul></li>\r\n<li><p>节点<span class=\"math inline\">\\(v_i\\)</span>和邻居节点及自身的重要性计算方法：\r\n<span class=\"math display\">\\[\r\ne_{ij} = a(\\textbf{F}_i\\Theta,\\textbf{F}_j\\Theta)\r\n\\]</span> 其中<span class=\"math inline\">\\(\\Theta\\)</span>是一个共享的参数矩阵。<span class=\"math inline\">\\(a()\\)</span>是一个共享的注意力函数，它是一个单层的前馈网络：\r\n<span class=\"math display\">\\[\r\na(\\textbf{F}_i\\Theta,\\textbf{F}_j\\Theta) = LeakyReLU(a^\\top\r\n[\\textbf{F}_i\\Theta,\\textbf{F}_j\\Theta])\r\n\\]</span> 其中<span class=\"math inline\">\\([\\cdot,\\cdot]\\)</span>指的是连接操作。<span class=\"math inline\">\\(a\\)</span>是一个参数化的向量。而<span class=\"math inline\">\\(LeakyReLU\\)</span>则是一种非线性激活函数。</p>\r\n<p>由方程式计算的分数在被用作聚合过程中的权重之前需要进行归一化，以将输出表示保持在合理的范围内。\r\n<span class=\"math inline\">\\(v_i\\)</span> 的所有邻居的归一化是通过\r\nsoftmax 层执行的：<img src=\"/2023/01/16/GNN4NLP/image-20221012165608095-167384297066342.png\" alt=\"image-20221012165608095\"></p>\r\n<p><span class=\"math inline\">\\(\\alpha_{ij}\\)</span>就是标准化后的节点<span class=\"math inline\">\\(v_j\\)</span>到节点<span class=\"math inline\">\\(v_i\\)</span>的重要性分数。</p>\r\n<p>有了重要性分数，节点的新的表示的计算过程就可以写成： <span class=\"math display\">\\[\r\n\\textbf{F}_i^{\\prime} = \\sum_{v_j\\in\r\nN(v_i)\\cup\\{v_i\\}}\\alpha_{ij}\\textbf{F}_j\\Theta\r\n\\]</span> 其中<span class=\"math inline\">\\(\\Theta\\)</span>是计算e的时候的同一个参数矩阵。</p>\r\n<p>为了稳定自注意力机制，提出了多头注意力机制。</p>\r\n<p>M个独立的拥有不同的<span class=\"math inline\">\\(\\Theta^m\\)</span>和<span class=\"math inline\">\\(\\alpha_{ij}^m\\)</span>的，按照上式形式的注意力机制被平行地使用，他们的输出之后会连接在一起来产生最终的表示：<img src=\"/2023/01/16/GNN4NLP/image-20221012171329066-167384297066343.png\" alt=\"image-20221012171329066\"></p>\r\n<p>注意力机制的滤波器也是空间局部性的，并且初始版本中各个独立的注意力机制连接前要通过激活函数，这里为了方便进行省略。</p></li>\r\n</ul>\r\n</blockquote></li>\r\n<li><p>Recurrent-based Graph Filter</p>\r\n<p>使用Gated Recurrent Unit(GRU)改进GNN\r\nfilter就得到了<strong>GGNN-Filter</strong>。</p>\r\n<h5 id=\"学习-1\">学习</h5>\r\n<blockquote>\r\n<ul>\r\n<li><p>GGNN-Filter是为边是有向的同时具有不同类型的图而设计的。</p></li>\r\n<li><p>对边<span class=\"math inline\">\\((v_i,v_j)\\)</span>而言，我们使用<span class=\"math inline\">\\(tp(v_i,v_j)\\)</span>来表示其类型。过滤过程可以表示为：<img src=\"/2023/01/16/GNN4NLP/image-20221012194112426-167384297066344.png\" alt=\"image-20221012194112426\"></p>\r\n<p>其中<span class=\"math inline\">\\(\\Theta\\)</span>型的变量都是需要去学习的参数。第一步是要从传入邻居节点和传出邻居节点中聚合信息。在这个聚合过程中，转移矩阵<span class=\"math inline\">\\(\\Theta_{tp(v_j,v_i)}^e\\)</span>是被所有类型为<span class=\"math inline\">\\(tp(v_i,v_j)\\)</span>的边连接到节点<span class=\"math inline\">\\(v_i\\)</span>的节点共享的。</p>\r\n<p>剩下的四个式子是GRU的步骤，用来更新隐藏表示。因此，这个过程也可以表示为：<img src=\"/2023/01/16/GNN4NLP/image-20221012194657161-167384297066345.png\" alt=\"image-20221012194657161\"></p></li>\r\n</ul>\r\n</blockquote></li>\r\n</ul>\r\n<p><strong>Graph Pooling</strong></p>\r\n<h6 id=\"简介\">简介</h6>\r\n<blockquote>\r\n<ul>\r\n<li><p>图池化层是为了获取图级表示而提出的，主要服务于基于图滤波得到的节点embeddings进行图分类、预测的graph-focused的下流任务。</p></li>\r\n<li><p>图池化操作主要根据两种信息：</p>\r\n<ul>\r\n<li>node features</li>\r\n<li>graph structure</li>\r\n</ul></li>\r\n<li><p>主要分为平面池化层和分级池化层</p>\r\n<ul>\r\n<li>平面图池在一个步骤中直接从节点嵌入生成图级表示。</li>\r\n<li>分层图池包含几个图池层，每个池层都遵循一堆图过滤器。</li>\r\n</ul></li>\r\n<li><p>一个图池化层通常输入一个图，并返回粗化图： <span class=\"math display\">\\[\r\n\\textbf{A}^{(op)},\\textbf{F}^{(op)}=pool(\\textbf{A}^{(ip)},\\textbf{F}^{(ip)})\r\n\\]</span> 就是两件事：新的图+新的特征</p></li>\r\n</ul>\r\n</blockquote>\r\n<h4 id=\"easy\">easy</h4>\r\n<blockquote>\r\n<p><strong>Flat Graph Pooling</strong></p>\r\n<p>过程总结： <span class=\"math display\">\\[\r\n\\textbf{f}_G=pool(\\textbf{A}^{(ip)},\\textbf{F}^{(ip)})\r\n\\]</span> 其中<span class=\"math inline\">\\(\\textbf{f}_G\\in\r\n\\mathbb{R}^{1\\times d_{op}}\\)</span>是图表示。</p>\r\n<p>最大池化层： <span class=\"math display\">\\[\r\n\\textbf{f}_G = max(\\textbf{F}^{(ip)})\r\n\\newline\r\n\\textbf{f}_G[i] = max(\\textbf{F}^{(ip)}_{:,i})\r\n\\]</span> 平均池化层： <span class=\"math display\">\\[\r\n\\textbf{f}_G = ave(\\textbf{F}^{(ip)})\r\n\\]</span> 基于注意力的平面池化操作，称为<strong>门控全局池化(gated\r\nglobal\r\npooling)</strong>。衡量每个节点重要性的注意力分数用于总结节点表示以生成图形表示。具体来说，节点\r\nvi 的注意力分数计算为：<img src=\"/2023/01/16/GNN4NLP/image-20221012200836949-167384297066346.png\" alt=\"image-20221012200836949\"></p>\r\n<p>其中h是将输入映射到一个标量的前馈网络。</p>\r\n<p>使用学习到的注意力分数，图形表示可以从节点表示中总结为：</p>\r\n<figure>\r\n<img src=\"/2023/01/16/GNN4NLP/image-20221012201009369-167384297066347.png\" alt=\"image-20221012201009369\">\r\n<figcaption aria-hidden=\"true\">image-20221012201009369</figcaption>\r\n</figure>\r\n<p>其中<span class=\"math inline\">\\(\\Theta_{ip}\\)</span>是待学习的参数。</p>\r\n<p>一些层池化操作潜入了过滤层。一个“假”节点被添加到连接到所有节点的图中（Li\r\net al.,\r\n2015）。这个“假”节点的表示可以在过滤过程中学习。它的表示捕获整个图的信息，因为它连接到图中的所有节点。因此，“假”节点的表示可以用作下游任务的图表示。</p>\r\n</blockquote>\r\n<h5 id=\"进一步\">进一步</h5>\r\n<blockquote>\r\n<ul>\r\n<li><p>平面池化层在汇总图表示的节点表示时通常会忽略<strong>分层图结构信息</strong>。</p></li>\r\n<li><p>分层图池化层旨在通过逐步粗化图来保留分层图结构信息，直到实现图表示。</p></li>\r\n<li><p>分层池化层可以根据它们粗化图的方式进行粗略分组</p>\r\n<ul>\r\n<li>一种分层池化层通过<strong>子采样</strong>来粗化图，即选择最重要的节点作为粗化图的节点。</li>\r\n<li>一种不同类型的分层池化层将输入图中的<strong>节点组合</strong>在一起形成超级节点，作为粗化图的节点。</li>\r\n</ul></li>\r\n<li><p>Downsampling-based Pooling</p>\r\n<p>为了粗化输入的图，我们需要从输入的<span class=\"math inline\">\\(N_{op}\\)</span>个节点中根据重要性测度来进行筛选。</p>\r\n<p>在本类池化层中，主要由三个组成部分：</p>\r\n<ul>\r\n<li>developing the <strong>measure</strong> for downsampling</li>\r\n<li>generating <strong>graph structure</strong> for the coarsened\r\ngraph</li>\r\n<li>generating <strong>node features</strong> for the coarsened\r\ngraph</li>\r\n</ul>\r\n<p>开山之作——<strong>gPool layer</strong></p>\r\n<p>重要性测度是从输入节点特征中学到的： <span class=\"math display\">\\[\r\n\\textbf{y}=\\frac{\\textbf{F}^{(ip)}\\textbf{p}}{||\\textbf{p}||}\r\n\\]</span> <span class=\"math inline\">\\(\\textbf{F}^{(ip)}\\in\r\n\\mathbb{R}^{N_{ip}\\times d_{ip}}\\)</span>是代表输入节点特征的。而<span class=\"math inline\">\\(\\textbf{p}\\in\r\n\\mathbb{R}^{d_{ip}}\\)</span>是将输入特征投影到重要性分数中要学习的向量。</p>\r\n<p>得到重要性分数y之后就可以得到前k个最重要的节点了 <span class=\"math display\">\\[\r\nidx = rank(\\textbf{y},N_{op})\r\n\\]</span> 粗化图的图结构可以从输入图的图结构中推导出为： <span class=\"math display\">\\[\r\n\\textbf{A}^{(op)} = \\textbf{A}^{(ip)}(idx,idx)\r\n\\]</span> 等式右边的部分指的是从矩阵中按行和列进行抽取。</p>\r\n<p>同样，也可以从输入节点特征中提取节点特征。在（Gao 和\r\nJi，2019）中，采用门控系统来控制从输入特征到新特征的信息流。具体来说，具有较高重要性分数的选定节点可以有更多的信息流到粗化图，可以建模为：<img src=\"/2023/01/16/GNN4NLP/image-20221012220208418-167384297066348.png\" alt=\"image-20221012220208418\"></p>\r\n<p>其中，<span class=\"math inline\">\\(\\sigma()\\)</span>是将重要性分数映射到(0,1)的激活函数。而<span class=\"math inline\">\\(\\textbf{1}_{d_{ip}}\\in\r\n\\mathbb{R}^{d_{ip}}\\)</span>是全1向量。请注意，y(idx) 根据 idx\r\n中的索引从 y 中提取相应的元素，而 <span class=\"math inline\">\\(F^{(ip)}(idx)\\)</span> 根据 idx 检索相应的行。</p>\r\n<p>gPool\r\nlayer中学习重要性的方法忽视了<strong>图结构信息</strong>，为了容纳图结构信息，GCN-Filter被用来学习重要性分数。\r\n<span class=\"math display\">\\[\r\n\\textbf{y}=\\alpha(GCN-Filter(\\textbf{A}^{(ip)},\\textbf{F}^{(ip)}))\r\n\\]</span> 其中<span class=\"math inline\">\\(\\alpha\\)</span>是激活函数。请注意，y\r\n是向量而不是矩阵。也就是说，GCN-Filter的输出通道数设置为1。这个图池化操作被命名为SAGPool。</p></li>\r\n<li><p>Supernode-based Hierarchical Graph Pooling</p>\r\n<p>基于超节点的池化方法旨在通过生成超节点来粗化输入图。具体来说，他们尝试学习将输入图中的节点分配到不同的集群中，这些集群被视为超级节点。</p>\r\n<p>这些超节点被视为粗化图中的节点。然后生成超节点之间的边和这些超节点的特征以形成粗化图。</p>\r\n<p>这种方法有三种关键的组分：</p>\r\n<ul>\r\n<li>generating supernodes as the nodes for the coarsened graph</li>\r\n<li>generating graph structure for the coarsened graph</li>\r\n<li>generating node features for the coarsened graph</li>\r\n</ul>\r\n<p><strong>diffpool</strong></p>\r\n<p>具体来说，使用 GCN-Filter 学习从输入图中的节点到超节点的软分配矩阵：\r\n<span class=\"math display\">\\[\r\n\\textbf{S} = softmax(GCN-Filter(\\textbf{A}^{(ip)}))\r\n\\]</span> 其中<span class=\"math inline\">\\(\\textbf{S}\\in\r\n\\mathbb{R}^{N_{ip}\\times N_{op}}\\)</span>是需要被学习的分配矩阵。<span class=\"math inline\">\\(\\textbf{F}^{(ip)}\\)</span>通常是最新的图滤波层的输出。此外，可以堆叠几个\r\nGCN 过滤器来学习分配矩阵，尽管在上面的方程式中只使用了一个过滤器</p>\r\n<p>分配矩阵的每一行都可以看成是一个超节点。第 i 行中的第 j\r\n个元素表示将第 i 个节点分配给第 j 个超节点的概率。</p>\r\n<p>具体来说，粗化图的图结构可以通过利用软分配矩阵 S 从输入图生成：<img src=\"/2023/01/16/GNN4NLP/image-20221012231514618-167384297066449.png\" alt=\"image-20221012231514618\"></p>\r\n<p>同理，根据赋值矩阵 S\r\n对输入图的节点特征进行线性组合，可以得到超节点的节点特征：<img src=\"/2023/01/16/GNN4NLP/image-20221012231716198-167384297066450.png\" alt=\"image-20221012231716198\"></p>\r\n<p>其中<span class=\"math inline\">\\(\\textbf{F}^{(inter)}\\in\r\n\\mathbb{R}^{N_{ip}\\times\r\nd_{op}}\\)</span>是通过GCN-Filter学习到的中间特征： <span class=\"math display\">\\[\r\nF ^ { ( inter ) } = G C N - F i l t e r ( A ^ { ( i p ) } , F ^ { ( i p\r\n) } )\r\n\\]</span> 上面式子中的GCN-Filter也是可以堆叠多个。</p>\r\n<p><strong>EigenPooling</strong></p>\r\n<p>EigenPooling (Ma et al., 2019b)\r\n使用谱聚类方法生成超节点，并专注于为粗化图形成图结构和节点特征。</p>\r\n<p>应用谱聚类算法后，得到一组不重叠的簇，也可以作为粗化图的超节点。</p>\r\n<p>输入节点和输出节点之间的<strong>分配矩阵</strong>可以被表示为<span class=\"math inline\">\\(\\textbf{S} \\in \\{0,1\\}^{N_{ip}\\times\r\nN_{op}}\\)</span>,其中一行中只有一个元素是1，其余全是0。</p>\r\n<p>更具体地说，仅当第 i 个节点分配给第 j 个超级节点时，S[i, j] =\r\n1。对于第 k 个超级节点，我们使用 <span class=\"math inline\">\\(A(k) \\in\r\n\\mathbb{R}^{N^{(k)}×N^{(k)}}\\)</span> 来描述其对应集群中的图结构，其中\r\nN(k) 是该集群中的节点数。我们将采样算子<span class=\"math inline\">\\(\\textbf{C}^{(k)}\\in \\{0,1\\}^{N_{ip}\\times\r\nN^{(k)}}\\)</span>​定义为： <span class=\"math display\">\\[\r\n\\textbf{C}^{(k)}[i,j] = 1\\quad if\\ and \\ only \\ if\r\n\\  \\Gamma^{(k)}(j)=v_i\r\n\\]</span> 其中 <span class=\"math inline\">\\(Γ^{(k)}\\)</span> 表示第 k\r\n个集群中的节点列表，<span class=\"math inline\">\\(Γ^{(k)}(j) =\r\nv_i\\)</span> 表示节点 vi 对应于该集群中的第 j\r\n个节点。使用这个采样算子，第 k 个簇的邻接矩阵可以正式定义为：<img src=\"/2023/01/16/GNN4NLP/image-20221012235316597-167384297066451.png\" alt=\"image-20221012235316597\"></p>\r\n<p>接下来，我们讨论为粗化图生成图结构和节点特征的过程。为了形成超级节点之间的图结构，只考虑原始图中跨集群的连接。为了实现这个目标，我们首先为输入图生成簇内邻接矩阵，该矩阵仅由每个簇内的边组成：<img src=\"/2023/01/16/GNN4NLP/image-20221012235530298-167384297066452.png\" alt=\"image-20221012235530298\"></p>\r\n<p>然后，仅由跨集群的边组成的集群间邻接矩阵可以表示为 <span class=\"math inline\">\\(A_{ext} =\r\nA-A_{int}\\)</span>。粗化图的邻接矩阵可以得到：<img src=\"/2023/01/16/GNN4NLP/image-20221012235630483-167384297066453.png\" alt=\"image-20221012235630483\"></p>\r\n<p>采用<strong>图傅里叶变换</strong>生成节点特征。具体而言，利用每个子图（或集群）的图结构和节点特征来生成相应超节点的节点特征。接下来，我们以第\r\nk 个集群作为说明性示例来演示该过程。令 L(k) 表示该子图的拉普拉斯矩阵，$\r\nu<sup>{(k)}<em>1 , . . . , u^{(k)}</em>{n</sup>{(k)}}$\r\n是其对应的特征向量。该子图中节点的特征可以通过使用采样算子 <span class=\"math inline\">\\(C^{(k)}\\)</span> 从 <span class=\"math inline\">\\(F^{(ip) }\\)</span>中提取，如下所示：</p>\r\n<figure>\r\n<img src=\"/2023/01/16/GNN4NLP/image-20221013000115875-167384297066454.png\" alt=\"image-20221013000115875\">\r\n<figcaption aria-hidden=\"true\">image-20221013000115875</figcaption>\r\n</figure>\r\n<p>其中<span class=\"math inline\">\\(\\textbf{F}_{ip}^{(k)} \\in\r\n\\mathbb{R}^{N^{(k)}\\times\r\nd_{ip}}\\)</span>是第k集簇中的节点的输入特征。</p>\r\n<p>之后，我们使用傅里叶变换来为<span class=\"math inline\">\\(\\textbf{F}^{(k)}_{ip}\\)</span>中的所有频道产生系数：<img src=\"/2023/01/16/GNN4NLP/image-20221013000529937-167384297066455.png\" alt=\"image-20221013000529937\"></p>\r\n<p>其中<span class=\"math inline\">\\(\\textbf{f}^{(k)}_i \\in\r\n\\mathbb{R}^{1\\times d_{ip}}\\)</span>由所有特征通道的第 i\r\n个图傅里叶系数组成。</p>\r\n<p>第 k 个超节点的节点特征可以通过将这些系数连接起来形成：<img src=\"/2023/01/16/GNN4NLP/image-20221013000931882-167384297066456.png\" alt=\"image-20221013000931882\"></p>\r\n<p>我们通常只利用<strong>前几个系数</strong>来生成超级节点的特征，原因有两个。</p>\r\n<ul>\r\n<li>首先，不同的子图可能有不同数量的节点；因此，为了确保特征的相同维度，需要丢弃一些系数。</li>\r\n<li>其次，前几个系数通常捕获大部分重要信息，因为实际上，大多数图形信号都是平滑的。</li>\r\n</ul></li>\r\n</ul>\r\n</blockquote>\r\n","tags":["GNN in NLP"]},{"title":"Survey","url":"/2022/07/29/Survey/","content":"<h1 id=\"a-survey\">A Survey</h1>\r\n<blockquote>\r\n<p>目前的seq2seq类的模型在NLP领域大行其道。但是，它将各种复合句都编码为一个向量来处理。这样会出现组合爆炸问题，即简单句的组合会导致数据量剧增。因此，分析句子的结构，对复合句进行拆分，并将句子的结构知识运用到模型中，有望解决数据获取难等问题。针对该直觉，我进行了相关工作的调研，现将一些调研结果总结如下。\r\n<span id=\"more\"></span></p>\r\n</blockquote>\r\n<h2 id=\"句子的拆分和改写sentence-split-and-rephrase\">句子的拆分和改写：Sentence\r\nSplit and Rephrase</h2>\r\n<ul>\r\n<li>Fact-Aware Sentence Split and Rephrase with Permutation Invariant\r\nTraining\r\n<ul>\r\n<li>训练过程中对事实的遗失：Fact-aware Sentence Encoding</li>\r\n<li>简单句的顺序的影响：PIT(Permutation Invariant Training)</li>\r\n<li><img src=\"/2022/07/29/Survey/img-20220730153106.png\" title=\"fig:\" alt=\"img-20220730153106\"></li>\r\n</ul></li>\r\n</ul>\r\n<h2 id=\"语义解析semantic-parsing\">语义解析:Semantic Parsing</h2>\r\n<ul>\r\n<li>Iterative Utterance Segmentation for Neural Semantic Parsing\r\n<ul>\r\n<li>神经语义解析器通常无法将长时间和复杂的话语解析为正确的含义表示，因为缺乏利用组成的原则。为了解决这个问题，我们提出了一个新颖的框架，用于通过迭代话语细分来促进神经语义解析器。给定输入话语，我们的框架在两个神经模块之间迭代：用于从说话分割跨度的细分器，以及将跨度映射到部分含义表示的解析器。然后，这些中间解析结果被组成到最终含义表示形式中。一个关键优势是，该框架不需要任何手工艺模板或其他标记的数据进行分割：我们通过提出一种新颖的培训方法来实现这一目标，在这种方法中，解析器为细分器提供伪监督</li>\r\n<li><img src=\"/2022/07/29/Survey/img-20220730153231.png\" title=\"fig:\" alt=\"img-20220730153231\"></li>\r\n</ul></li>\r\n<li>SEQZERO: Few-shot Compositional Semantic Parsing with Sequential\r\nPrompts and Zero-shot Models\r\n<ul>\r\n<li>Seqzero将问题分解为一系列子问题，这些序列与形式语言的子语言相对应。基于分解，LMS只需要使用预测子段的提示来生成简短的答案。因此，seqzero避免了一次产生长长的规范话语。此外，Seqzero不仅采用了几个射击模型，而且还采用了零拍模型来减轻过度拟合。特别是，Seqzero通过配备了我们建议的约束重新制定的合奏来阐明两种模型的优点。\r\nSeqzero在GeoQuery和EcommerceQuery上实现了基于BART的模型的SOTA性能，它们是两个具有组成数据分配的少量数据集。</li>\r\n<li>SQL拆分示例<img src=\"/2022/07/29/Survey/img-20220730151705.png\" alt=\"img-20220730151705\"></li>\r\n</ul></li>\r\n</ul>\r\n<h2 id=\"组合泛化compositional-generalization\">组合泛化：Compositional\r\nGeneralization</h2>\r\n<ul>\r\n<li>Hierarchical Poset Decoding for Compositional Generalization in\r\nLanguage\r\n<ul>\r\n<li>我们将人类语言理解形式化为结构化的预测任务，其中输出为部分有序集（POSET）。当前的编码器架构不能正确考虑语义的POSET结构，因此遭受了不良的组成概括能力。在本文中，我们提出了一种新型的层次poset解码范式，用于语言中的组成概括。直觉：（1）拟议的范式在语义中执行部分置换不变性，从而避免过拟合bias\r\nordering信息；\r\n（2）分层机制允许捕获POSET的高级结构。我们评估了建议的decoder关于CFQ的表现。这是一个庞大而现实的自然语言问题，回答数据集，专门设计用于衡量组成概括。结果表明，它的表现优于当前解码器。</li>\r\n<li><img src=\"/2022/07/29/Survey/img-20220730153205.png\" title=\"fig:\" alt=\"img-20220730153205\"></li>\r\n</ul></li>\r\n<li>SUBS: Subtree Substitution for Compositional Semantic Parsing\r\n<ul>\r\n<li>将子树替换用于组成数据增强，在此我们认为具有类似语义函数的子树是可交换的。</li>\r\n<li>子树替换示例：<img src=\"/2022/07/29/Survey/img-20220730151835.png\" alt=\"img-20220730151835\"></li>\r\n</ul></li>\r\n<li>Unlocking Compositional Generalization in Pre-trained Models Using\r\nIntermediate Representations\r\n<ul>\r\n<li>序列到序列（SEQ2SEQ）模型在语义解析中很普遍，但已被发现在分布外的组成概括方面挣扎。尽管已经提出了专门的模型体系结构和SEQ2SEQ模型的预培训来解决此问题，但前者通常以一般性为代价，而后者仅显示有限的成功。在本文中，我们研究了中间表示对预训练的SEQ2SEQ模型中组成概括的影响，而无需更改模型体系结构，并确定设计有效表示的关键方面。我们没有将自然语言直接映射到可执行形式的训练，而是将其映射到具有更强的与自然语言的结构对应的可逆或有损中间表示形式。我们提出的中间表示和预培训模型的组合非常有效，最佳组合获得了CFQ上的最新最新作品（+14.8精度点）以及三个文本到\r\n- 到 -\r\nSQL数据集（+15.0至+19.4精度点）。这项工作强调了中间表示提供了一种重要且可能被忽视的自由度，以提高预训练的SEQ2SEQ模型的组成概括能力。</li>\r\n</ul></li>\r\n</ul>\r\n<h2 id=\"语义角色标注semantic-role-labeling\">语义角色标注：Semantic role\r\nlabeling</h2>\r\n<ul>\r\n<li>Syntax-aware Neural Semantic Role Labeling∗\r\n<ul>\r\n<li>传统的基于离散功能的SRL方法是由句法和语义结构之间的紧密相关性的激励，从而大大利用了句法特征。相反，基于深神经网络的方法通常将输入句子编码为单词序列，而无需考虑句法结构。在这项工作中，我们研究了以前的几种编码句法树的方法，并对额外的语法感知表示是否对神经SRL模型有益。基准CONLL-2005数据集的实验表明，语法感知的SRL方法可以通过Elmo的外部单词表示有效地改善强大基线的性能。借助额外的语法意识表示，我们的方法在测试数据上实现了新的最新的85.6\r\nF1（单个模型）和86.6\r\nF1（集合），分别优于0.8和1.0的Elmo的相应强基础。进行了详细的错误分析，以获得有关研究方法的更多见解。</li>\r\n</ul></li>\r\n</ul>\r\n<h2 id=\"自动化合规检查accautomated-compliance-checking\">自动化合规检查：ACC（Automated\r\nCompliance Checking）</h2>\r\n<ul>\r\n<li>SPAR.txt, a cheap Shallow Parsing approach for Regulatory texts\r\n<ul>\r\n<li>自动化合规检查（ACC）系统旨在将语义解析为一组规则。但是，已知语义解析很难，需要大量的培训数据。创建此类培训数据的复杂性导致了研究的研究，该研究重点是小型子任务，例如浅解析或提取有限的规则子集。这项研究介绍了一项浅解析任务，培训数据相对便宜，目的是学习ACC的词典。我们注释了200个句子Spar.txt1的小域特异性数据集，并训练一个序列标记器，该序列标记器在测试集上达到79,93\r\nF1分数。</li>\r\n<li><img src=\"/2022/07/29/Survey/img-20220730152243.png\" title=\"fig:\" alt=\"img-20220730152243\"></li>\r\n</ul></li>\r\n</ul>\r\n"},{"title":"Self-supervised_Learning","url":"/2023/01/16/Self-supervised-Learning/","content":"<h1 id=\"self-supervised-learning-generative-or-contrastive\">Self-supervised\r\nLearning: Generative or Contrastive</h1>\r\n<blockquote>\r\n<p>本文简要介绍Self-supervised\r\nLearning技术，为数据扩充等前期处理做准备。</p>\r\n<span id=\"more\"></span>\r\n</blockquote>\r\n<h2 id=\"背景\">背景</h2>\r\n<p>深度学习模型是data-hungry的。为何如此？和传统的feature-based的方法相比，深度学习通常采用end-to-end的模式。它只有<strong>很少的先验假设</strong>，这就导致了在数据不够的情况下出现过拟合和偏差的问题。</p>\r\n<p>文献表明，简单的多层感知器具有非常差的泛化能力（总是假设分布外（OOD）样本的线性关系）[145]，这会导致over-confident（和错误）的预测。</p>\r\n<blockquote>\r\n<p>这里对over-confident现象进行说明和补充。</p>\r\n<figure>\r\n<img src=\"/2023/01/16/Self-supervised-Learning/v2-4caa39b9d3e0ee762eccff0aaeceba82_b.jpg\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure>\r\n<p>首先，让咱们来思考一个普通图像分类任务。对于一张“koala”的图像，在经过神经网络后会得到\r\nlogits 输出 z(x)=[−5.2,0.1,2.1] ，经过softmax\r\n层后得到对各类别的预测的后验概率，接着我们选择概率最大的类别（\r\nkoala）输出为最后的预测类别。这里，最终的预测类别 、<span class=\"math inline\">\\(\\hat{y}(x)=koala\\)</span> ，其对应的置信度为 <span class=\"math inline\">\\(\\hat{p}(x)=0.88\\)</span>\r\n。在大多情况下，我们<strong>只关心类别的预测 <span class=\"math inline\">\\(\\hat{y}(x)\\)</span> 有多准</strong>，根本不 care\r\n置信度是怎样的。然而，在一些实际应用场景下，<strong>置信度</strong>的度量也同样重要。例如：</p>\r\n<figure>\r\n<img src=\"/2023/01/16/Self-supervised-Learning/v2-59aa2c112bf8c94cc6e8e2594e13da80_b.jpg\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure>\r\n<p>如上图，对于自动驾驶中的目标识别任务，车辆的前方出现了一个人，神经网络会将其<strong>识别成塑料袋</strong>，此时输出的置信度为50%（低于阈值），则可通过其它传感器进行二次的正确识别（识别为人）。但想想看，若神经网络对塑料袋预测的置信度为90%会怎样？</p>\r\n<p><strong>思考</strong>：就是说给很多类别打分，其中塑料袋分数最高。如果仅仅考虑分类正确与否，只要正确的类比其他类更高就行了。但是实际应用场景可能会根据置信度进行进一步的确认等操作，这个时候仅仅是相对更高是无法保证任务的顺利完成的。</p>\r\n<p>再例如：<img src=\"/2023/01/16/Self-supervised-Learning/v2-48719807a50725f127c6e54fcaaf0675_b.jpg\" alt=\"img\"></p>\r\n<p>使用 Resnet\r\n模型简单的对一些图片任务进行训练，收敛后的模型对测试集的平均置信度高达80%-85%，然而只有将近70%的图片能被正确分对（红色代表分错，绿色代表分对）。这意味着啥？训练好的模型好像有点盲目自信，即出现\r\n<strong>overconfidence</strong>\r\n现象，或者可以称为模型的准确率和置信度不匹配（<strong>miscalibration</strong>）。</p>\r\n<p><strong>预期校准误差（ECE）</strong></p>\r\n<p>直观地看，模型的准确率应当和置信度相匹配。一个完美校准的模型可定义成如下所示：\r\n<span class=\"math display\">\\[\r\n\\mathbb{P}(\\hat{Y}=Y|\\hat{P}=p)=p,\\forall p \\in [0,1]\r\n\\]</span> 即模型的置信度等于概率p的条件下模型的预测<span class=\"math inline\">\\(\\hat{Y}\\)</span>为真实标记Y的概率同样也为p。因此，本文提出一个新的度量方式叫做<strong>预期校准误差（ECE）</strong>来描述模型学习的匹配程度\r\n<span class=\"math display\">\\[\r\nECE = \\mathbb{E}_{\\hat{P}}[|\\mathbb{P}(\\hat{Y}=Y|\\hat{P}=p)-p|]\r\n\\]</span></p>\r\n</blockquote>\r\n<p>为了解决这样的问题，很多人去设计更新的网络的架构，但是还有一个很简单的方法就是增大可以使用的数据量（基于人工标注的高昂成本的现实，这个方法很难实现。）</p>\r\n<p>因此，self-supervised\r\nlearning应运而生。它的成功在于它找到了一种可以有效地使用大量未标记数据的方法。</p>\r\n<h2 id=\"基本思想及分类\">基本思想及分类</h2>\r\n<p><strong>It a time for deep learning algorithms to get rid of human\r\nsupervision and turn back to data’s selfsupervision.</strong></p>\r\n<p>自我监督学习的直觉是利用<strong>数据固有的共现关系</strong>作为自我监督，这可能是多种多样的。</p>\r\n<p>我们可以将self-supervision归结为三个宽泛的类别中：</p>\r\n<ul>\r\n<li>Generative: train an encoder to encode input x into an explicit\r\nvector z and a decoder to reconstruct x from z (e.g., the cloze test,\r\ngraph generation)</li>\r\n<li>Contrastive: train an encoder to encode input x into an explicit\r\nvector z to measure similarity (e.g., mutual information maximization,\r\ninstance discrimination)</li>\r\n<li>Generative-Contrastive (Adversarial): train an encoderdecoder to\r\ngenerate fake samples and a discriminator to distinguish them from real\r\nsamples (e.g., GAN)</li>\r\n</ul>\r\n<p>他们的区别在于模型结构和目标：<img src=\"/2023/01/16/Self-supervised-Learning/image-20221023231711831.png\" alt=\"image-20221023231711831\"></p>\r\n<p>区别主要在于：</p>\r\n<ul>\r\n<li>For latent distribution <span class=\"math inline\">\\(z\\)</span>: in\r\ngenerative and contrastive methods, <span class=\"math inline\">\\(z\\)</span> is explicit and is often leveraged by\r\ndownstream tasks; while in GAN, <span class=\"math inline\">\\(z\\)</span>\r\nis implicitly modeled.</li>\r\n<li>For discriminator: the generative method does not have a\r\ndiscriminator while GAN and contrastive have. Contrastive discriminator\r\nhas comparatively fewer parameters (e.g., a multi-layer perceptron with\r\n2-3 layers) than GAN (e.g., a standard ResNet [53]).</li>\r\n<li>For objectives: the generative methods use a reconstruction loss,\r\nthe contrastive ones use a contrastive similarity metric (e.g.,\r\nInfoNCE), and the generative-contrastive ones leverage distributional\r\ndivergence as the loss (e.g., JS-divergence, Wasserstein Distance).</li>\r\n</ul>\r\n<p>与下游任务相关的正确设计的<strong>训练目标</strong>可以将我们随机初始化的模型变成优秀的<strong>预训练特征提取器</strong>。</p>\r\n<p>The art of self-supervised learning primarily lies in defining proper\r\nobjectives for unlabeled data.</p>\r\n<h2 id=\"generative-self-supervised-learning\">Generative Self-supervised\r\nLearning</h2>\r\n<h3 id=\"auto-regressivear-model\">Auto-regressive(AR) Model</h3>\r\n<p>AR模型可以被认为是“Bayes net\r\nstructure”(有向图模型)。联合分布可以分解为条件的乘积： <span class=\"math display\">\\[\r\n\\max_{\\theta}p_{\\theta}(X) = \\sum_{t=1}^T\\log p_{\\theta}(x_t|X_{1:t-1})\r\n\\]</span> 其中每个变量的概率取决于先前的变量。</p>\r\n<blockquote>\r\n<p>这里补充AR模型的基本概念和范式。</p>\r\n<ul>\r\n<li><p>回归分析：回归分析是<strong>分析自变量与因变量之间定量的因果关系</strong>，并且用回归方程描述。</p></li>\r\n<li><p>自回归：因变量和自变量都为同一个变量的回归方法</p></li>\r\n<li><p>自回归模型</p>\r\n<ul>\r\n<li>如果<span class=\"math inline\">\\(\\{\\epsilon_t\\}\\)</span>为白噪声，服从<span class=\"math inline\">\\(N(0,\\sigma^2)\\)</span>，<span class=\"math inline\">\\(a_0,a_1,...,a_p(a_p\\neq0)\\)</span>为实数，就称<span class=\"math inline\">\\(p\\)</span>阶差分方程 <span class=\"math display\">\\[\r\nX_t = a_0 + a_1X_{t-1}+a_2X_{t-2}+\\cdot \\cdot \\cdot +a_pX_{t-p} +\r\n\\epsilon_t,t\\in\\mathbb{Z}\r\n\\]</span> 是一个p阶自回归模型，简称<span class=\"math inline\">\\(AR(p)\\)</span>模型，称<span class=\"math inline\">\\(\\textbf{a} =\r\n(a_0,a_1,...,a_p)^T\\)</span>是模型中的回归系数。</li>\r\n</ul></li>\r\n</ul>\r\n</blockquote>\r\n<p>在NLP中，AR语言模型的目标通常是最大化在前向自回归分解下的似然性。<strong>Auto-regressive\r\nlanguage model\r\n（自回归语言模型）</strong>就是根据上文信息预测下文信息（反之亦可)，正是这由于自回归模型的训练方法完美适应于生成任务，因此被广泛应用于例如文本生成和机器翻译等任务。</p>\r\n<p>即，将下一个词作为标签，将前几个词作为输入，然后建模最大化其对应的似然性的自回归模型。</p>\r\n<p>与 GPT 不同，GPT-2\r\n去除了不同任务的微调过程。为了学习泛化不同任务的统一表示，GPT-2 对 <span class=\"math inline\">\\(p(output|input, task)\\)</span>\r\n建模，这意味着给定不同的任务，相同的输入可以有不同的输出。</p>\r\n<h3 id=\"flow-based-model\">Flow-based Model</h3>\r\n<blockquote>\r\n<p>首先补充Flow-based Generative Model的相关知识</p>\r\n<figure>\r\n<img src=\"/2023/01/16/Self-supervised-Learning/v2-ec15bcddf873e5c63b58cfa21cfbad5f_b.jpg\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure>\r\n<p>回顾一下之前GAN的相关内容，我们知道GAN的目标就是通过生成器学习得到一个生成分布，并使其尽可能的接近于真实的数据分布。对于该过程我们可以表述为以下的公式：\r\n<span class=\"math display\">\\[\r\nG^* = arg\\max_G\\sum_{i=1}^m \\log P_G(x^i) \\{x_1,x_2,..,x_m \\ from \\\r\nP_{data}\\}\r\n\\]</span> 在前面涉及到的 GAN\r\n的相关内容中我们已经知道，目前生成器往往是参数量十分巨大的 NN，所以\r\n<span class=\"math inline\">\\(P_G\\)</span>\r\n的具体表达式我们很难获得，所以在 GAN\r\n里面需要在生成分布和真实分布中进行<strong>采样</strong>，通过采样尽可能多的数据来近似两个不同的分布，然后引入判别器来衡量两个分布之间的差距，以此来引导\r\nNN 的训练。而在 flow-based\r\n生成模型中，我们将会<strong>对上述的公式直接进行求解</strong>，这就是其与\r\nGAN 存在的最大区别。</p>\r\n<blockquote>\r\n<p>这里需要补充一点知识，已知两个概率密度函数 <span class=\"math inline\">\\(\\pi(z)\\)</span>和 <span class=\"math inline\">\\(p(x)\\)</span>，这两个概率密度分布存在以下变换关系\r\n<span class=\"math inline\">\\(x=f(z)\\)</span>\r\n，在两个不同的概率密度函数上对应的部分取 <span class=\"math inline\">\\(dz\\)</span> 和 <span class=\"math inline\">\\(dx\\)</span>进行积分，根据两个函数对应部分积分的面积相等，可以得到关系式：\r\n<span class=\"math inline\">\\(p(x)|det(J_f)|=\\pi(z)\\)</span> ，其中 <span class=\"math inline\">\\(J_f\\)</span>为函数 <span class=\"math inline\">\\(f\\)</span> 的雅可比矩阵，由于 <span class=\"math inline\">\\(\\det(J_f)=\\frac{1}{\\det(J_f^{-1})}\\)</span> ,\r\n所以有 <span class=\"math inline\">\\(p(x)=\\pi(z)|det(J_f^{-1})|\\)</span>。</p>\r\n</blockquote>\r\n<p>上面讲到的 <span class=\"math inline\">\\(\\pi(z)\\)</span>\r\n其实就是下图中对应的采样分布，而 <span class=\"math inline\">\\(p(x)\\)</span>就对应生成分布。那么有了以上补充部分的结论便可得到\r\n<span class=\"math inline\">\\(P_G(x^i)=\\pi(z^i)|det(J_G^{-1})|\\)</span> 和\r\n<span class=\"math inline\">\\(z^i=G^{-1}(x^i)\\)</span> 。</p>\r\n<p><img src=\"/2023/01/16/Self-supervised-Learning/v2-518e7588b6b2cbb7daddb3f561037ec2_r.jpg\"></p>\r\n<p>那么现在我们便可以把最上面的优化目标 <span class=\"math inline\">\\(G^*\\)</span> 转化为：</p>\r\n<p><span class=\"math display\">\\[G^*=\\arg \\max \\limits_{G}\r\n\\sum^{m}_{i=1}log\\pi(G^{-1}(x^i))+log|det(J_G^{-1})| \\ \\ \\ \\\r\n(2)\\\\\\]</span></p>\r\n<p>要求解以上的表达式就需要先计算出 <span class=\"math inline\">\\(G^{-1}\\)</span>和 <span class=\"math inline\">\\(det(J_G^{-1})\\)</span>，所以显而易见这里的<strong>生成器\r\n<span class=\"math inline\">\\(G\\)</span>\r\n必须是可逆的</strong>。在这里我们可以通过逆序来训练 <span class=\"math inline\">\\(G^{-1}\\)</span>，在真实分布中采样 <span class=\"math inline\">\\(x^i\\)</span>，生成 Normal Distribution 中的 sample\r\n<span class=\"math inline\">\\(z^i\\)</span> ，maximize 上面的 objective\r\nfunction（这里一般需要保证 <span class=\"math inline\">\\(x^i\\)</span>和\r\n<span class=\"math inline\">\\(z^i\\)</span>具有相同的尺寸）。</p>\r\n<p><img src=\"/2023/01/16/Self-supervised-Learning/v2-717a108926ab2a7feee1ed88596b1b85_r.jpg\"></p>\r\n<p>由于单个 <span class=\"math inline\">\\(G\\)</span>\r\n受到了较多的约束，所以可能表征能力有限，需要注意的是这里的 <span class=\"math inline\">\\(G\\)</span>\r\n是可以进行多层扩展的，其对应的关系式只要进行递推便可。</p>\r\n<p><img src=\"/2023/01/16/Self-supervised-Learning/v2-1d49f1b8d24bc755e8c1fc4a2b105211_r.jpg\"></p>\r\n<p>而对于满足以上这种可逆性质的 <span class=\"math inline\">\\(G\\)</span>\r\n的一种设计方法便是为 coupling layer，其被应用在 <a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1410.8516\">NICE</a>\r\n和 <a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1605.08803\">Real\r\nNVP</a> 这两篇论文当中。</p>\r\n<p>coupling layer 采用以下结构，其中 F 和 H\r\n为两个变换函数，其可以是一个神经网络：</p>\r\n<p><img src=\"/2023/01/16/Self-supervised-Learning/v2-067db86c25e2e1fba0ad42c16cb0a2ec_r.jpg\"></p>\r\n<p>整个流程可以表述为：</p>\r\n<p>首先将输入 <span class=\"math inline\">\\(z\\)</span>\r\n拆分成两个部分（可以是按 channel 进行拆分，也可以还是按照 pixel 的\r\nlocation 进行拆分），对于上面的部分 <span class=\"math inline\">\\(z_1,...,z_d\\)</span> 直接 copy 得到对应的 output\r\n<span class=\"math inline\">\\(x_1,...,x_d\\)</span>,\r\n而对于下面的分支则有如下的变换： <span class=\"math inline\">\\((z_{d+1},...,z_D)\\odot\r\nF(z_1,...,z_d)+H(z_1,...,z_d)=x_{d+1},...,x_D\\)</span> 可以简化为：\r\n<span class=\"math inline\">\\((z_{d+1},...,z_D)\\odot\r\n(\\beta_1,...,\\beta_d)+ (\\gamma_1,...,\\gamma_d)=x_{d+1},...,x_D\\)</span>\r\n或 <span class=\"math inline\">\\(\\beta_i z_i+\\gamma_i=x_{i&gt;d}\\)</span>\r\n。之所以采用以上设计结构的原因在于上述的结构容易进行逆运算。</p>\r\n<p><img src=\"/2023/01/16/Self-supervised-Learning/v2-9642b94de841d63f752c3412f2ced928_r.jpg\"></p>\r\n<p>在逆向过程中，容易得到 <span class=\"math inline\">\\(z_{i\\leq\r\nd}=x_i\\)</span> 和<span class=\"math inline\">\\(z_{i&gt;d}=\\frac{x_i-\\gamma_i}{\\beta_i}\\)</span>\r\n。解决完 <span class=\"math inline\">\\(G^{-1}\\)</span>部分，还需要求解生成器对应的雅可比矩阵\r\n<span class=\"math inline\">\\(J_G\\)</span> 的行列式。我们直接采用李老师的\r\nPPT 中的图片来解释。</p>\r\n<p><img src=\"/2023/01/16/Self-supervised-Learning/v2-c395fc7e5d97412001cd965103a5a846_r.jpg\"></p>\r\n<p>我们可以将生成器对应的雅克比矩阵分为以上的四个子块，左上角由于是直接\r\ncopy 的，所以对应的部分应该是一个单位矩阵，右上角中由于 <span class=\"math inline\">\\(x_1,...,x_d\\)</span> 与 <span class=\"math inline\">\\(z_{d+1},...,z_D\\)</span>\r\n没有任何关系，所以是一个零矩阵，而左下角呢，We don't\r\ncare，就是这么任性，因为行列式的右上角为\r\n0，所以只需要求解主对角线上的值即可。而右下角呢，由于只有 <span class=\"math inline\">\\(x_i\\)</span> 和 <span class=\"math inline\">\\(z_i\\)</span>\r\n之间才会产生联系，所以自然而然右下角应该是一个对角阵，对角线上的元素分别为\r\n<span class=\"math inline\">\\(\\beta_{d+1},...,\\beta_{D}\\)</span>。所以上述的\r\n<span class=\"math inline\">\\(|det(J_G)|=|\\beta_{d+1}\\beta_{d+2}\\cdot\\cdot\\cdot\\beta_{D}|\\)</span>\r\n。那么这么一来 coupling layer 的设计把 <span class=\"math inline\">\\(G^{-1}\\)</span>和 <span class=\"math inline\">\\(det(J_G)\\)</span>这个问题都解决了。</p>\r\n<p>最后呢，为了进一步的增强 coupling layer\r\n的表征能力，我们还以根据不同的策略利用 Coupling Layer 作为子模块进行\r\nStacking。</p>\r\n<p><img src=\"/2023/01/16/Self-supervised-Learning/v2-c7f0c2407835450b6eb799f1ded512f8_r.jpg\"></p>\r\n<p>除了 Coupling Layer 之外呢，还有 <a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1807.03039\">GLOW</a>\r\n这个工作采用了 1×1 的卷积来解决上述的问题。</p>\r\n<p>参考文章：</p>\r\n<p><a href=\"https://link.zhihu.com/?target=https%3A//lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html\">https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html</a></p>\r\n</blockquote>\r\n<p>基于<strong>流</strong>的模型的目标是从数据中估计复杂的高维密度 <span class=\"math inline\">\\(p(x)\\)</span>。直观地想，直接形式化密度是困难的。</p>\r\n<p>为了获得复杂的密度，我们希望通过堆叠一系列分别描述<strong>不同数据特征</strong>的变换函数来“逐步”生成它。</p>\r\n<p>通常，基于流的模型首先定义一个潜在变量 <span class=\"math inline\">\\(z\\)</span>，它遵循已知分布 <span class=\"math inline\">\\(p_Z (z)\\)</span>。然后定义 <span class=\"math inline\">\\(z = f_{\\theta}(x)\\)</span>，其中 <span class=\"math inline\">\\(f_{\\theta}\\)</span>\r\n是一个可逆且可微的函数。目标是学习 <span class=\"math inline\">\\(x\\)</span> 和 <span class=\"math inline\">\\(z\\)</span> 之间的转换，以便可以描述 <span class=\"math inline\">\\(x\\)</span> 的密度。根据积分规则，<span class=\"math inline\">\\(p_{\\theta}(x)dx = p(z)dz\\)</span>。因此，<span class=\"math inline\">\\(x\\)</span> 和 <span class=\"math inline\">\\(z\\)</span> 的密度满足： <span class=\"math display\">\\[\r\np_{\\theta}(x) = p(f_{\\theta}(x))|\\frac{\\partial f_{\\theta}(x)}{\\partial\r\nx}|\r\n\\]</span> 所以，目标就是最大化似然函数： <span class=\"math display\">\\[\r\n\\max_{\\theta}\\sum_i\\log p_{\\theta}(x^{(i)}) = \\max_{\\theta}\\sum_i\\log\r\np_{Z}(f_{\\theta}(x^{(i)}))+ \\log |\\frac{\\partial f_{\\theta}}{\\partial\r\nx}(x^{(i)})|\r\n\\]</span> 基于流的模型的优点是 <span class=\"math inline\">\\(x\\)</span> 和\r\n<span class=\"math inline\">\\(z\\)</span> 之间的映射是可逆的。</p>\r\n<p>但是，它还要求 <span class=\"math inline\">\\(x\\)</span> 和 <span class=\"math inline\">\\(z\\)</span> 必须具有相同的维度。 <span class=\"math inline\">\\(f_{\\theta}\\)</span>\r\n需要仔细设计，因为它应该是可逆的并且是等式中的雅可比行列式并且第一个式子也应该容易计算才行。</p>\r\n<p>NICE [35] 和 RealNVP [36] 设计仿射耦合层来参数化 <span class=\"math inline\">\\(f_{\\theta}\\)</span>。核心思想是将 <span class=\"math inline\">\\(x\\)</span> 分成两个块 <span class=\"math inline\">\\((x_1, x_2)\\)</span>\r\n并以<strong>自回归</strong>的方式应用从 <span class=\"math inline\">\\((x_1, x_2)\\)</span> 到 <span class=\"math inline\">\\((z_1, z_2)\\)</span> 的变换，即 <span class=\"math inline\">\\(z_1 = x_1\\)</span> 和 <span class=\"math inline\">\\(z_2 = x_2 + m( x_1)\\)</span>。</p>\r\n<p>最近，提出了 Glow [68]，它引入了<strong>可逆的 1×1\r\n卷积</strong>并简化了 RealNVP。</p>\r\n<h3 id=\"auto-encodingae-model\">Auto-encoding(AE) Model</h3>\r\n<p>自动编码模型的目标是从（损坏的）输入重构（部分）输入。由于其灵活性，AE\r\n模型可能是最流行的具有许多变体的生成模型。</p>\r\n<h4 id=\"basic-ae-model\">Basic AE Model</h4>\r\n<p>在自编码器之前，Restricted Boltzmann Machine\r\n(RBM)也可以看做是一种特殊的自编码器。</p>\r\n<blockquote>\r\n<blockquote>\r\n<p>结构化概率模型为随机变量之间的直接作用提供了一个正式地建模框架。这种方式大大减少了模型的参数个数，以至于模型只需要更少的数据来进行有效的估计。这些更小的模型大大减少了再模型存储、模型推断以及从模型中采样时的计算开销。</p>\r\n<p>一种常见的方式是使用图来表示随机变量之间的相互作用。</p>\r\n<p>其中有向图模型被称为信念网络（belief network）或贝叶斯网络（Bayesian\r\nnetwork）</p>\r\n<p>每个结点代表一个随机变量，从a指向b的边是说我们用一个条件分布来定义b，而a是作为这个条件分布分布符号右边的一个变量。即b的分布依赖于a的取值。正式地说，变量x的有向概率模型是通过有向无环图<span class=\"math inline\">\\(\\mathcal{G}\\)</span>（每个结点都是模型中的随机变量）和一系列局部条件概率分布<span class=\"math inline\">\\(p(x_i|Pa_{\\mathcal{G}}(x_i))\\)</span>。其中<span class=\"math inline\">\\(Pa_{\\mathcal{G}}(x_i)\\)</span>代表结点<span class=\"math inline\">\\(x_i\\)</span>的所有父节点。x的概率分布可以表示为：\r\n<span class=\"math display\">\\[\r\np(\\textbf{x}) = \\prod_{i}p(x_i|Pa_{\\mathcal{G}}(x_i))\r\n\\]</span> 而同样地，还有无向图模型，也被称为马尔可夫随机场(Markov random\r\nfield,MRF)或马尔可夫网络(Markov\r\nnetwork)。当随机变量之间的相互作用并没有本质性的指向，或者是明确的双向相互作用时，使用无向图模型更合适。</p>\r\n<p>正式地说，一个无向图模型是一个定义在无向模型<span class=\"math inline\">\\(\\mathcal{G}\\)</span>上的该绿化模型。对图中的每个团<span class=\"math inline\">\\(\\mathcal{C}\\)</span>（图中结点的子集，其中的点是全连接的），一个因子<span class=\"math inline\">\\(\\phi(\\mathcal{C})\\)</span>被称为团势能，衡量了团中变量每一种可能的联合状态所对应的密切程度。这些因子被限制是非负的，一起定义了未归一化概率函数：\r\n<span class=\"math display\">\\[\r\n\\widetilde{p}(\\textbf{x}) = \\prod_{\\mathcal{C}\\in\r\n\\mathcal{G}}\\phi(\\mathcal{C})\r\n\\]</span>\r\n尽管这个概率是非负的，但是我们无法保证其积分或和为1。为了得到一个有效的概率分布，我们需要使用对应的归一化的概率分布（一个通过归一化团势能乘积定义的分布也被称为<strong>吉布斯分布</strong>）：\r\n<span class=\"math display\">\\[\r\np(\\textbf{x}) = \\frac{1}{Z}\\widetilde{p}(\\textbf{x})\r\n\\]</span> 其中<span class=\"math inline\">\\(Z\\)</span>是是的所有概率之和或者积分为1的常数，并且满足：\r\n<span class=\"math display\">\\[\r\nZ = \\int\\widetilde{p}(\\textbf{x})d\\textbf{x}\r\n\\]</span> 当函数<span class=\"math inline\">\\(\\phi\\)</span>固定时，我们可以把<span class=\"math inline\">\\(Z\\)</span>当成一个常数。但是如果函数<span class=\"math inline\">\\(\\phi\\)</span>带有参数时，那么<span class=\"math inline\">\\(Z\\)</span>是这些参数的一个函数。</p>\r\n<p>归一化常数<span class=\"math inline\">\\(Z\\)</span>被称为是配分函数。</p>\r\n<p>由于<span class=\"math inline\">\\(Z\\)</span>通常是对所有可能的<span class=\"math inline\">\\(\\textbf{x}\\)</span>状态的联合分布空间求和或者求积分得到的，它通常是很难计算的。</p>\r\n<p>为了获得一个无向模型的归一化概率分布，模型的结构和函数<span class=\"math inline\">\\(\\phi\\)</span>的定义通常需要设计为有助于高效地计算<span class=\"math inline\">\\(Z\\)</span>。</p>\r\n<p>在深度学习中，<span class=\"math inline\">\\(Z\\)</span>通常是难以处理的。只能使用一些近似的方法来估计。</p>\r\n<p><strong>基于能量的模型</strong></p>\r\n<p>无向模型中很多理论结果都依赖于<span class=\"math inline\">\\(\\forall x,\r\n\\widetilde{p}(x)&gt;0\\)</span>这个假设。使这个假设满足的一种简单方式就是基于能量的模型(Energy-based\r\nmodel,EBM)，其中 <span class=\"math display\">\\[\r\n\\widetilde{p}(\\textbf{x}) = \\exp(-E(\\textbf{x}))\r\n\\]</span> 其中<span class=\"math inline\">\\(E(\\textbf{x})\\)</span>被称为能量函数。</p>\r\n<p>由于<span class=\"math inline\">\\(\\exp(\\cdot)\\)</span>的非负性，我们可以完全自由地选择那些能够简化学习过程的能量函数。可以做个比较，当学习团势能的时候，还需要添加一个非负的限制再去优化。但是在学习能量函数的时候是没有任何限制的。</p>\r\n<p>服从上面式子形式的任何分布都是<strong>玻尔兹曼分布</strong>的一个实例。由于这个原因，我们把许多基于能量的模型称为<strong>玻尔兹曼机</strong>。</p>\r\n</blockquote>\r\n<p>我们在d维二值随机向量<span class=\"math inline\">\\(\\textbf{x} \\in\r\n\\{0,1\\}^d\\)</span>上定义玻尔兹曼机。他是一种基于能量的模型，意味着我们可以使用能量函数定义联合概率分布：\r\n<span class=\"math display\">\\[\r\nP(x) = \\frac{exp(-E(x))}{Z}\r\n\\]</span> 其中<span class=\"math inline\">\\(E(x)\\)</span>是能量函数，<span class=\"math inline\">\\(Z\\)</span>是确保<span class=\"math inline\">\\(\\sum_xP(x)=1\\)</span>的配分函数。玻尔兹曼机的能量函数如下给出：\r\n<span class=\"math display\">\\[\r\nE(x) = -x^\\top Ux-b^\\top x\r\n\\]</span> 其中<span class=\"math inline\">\\(U\\)</span>是模型参数的\"权重\"矩阵，<span class=\"math inline\">\\(b\\)</span>是偏置向量。</p>\r\n<p>本质而言，上述模型的表达能力是有限的，因为能量函数E是2阶多项式。它关于某个具体的<span class=\"math inline\">\\(x_i\\)</span>的边缘分布是LR（比LR多了一个平方项，但是平方项等于自身，因为<span class=\"math inline\">\\(x_i\\)</span>取值是0或1，所以还是只包含一次项）。变量与变量之间的关系是线性关系。</p>\r\n<p>如果在玻尔兹曼机里加入隐变量，或者说不是所有变量都是可见的，那么其表达能力大大加强，可以逼近<strong>任何的关于可见变量的概率分布函数</strong>。</p>\r\n<p>在上式中，把变量分为<strong>可见变量v</strong>与<strong>不可见变量h</strong>，则能量函数可以改写成\r\n<span class=\"math display\">\\[\r\nE(v,h) = -v^\\top Rv - v^\\top Wh-h^\\top Sh -b^\\top v -c^\\top h\r\n\\]</span>\r\n玻尔兹曼机的学习算法通常基于最大似然。但是所有的玻尔兹曼机都有难以处理的分配函数，因此最大似然梯度也需要使用近似方法来估计。</p>\r\n<p>当基于最大似然的学习规则时，连接两个单元的特定权重的更新仅取决于这两个单元在不同分布下收集的统计信息：<span class=\"math inline\">\\(P_{model}(v)\\)</span>和<span class=\"math inline\">\\(\\hat{P}_{data}(v)P_{model}(h|v)\\)</span>，即学习规则是<strong>局部的</strong>。网络的其余部分虽然参与塑造这些统计信息，但权重可以再完全不知道网络其余部分的条件下更新。</p>\r\n<p><strong>受限玻尔兹曼机</strong></p>\r\n<p>RBM是包含一层可观察变量和单层潜变量的无向概率图模型。RBM可以堆叠起来（一个在另一个的顶部）形成更深的模型。RBM本身的结构是一个二分图，观察层或潜层中的任何单元之间不允许存在连接。</p>\r\n<p>令观察层由一组<span class=\"math inline\">\\(n_v\\)</span>个二值随机变量组成，我们统称为向量<span class=\"math inline\">\\(\\textbf{v}\\)</span>。我们将<span class=\"math inline\">\\(n_h\\)</span>个二值随机变量的潜在或隐藏层记为<span class=\"math inline\">\\(h\\)</span>。</p>\r\n<p>受限玻尔兹曼机也是基于能量的模型，其联合概率也是由能量函数指定：\r\n<span class=\"math display\">\\[\r\nP(\\textbf{v}=v,\\textbf{h}=h) = \\frac{1}{Z}exp(-E(v,h))\r\n\\]</span> RBM的能量函数由如下给出： <span class=\"math display\">\\[\r\nE(v,h) = -b^\\top v - c^\\top h - v^\\top Wh\r\n\\]</span> 其中的Z是被称为配分函数的归一化常数： <span class=\"math display\">\\[\r\nZ = \\sum_{v}\\sum_{h}exp\\{-E(v,h)\\}\r\n\\]</span> 已证明，<span class=\"math inline\">\\(Z\\)</span>是难解的。因此，归一化联合概率分布<span class=\"math inline\">\\(P(v)\\)</span>也难以估计。但是，RBM的二分图结构具有非常特殊的性质，其条件分布<span class=\"math inline\">\\(P(\\textbf{h}|\\textbf{v})\\)</span>和<span class=\"math inline\">\\(P(\\textbf{v}|\\textbf{h})\\)</span>是<strong>因子的（即条件独立，可以单独计算每个相乘的因子，即可计算最终的条件分布）</strong>，且计算和采样也是相对简单的。</p>\r\n<p>从联合分布导出条件分布是直观的</p>\r\n<figure>\r\n<img src=\"/2023/01/16/Self-supervised-Learning/image-20221025001451606.png\" alt=\"image-20221025001451606\">\r\n<figcaption aria-hidden=\"true\">image-20221025001451606</figcaption>\r\n</figure>\r\n<p>由于我们相对可见单元<span class=\"math inline\">\\(\\textbf{v}\\)</span>计算条件概率，相对于分布<span class=\"math inline\">\\(P(\\textbf{h}|\\textbf{v})\\)</span>我们可以将它们视为常数。条件分布<span class=\"math inline\">\\(P(\\textbf{h}|\\textbf{v})\\)</span>因子相乘的本质，我们可以将向量<span class=\"math inline\">\\(\\textbf{h}\\)</span>上的联合概率写成单独元素<span class=\"math inline\">\\(h_j\\)</span>上（未归一化）分布的乘积。现在原问题变成了对单个二值<span class=\"math inline\">\\(h_j\\)</span>上的分布进行归一化的简单问题。</p>\r\n<figure>\r\n<img src=\"https://pic4.zhimg.com/v2-aa3bed434b00a6d4091d8328adea054b_b.jpg\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure>\r\n<p>现在我们可以将关于隐藏层的完全条件分布表达为因子形式，另一个条件分布也可以类似地推导和表达。</p>\r\n<figure>\r\n<img src=\"/2023/01/16/Self-supervised-Learning/image-20221025001902562.png\" alt=\"image-20221025001902562\">\r\n<figcaption aria-hidden=\"true\">image-20221025001902562</figcaption>\r\n</figure>\r\n<blockquote>\r\n<p>PGM（图概率模型）的典型问题之一就是推断问题。其定义是一致模型中的部分随机变量e，计算其他部分随机变量q的后验概率，即\r\n<span class=\"math display\">\\[\r\nP(\\textbf{q}|\\textbf{e})\r\n\\]</span> 一般的根据贝叶斯公式直接进行计算： <span class=\"math display\">\\[\r\nP(\\textbf{q}|\\textbf{e}) =\r\n\\frac{P(\\textbf{q},\\textbf{e})}{P(\\textbf{e})}\r\n\\]</span>\r\n但是在模型比较复杂，涉及的变量数量庞大的时候，直接精确计算开销太大。所以一般会采用一些近似计算的方法。</p>\r\n</blockquote>\r\n<p>下面讨论RBM参数的基本学习算法</p>\r\n<p>已知从RBM的观测数据集<span class=\"math inline\">\\(\\{v\\}_S\\)</span>来估计数据集的概率分布一般采用极大似然函数方法，即最大化似然函数：\r\n<span class=\"math display\">\\[\r\nln\\prod_{s}p(v)=\\sum_{S}\\left \\{ ln\\sum_{h}p(v,h)\\right \\}=\\sum_{S}\\left\r\n\\{ ln\\sum_{h}e^{-E(v,h)}-ln\\sum_{x,h}e^{-E(x,h)} \\right \\}\r\n\\]</span>\r\n其中v是已经观测的数据，x表示可观测的状态变量取值，大家要注意区分开来，都由v来表示容易搞混，<strong>第二项其实是归一因子，与具体的观测数据v无关</strong>。可以采用梯度下降算法来求取参数。</p>\r\n<p>似然函数求梯度如下： <span class=\"math display\">\\[\r\n\\frac{\\partial lnp(v)}{\\partial w_{ij}}=\\sum_{h}\\left \\{ p(h|v)\\cdot\r\nh_{i}v_{j} \\right \\}-\\sum_{x}\\left \\{ \\sum_{h} p(h|x)\\cdot\r\nh_{i}x_{j}\\right \\}\r\n\\\\\r\n\\frac{\\partial lnp(v)}{\\partial b_{j}}=\\sum_{h}\\left \\{ p(h|v)\\cdot\r\nv_{j} \\right \\}-\\sum_{x}\\left \\{ \\sum_{h} p(h|x)\\cdot x_{j}\\right \\}\r\n\\\\\r\n\\frac{\\partial lnp(v)}{\\partial c_{i}}=\\sum_{h}\\left \\{ p(h|v)\\cdot\r\nh_{i} \\right \\}-\\sum_{x}\\left \\{ \\sum_{h} p(h|x)\\cdot h_{i}\\right \\}\r\n\\]</span> 而又因为： <span class=\"math display\">\\[\r\n\\sum_{h} p(h|v)\\cdot h_{i}v_{j} = \\sum_{h_{i}}\\left \\{\r\n\\sum_{h_{-i}}p(h_{i}|v)\\cdot p(h_{-i}|v)h_{i}v_{j} \\right\r\n\\}=\\sum_{h_{i}}\\left \\{ p(h_{i}|v)h_{i}v_{j} \\sum_{h_{-i}}\\cdot\r\np(h_{-i}|v)\\right \\}=\\sum_{h_{i}}p(h_{i}|v)h_{i}v_{j}\r\n\\]</span> 其中，<span class=\"math inline\">\\(h_{-i}\\)</span>表示随机状态h的除第i个分量以外的其他随机分量，所以：\r\n<span class=\"math display\">\\[\r\n\\sum_{h_{-i}}p(h_{-i}|v)=1\r\n\\]</span> 因此，我们进一步简化似然函数的梯度： <span class=\"math display\">\\[\r\n\\frac{\\partial lnp(v)}{\\partial\r\nw_{ij}}=\\sum_{h_i}p(h_i|v)h_iv_j-\\sum_{x}p(x,h)h_iv_j\r\n\\\\\r\n\\frac{\\partial lnp(v)}{\\partial b_{j}}=v_{j}-\\sum_{x}p(x)x_{j}\r\n\\\\\r\n\\frac{\\partial lnp(v)}{\\partial\r\nc_j}=\\sum_{h_i}p(h_i|v)h_i-\\sum_{x}p(x,h)h_i\r\n\\]</span>\r\n上面三个式子中第二项的复杂性是随机状态分变量数目的指数函数，直接计算效率太低。这时就需要估计第二项的期望值。</p>\r\n</blockquote>\r\n<p>RBM的目标是最小化模型的边际分布和数据分布之间的差异。相比之下，自编码器是有向图模型，可以更有效地进行训练。</p>\r\n<p>简单来说，RBM是有两层结点，一层可见结点和一层隐藏层结点。两层结点是以二分图的形式连接的（所谓的受限就是同层的结点之间不能连接），且同层结点之间在给定另一层结点状态的条件下是独立的（条件独立的）。而这个模型训练的过程是首先输入X作为可见结点的输入，然后计算隐藏层结点的输出并通过激活函数，然后将激活值再作为输入，再通过可见层结点来还原原本的X。还原值和输入值之间的差距就是模型优化的目标。</p>\r\n<p>那如何训练并使用模型呢？我们使用推荐系统的例子来进一步理解。</p>\r\n<p>比如可见层的6个节点代表6首歌曲，然后每个用户是否听过这些歌（听过的话对应位置置1）作为输入向量。然后隐藏层的2个节点可能代表歌曲的类型流派。我们经过对大量用户的喜好分析后，模型具备了分析用户喜好及对应的听过哪些歌曲的能力。（即通过用户听过的歌曲推断其喜好，再从喜好反推听过哪些歌）。这个时候，我们给出一个用户，他听过2首歌，我们可以通过训练好的模型，给出他会去听另外4首没听过的歌的推荐值。基于此，我们可以实现对用户歌曲的推荐功能。</p>\r\n<h4 id=\"context-prediction-modelcpm\">Context Prediction Model(CPM)</h4>\r\n<p>CPM的思想是根据输入预测上下文信息。</p>\r\n<p>在NLP领域中，在word\r\nembedding学习中的自监督学习，CBOW和Skip-Gram是其中的先驱工作。CBOW\r\n旨在根据上下文标记预测输入标记。相比之下，Skip-Gram\r\n旨在根据输入标记预测上下文标记。</p>\r\n<h4 id=\"denoising-ae-model\">Denoising AE Model</h4>\r\n<p>去噪自编码器模型的直觉是表示应该对噪声的引入具有鲁棒性。</p>\r\n<p>MLM是其中最成功的架构之一。</p>\r\n<p>相较于AR模型，DAE模型在预测时可以结合上下文信息，而不只是单方向的信息。然而，MLM\r\n假设如果<strong>给定未屏蔽的令牌（实际上不成立），则预测的令牌是独立的</strong>这一事实长期以来一直被认为是其固有的缺点。</p>\r\n<h4 id=\"variational-ae-model\">Variational AE Model</h4>\r\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># <span class=\"doctag\">TODO:</span> learn from the weekend meeting.</span></span><br></pre></td></tr></table></figure>\r\n<h3 id=\"hybrid-generative-model\">Hybrid Generative Model</h3>\r\n<h4 id=\"ar-ae\">AR &amp; AE</h4>\r\n<p>MADE为了结合AR和AE的优势，对AE做了一些简单的修改。</p>\r\n<p>在原始的AE模型中，相邻层的神经元是通过MLP全连接的。但是在MADE中，相邻层之间的一些连接被masked掉来确保每个输入维度仅从其维度重建。</p>\r\n<p>MADE可以很容易的在条件计算上并行化，并且可以很直接且便宜地通过结合AR和AE得到高维联合分布的估计。</p>\r\n<p>在NLP中，PLM是将AR和AE的又是结合起来的表示模型。<img src=\"/2023/01/16/Self-supervised-Learning/image-20221026190346668.png\" alt=\"image-20221026190346668\"></p>\r\n<p>正式地说，令<span class=\"math inline\">\\(\\mathcal{Z}_\\mathcal{T}\\)</span>代表一个长度为T的索引序列<span class=\"math inline\">\\([1,2,...,T]\\)</span>的所有可能的排列。则PLM的目标可以表示为：\r\n<span class=\"math display\">\\[\r\n\\max_{\\theta}\\mathbb{E}_{\\textbf{z}\\sim\r\n\\mathcal{Z}_{\\mathcal{T}}}[\\sum_{t=1}^T\\log\r\np_{\\theta}(x_{z_t}|\\textbf{x}_{\\textbf{z}&lt;t})]\r\n\\]</span>\r\n实际上，对于每个文本序列，都会对不同的分解顺序进行采样。因此，每个令牌都可以从双方看到其上下文信息。基于排列顺序，XLNet\r\n还对位置进行重新参数化，让模型知道需要预测哪个位置。然后引入了一种特殊的双流自注意力来进行目标感知预测。</p>\r\n<h4 id=\"ae-flow-based-models\">AE &amp; Flow-based Models</h4>\r\n<p>在图域中，GraphAF [115]\r\n是一种基于流的自回归模型，用于生成分子图。它可以在迭代过程中生成分子，还可以并行计算确切的可能性。\r\nGraphAF\r\n将分子生成形式化为顺序决策过程。它将详细的领域知识纳入奖励设计，例如效价检查。受基于流模型的最新进展的启发，它定义了从基本分布（例如，多元高斯分布）到分子图结构的可逆变换。此外，反量化技术\r\n[56] 用于将离散数据（包括节点类型和边缘类型）转换为连续数据。</p>\r\n<h3 id=\"pros-and-cons\">Pros and Cons</h3>\r\n<ul>\r\n<li><p>优点</p>\r\n<p>生成式自我监督学习在自我监督学习中取得成功的一个原因是它能够在不假设下游任务的情况下恢复原始数据分布，这使得生成式模型能够得到广泛的应用。</p></li>\r\n<li><p>缺点</p>\r\n<ul>\r\n<li><p>尽管生成式自监督学习在生成任务中处于中心地位，但最近发现在某些分类场景中，生成式自我监督学习的竞争力远低于对比自我监督学习，因为<strong>对比学习的目标天然符合分类目标</strong>。包括\r\nMoCo [52]、SimCLR [19]、BYOL [47] 和 SwAV [18] 在内的作品在各种 CV\r\n基准测试中表现出压倒性的表现。尽管如此，在 <strong>NLP\r\n领域</strong>，研究人员仍然依赖<strong>生成语言模型来进行文本分类</strong>。</p></li>\r\n<li><p>生成目标的逐点性质存在一些固有缺陷。这个目标通常被表述为最大似然函数$\r\n_{MLE} = − _x p(x|c)$ 其中 <span class=\"math inline\">\\(x\\)</span>\r\n是我们希望建模的所有样本，<span class=\"math inline\">\\(c\\)</span>\r\n是条件约束，例如上下文信息。考虑到它的形式，MLE 有两个致命的问题：</p>\r\n<ul>\r\n<li><p>Sensitive and Conservative Distribution</p>\r\n<p>当 p(x|c) → 0 时，<span class=\"math inline\">\\(\\mathcal{L}_{MLE}\\)</span>\r\n非常大，使得生成模型对稀有样本极为敏感。它直接导致保守分布，性能低下。</p></li>\r\n<li><p>Low-level Abstraction Objective</p>\r\n<p>在MLE中，表示分布在 <span class=\"math inline\">\\(x\\)</span>\r\n的级别（即逐点级别）建模，例如图像中的像素、文本中的单词和图中的节点。然而，大多数分类任务都针对高级抽象，例如对象检测、长段落理解和分子分类。</p></li>\r\n</ul>\r\n<blockquote>\r\n<p>MLE是最大似然估计。</p>\r\n</blockquote></li>\r\n</ul></li>\r\n</ul>\r\n<p>作为一种相反的方法，generative-contrastive自我监督学习<strong>放弃了逐点目标</strong>。它转向更强大且更好地处理数据流形中的<strong>高级抽象</strong>挑战的分布匹配目标。</p>\r\n<h2 id=\"contrastive-self-supervised-learning\">Contrastive\r\nSelf-supervised Learning</h2>\r\n<p>从统计的角度来看，机器学习可以分为生成模型和判别模型。</p>\r\n<p>给定输入<span class=\"math inline\">\\(X\\)</span>和目标<span class=\"math inline\">\\(Y\\)</span>的联合分布<span class=\"math inline\">\\(P(X,Y)\\)</span></p>\r\n<ul>\r\n<li>生成模型计算<span class=\"math inline\">\\(p(X,Y)\\)</span></li>\r\n<li>判别模型计算<span class=\"math inline\">\\(p(Y|X=x)\\)</span></li>\r\n</ul>\r\n<p>由于表示学习想要的是从X中学到其特征表示，是在挖掘X中的关系。所以，长期以来认为生成模型才是表示学习的唯一选择。</p>\r\n<p>但是最近对比学习中的突破展示了判别模型在特征学习中的潜力。</p>\r\n<p>对比模型旨在从Noise Contrastive Estimation(NCE)目标中学习比较（learn\r\nto compare） <span class=\"math display\">\\[\r\n\\mathcal{L} = \\mathbb{E}_{x,x^+,x^-}[-\\log (\\frac{e^{f(x)^Tf(x^+)}}\r\n{e^{f(x)^Tf(x^+)} + e^{f(x)^Tf(x^-)}})]\r\n\\]</span> 其中<span class=\"math inline\">\\(x^+\\)</span>是和<span class=\"math inline\">\\(x\\)</span>相似的，<span class=\"math inline\">\\(x^-\\)</span>是和<span class=\"math inline\">\\(x\\)</span>不相似的，<span class=\"math inline\">\\(f\\)</span>是编码器（表示方程）。</p>\r\n<p>由于涉及更多不同的对，我们将 InfoNCE [95] 制定为： <span class=\"math display\">\\[\r\n\\mathcal{L} = \\mathbb{E}_{x,x^+,x^k}[-\\log (\\frac{e^{f(x)^Tf(x^+)}}\r\n{e^{f(x)^Tf(x^+)} + \\sum_{k=1}^{K} e^{f(x)^Tf(x^k)}})]\r\n\\]</span> 在这里，我们将最近的对比学习框架分为两种类型：</p>\r\n<ul>\r\n<li>context-instance</li>\r\n<li>instance-instance</li>\r\n</ul>\r\n<p>它们都在下游任务中取得了惊人的表现，尤其是在<strong>classification\r\nproblems under the linear protocol</strong>。</p>\r\n<blockquote>\r\n<p><strong>实验一：Linear Classification Protocol</strong></p>\r\n<p>评价一个自监督模型的性能，最关键和最重要的实验莫过于 <strong>Linear\r\nClassification Protocol</strong> 了，它也叫做 <strong>Linear\r\nEvaluation</strong>，具体做法就是先使用自监督的方法预训练\r\nEncoder，这一过程不使用任何 label。预训练完以后 Encoder\r\n部分的权重也就确定了，这时候把它的权重 freeze 住，同时在 Encoder\r\n的末尾添加Global Average Pooling和一个线性分类器\r\n(具体就是一个FC层+softmax函数)，并在某个数据集上做Fine-tune，这一过程使用全部的\r\nlabel。</p>\r\n</blockquote>\r\n<h3 id=\"context-instance-contrast\">Context-Instance Contrast</h3>\r\n<p>Context-instance对比，或所谓的global-local对比，侧重于对样本的局部特征与其全局上下文表示之间的归属关系进行建模。</p>\r\n<p>当我们学习一个局部特征的时候，我们希望它可以和全局上下文的表示想关联。比如条纹到老虎，句子到它的段落，节点到它的邻居节点。</p>\r\n<p>Context-Instance Contrast主要分为两类：</p>\r\n<ul>\r\n<li><p>Predict Relative Position(PRP)</p>\r\n<p>PRP\r\n侧重于学习<strong>局部组件之间的相对位置</strong>。全局上下文是预测这些关系的<strong>隐含要求</strong>（例如了解大象的长相对于预测其头部和尾部之间的相对位置至关重要）。</p></li>\r\n<li><p>Maximize Mutual Information(MI)</p>\r\n<p>MI专注于学习局部部分和全局上下文之间的<strong>直接归属关系</strong>。局部零件之间的相对位置被忽略。</p></li>\r\n</ul>\r\n<h4 id=\"predict-relative-position\">Predict Relative Position</h4>\r\n<p>许多数据在其部分之间都有着丰富的空间（spatial）或序列（sequential）关系。</p>\r\n<blockquote>\r\n<p>——Nice to meet you.</p>\r\n<p>——Nice to meet you,too.</p>\r\n</blockquote>\r\n<p>这种数据局部的相对关系在很多模型中都作为辅助任务（pretext\r\ntask）。</p>\r\n<p>PRP也可以作为工具来生成hard positive samples。</p>\r\n<blockquote>\r\n<p><strong>Hard positive\r\nsamples：</strong>即很容易分类成negative的positive样本。和hard negative\r\nsamples统称为hard samples。</p>\r\n</blockquote>\r\n<p>例如，PIRL [87] 中应用了拼图技术来增加正样本，但 PIRL\r\n并未将解决拼图和恢复空间关系作为其目标。</p>\r\n<blockquote>\r\n<p>PIRL 添加了上述提到的”解拼图“的困难正样本（hard positive\r\nsample）增强。为了产生一个 pretext-invariant 的表示，PIRL\r\n要求编码器把一个<strong>图像和它的拼图作为相似的对</strong>。</p>\r\n<p>即一个图像的拼图是它自己的hard positive sample</p>\r\n</blockquote>\r\n<p>NSP也是类似，判断是否是序列关系。</p>\r\n<p>为了取代 NSP，ALBERT [74] 提出了句子顺序预测 (SOP) 任务。这是因为，在\r\nNSP 中，否定的下一句是从可能与当前主题不同的其他段落中采样的，这将 NSP\r\n变成了一个<strong>更容易的主题模型问题</strong>。在 SOP\r\n中，<strong>交换位置的两个句子被视为负样本</strong>，使模型专注于<strong>语义的连贯性</strong>。</p>\r\n<h4 id=\"maximize-mutual-information\">Maximize Mutual Information</h4>\r\n<p><strong>互信息（mutual\r\ninformation）</strong>的目标是对两个变量之间的关联进行建模，我们的目标是最大化它。一般来说，这种模型优化目标：\r\n<span class=\"math display\">\\[\r\n\\max_{g_1\\in \\mathcal{G},g_2\\in \\mathcal{G}} I(g_1(x_1),g_2(x_2))\r\n\\]</span> 其中<span class=\"math inline\">\\(g_i\\)</span>是<strong>表示编码器</strong>，<span class=\"math inline\">\\(\\mathcal{G}_i\\)</span>是一类有限制的编码器，而<span class=\"math inline\">\\(I(\\cdot,\\cdot)\\)</span>则是用于准确的互信息的基于样本的估计器。</p>\r\n<p>在应用程序中，MI 因其复杂的计算而臭名昭著。一种常见的做法是使用 NCE\r\n目标交替最大化 I 的下限。</p>\r\n<p>Deep InfoMax [55]\r\n是第一个通过对比学习任务显式建模互信息的方法，该任务最大化局部补丁与其全局上下文之间的\r\nMI。对于实际实践，以图像分类为例，我们可以将<strong>猫图像 <span class=\"math inline\">\\(x\\)</span> 编码为</strong> <span class=\"math inline\">\\(f(x) \\in \\mathbb{R}^{M\\times M\\times\r\nd}\\)</span>，并取出一个<strong>局部特征向量</strong> <span class=\"math inline\">\\(v \\in\r\n\\mathbb{R}^d\\)</span>。为了在实例和上下文之间进行对比，我们需要另外两件事：</p>\r\n<ul>\r\n<li>一个摘要函数<span class=\"math inline\">\\(g\\)</span>：<span class=\"math inline\">\\(\\mathbb{R}^{M\\times M\\times d} \\rightarrow\r\n\\mathbb{R}^d\\)</span>来产生<strong>上下文向量</strong><span class=\"math inline\">\\(s = g(f(x))\\in \\mathbb{R}^d\\)</span></li>\r\n<li><strong>另一个猫图像</strong><span class=\"math inline\">\\(x^-\\)</span>和它的<strong>上下文向量</strong><span class=\"math inline\">\\(s^- = g(f(x^-))\\)</span></li>\r\n</ul>\r\n<p>这样的话，对比目标可以定义为 <span class=\"math display\">\\[\r\n\\mathcal{L} = \\mathbb{E}_{v,x}[-\\log(\\frac{e^{v^T\\cdot s}}{e^{v^T\\cdot\r\ns} + e^{v^T\\cdot s^-}})]\r\n\\]</span></p>\r\n<figure>\r\n<img src=\"/2023/01/16/Self-supervised-Learning/image-20221026215710486.png\" alt=\"image-20221026215710486\">\r\n<figcaption aria-hidden=\"true\">image-20221026215710486</figcaption>\r\n</figure>\r\n<blockquote>\r\n<p>互信息在对比学习中的应用的两个代表。 Deep InfoMax (DIM) [55]\r\n首先将图像编码为特征图，并利用Read\r\nout函数（或所谓的摘要函数）生成摘要向量。 AMDIM [7]\r\n通过随机选择<strong>图像的另一个视图</strong>来生成摘要向量来增强\r\nDIM。</p>\r\n</blockquote>\r\n<p>AMDIM [7] 增强了局部特征与其上下文之间的正相关（positive\r\nassociation）。它随机采样图像的两个不同视图（截断、变色等），分别生成<strong>局部特征向量</strong>和<strong>上下文向量</strong>。</p>\r\n<p>在语言预训练中，InfoWord [72]\r\n提出最大化<strong>句子的全局表示</strong>与其中的\r\n<strong>n-gram</strong> 之间的互信息。上下文是从句子中推断出来的，选择的\r\nn-gram 被屏蔽，负面上下文是从语料库中随机挑选出来的。</p>\r\n<p>在图学习中，Deep Graph\r\nInfoMax(DGI)将一个节点的表示视为局部特征，将随机抽样的2跳邻居节点的平均值作为上下文。但是，它很难从一个单独的图中产生负样本。为了解决这个问题，DGI提出了通过保持子图结构并重新排列节点特征来破坏原始的上下文，产生负样本。</p>\r\n<figure>\r\n<img src=\"/2023/01/16/Self-supervised-Learning/image-20221026222739800.png\" alt=\"image-20221026222739800\">\r\n<figcaption aria-hidden=\"true\">image-20221026222739800</figcaption>\r\n</figure>\r\n<blockquote>\r\n<p>Deep Graph InfoMax [136] 使用一个Read out函数生成摘要向量 <span class=\"math inline\">\\(s_1\\)</span>，并将其放入判别器中，节点 1 的嵌入\r\n<span class=\"math inline\">\\(x_1\\)</span> 和损坏的嵌入 <span class=\"math inline\">\\(\\widetilde{x}_1\\)</span>\r\n分别用于识别哪个嵌入是真正的嵌入。corruption是打乱节点的位置。</p>\r\n</blockquote>\r\n<p>DGI之后有很多工作，例如InfoGraph\r\n[118]，其目标是学习<strong>图级表示</strong>而不是节点级，最大化图级表示与不同层次子结构之间的互信息。</p>\r\n<p>作为<strong>统一图预训练</strong>的尝试，在[57]中，作者从属性/结构和节点级/图级两个维度系统地分析了图神经网络的预训练策略。对于节点级别的结构预测，他们提出了上下文预测以最大化\r\nk-hop 邻域的表示与其上下文图之间的 MI。对于化学领域的属性，他们提出了\r\nAttribute Mask 来根据其邻域预测节点的属性，这是一个类似于 BERT\r\n中的令牌掩码的生成目标。</p>\r\n<p>S2GRL [98] 进一步将上下文图中的节点分离为 k-hop\r\n上下文子图，并分别最大化它们与目标节点的\r\nMI。然而，图预训练的一个基本问题是学习<strong>跨图的归纳偏差</strong>，现有的图预训练工作<strong>仅适用于特定领域</strong>。</p>\r\n<h3 id=\"instance-instance-contrast\">Instance-Instance Contrast</h3>\r\n<p>[129] 提供了经验证据，通过表明<strong>上限 MI\r\n估计器会导致病态和较低的性能表示</strong>说明了上述模型的成功只是松散地与\r\nMI 相关。</p>\r\n<p>相反，更多应该归功于<strong>编码器架构</strong>和与<strong>度量学习</strong>相关的<strong>负采样策略</strong>。</p>\r\n<blockquote>\r\n<p>度量学习（metric learning）</p>\r\n<ul>\r\n<li><p>度量：</p>\r\n<p>先说一下关于度量这个概念：在数学中，一个度量（或距离函数）是一个定义集合中元素之间距离的函数。一个具有度量的集合被称为度量空间。</p></li>\r\n<li><p>度量学习：</p>\r\n<p>度量学习也叫作<strong>相似度学习</strong>，根据这个叫法作用就很明确了。</p>\r\n<p>之所以要进行度量学习，一方面在一些算法中需要依赖给定的度量：如Kmeans在进行聚类的时候就用到了欧式距离来计算样本点到中心的距离、KNN算法也用到了欧式距离等。这里计算的度量，就是比较样本点与中心点的相似度。</p>\r\n<p>这里的度量学习在模式识别领域，尤其是在图像识别这方面，在比较两张图片是否是相同的物体，就通过比较两张图片的<strong>相似度</strong>，相似度大可能性就高。</p>\r\n<p>因为在研究时间序列这方面的问题，所以想到了在时间序列中度量学习的体现，如果是判断两个区间的相似性，通常用到的度量方式就是采用常用到的欧式或者其他人为定义的距离函数，这样也就局限于了这样一个二维或者多维的空间中，而如果是用到Flood\r\nSung大神提出的方法的话，我们把思路拓宽，能不能也是用神经网络来训练这个度量，这样的好处就是：</p>\r\n<ul>\r\n<li>长度不同的片段也可以进行比较。</li>\r\n<li>可以拓宽维度，从其他维度上寻找关联。</li>\r\n</ul></li>\r\n</ul>\r\n</blockquote>\r\n<p>度量学习的一个重要重点是在提高<strong>负采样效率</strong>的同时执行<strong>硬正采样</strong>。它们可能在基于\r\nMI 的模型的成功中发挥更关键的作用。</p>\r\n<p>作为替代方案，Instance-Instance Contrast丢弃 MI，和<strong>metric\r\nlearnig</strong>那样直接研究不同样本的<strong>实例级局部表示之间的关系</strong>。</p>\r\n<p>实例级表示，而不是上下文级，对于广泛的分类任务更为重要。比如句子情感分类，主要依靠的还是少数的关键词。</p>\r\n<p><strong>两个阶段</strong></p>\r\n<ul>\r\n<li><p>早期</p>\r\n<p>研究人员借鉴了半监督学习的思想，通过基于聚类的判别产生伪标签，并在表示上取得了较好的性能。</p></li>\r\n<li><p>近期</p>\r\n<p>最近，CMC [126]、MoCo [52]、SimCLR [19] 和 BYOL [47]\r\n通过优于上下文实例对比方法进一步支持上述结论，并在线性分类协议下实现了<strong>与监督方法的竞争结果</strong>。</p></li>\r\n</ul>\r\n<h4 id=\"cluster-discrimination\">Cluster Discrimination</h4>\r\n<p>图片分类问题要求同一个类别中的图片的表示应该是相似的。因此，动机是将相似图像在embedding空间中的表示要相近。</p>\r\n<p>在监督学习中，这个拉进的过程是通过标签监督来实现的。而在自监督学习中，我们没有这样的标签。为了解决标签问题，Deep\r\nCluster提出利用<strong>聚类来产生伪标签</strong>，并要求判别器来预测图片的标签。</p>\r\n<p>训练可以分成两部：</p>\r\n<ul>\r\n<li>Deep\r\nCluster使用k-means来对<strong>编码出的representation</strong>进行聚类，并未每个样本<strong>生成伪标签</strong></li>\r\n<li>判别器预测两个样本<strong>是否来自同一个簇</strong>，并反向传播到<strong>编码器</strong>。</li>\r\n</ul>\r\n<p>最近，Local Aggregation (LA) [162] 推进了基于集群的方法的边界。\r\n它指出了DeepCluster 的几个缺点并进行了相应的优化。</p>\r\n<ul>\r\n<li>首先，在 DeepCluster 中，样本被分配到互斥集群，但 LA\r\n为每个样本分别识别邻居。</li>\r\n<li>其次，DeepCluster 优化了交叉熵判别损失，而 LA\r\n采用了直接优化局部软聚类度量的目标函数。这两个变化大大提高了 LA\r\n表示在下游任务上的性能。</li>\r\n</ul>\r\n<p>基于聚类的鉴别也可能有助于其他预训练模型的泛化，更好地将模型从<strong>辅助任务</strong>转移到下游任务。</p>\r\n<p>传统的表示学习模型只有两个阶段：一个用于预训练，另一个用于评估。\r\nClusterFit [146] 在上述两个阶段之间引入了一个类似于 DeepCluster\r\n的聚类预测微调阶段，提高了表示在下游分类评估中的性能。</p>\r\n<p>尽管先前基于集群判别的对比学习取得了成功，但与后来的基于实例判别的方法（包括\r\nCMC [126]、MoCo [52] 和 SimCLR\r\n[19]）相比，两阶段训练范式<strong>耗时且性能不佳</strong>。这些基于实例区分的方法<strong>摆脱了缓慢的聚类阶段</strong>，并引入了有效的<strong>数据增强（即多视图）策略</strong>来提高性能。鉴于这些问题，SwAV\r\n[18] 的作者将在线聚类思想和多视图数据增强策略引入了聚类判别方法。 SwAV\r\n提出了一种交换预测对比目标来处理多视图增强。直觉是，给定一些（clustered）原型，相同图像的不同视图应该分配到相同的原型中。\r\nSwAV 将此“assignment”命名为“codes”。为了加速code计算，SwAV\r\n的作者设计了一种在线计算策略。当模型尺寸较小且计算效率更高时，SwAV\r\n优于基于实例区分的方法。基于 SwAV，在从 Instagram 收集的 10\r\n亿张网络图像上训练了一个 13 亿参数的 SEER [46]。</p>\r\n<p>在图学习中，M3S [121] 采用类似的思想来执行 DeepCluster\r\n式的<strong>自监督预训练</strong>，以获得更好的<strong>半监督预测</strong>。给定少量标记数据和许多未标记数据，对于每个阶段，M3S\r\n首先预训练自己以在未标记数据上生成伪标签，就像 DeepCluster\r\n所做的那样，然后将这些伪标签与在标记数据上受监督训练的模型预测的伪标签进行比较。只有前\r\nk\r\n个置信度标签被添加到标签集中，用于下一阶段的半监督训练。在[153]中，这个想法被进一步发展为三个预训练任务：拓扑分区（类似于谱聚类）、节点特征聚类和图完成。</p>\r\n<h4 id=\"instance-discrimination\">Instance Discrimination</h4>\r\n<p>利用实例辨别作为借口任务的原型是 InstDisc [142]。基于 InstDisc，CMC\r\n[126] 提出采用图像的多个不同视图作为正样本，并将另一个作为负样本。 CMC\r\n在嵌入空间中接近图像的多个视图，并远离其他样本。然而，它在某种程度上受到\r\nDeep InfoMax\r\n思想的限制，它只对每个正样本<strong>采样一个负样本</strong>。</p>\r\n<p>在 MoCo [52] 中，研究人员进一步发展了通过<strong>动量对比利用instance\r\ndiscrimination</strong>的想法，这大大<strong>增加了负样本的数量</strong>。例如，给定输入图像\r\n<span class=\"math inline\">\\(x\\)</span>，我们的直觉是通过可以将 <span class=\"math inline\">\\(x\\)</span> 与任何其他图像区分开来的query encoder\r\n<span class=\"math inline\">\\(f_q(\\cdot)\\)</span> 学习instinct\r\nrepresentation <span class=\"math inline\">\\(q =\r\nf_q(x)\\)</span>。因此，对于一组其他图像 <span class=\"math inline\">\\(x_i\\)</span>，我们采用异步更新的key encoder <span class=\"math inline\">\\(f_k(\\cdot)\\)</span> 来产生 <span class=\"math inline\">\\(k_+ = f_k(x)\\)</span> 和 <span class=\"math inline\">\\(k_i = f_k(x_i)\\)</span>​，并优化以下目标 <span class=\"math display\">\\[\r\n\\mathcal{L} = -log\\frac{exp(q\\cdot k_+/\\tau)}{\\sum_{i=0}^Kexp(q\\cdot\r\nk_i/\\tau)}\r\n\\]</span> 其中<span class=\"math inline\">\\(K\\)</span>是负样本的个数，这个公式是InfoNCE的形式。</p>\r\n<p>此外，MoCo 在处理负采样效率方面提出了另外两个关键思想。</p>\r\n<ul>\r\n<li>首先，它摒弃了传统的端到端训练框架。它设计了具有<strong>两个编码器</strong>（查询和键）的动量对比学习，防止了损失收敛在开始阶段的波动。</li>\r\n<li>其次，为了扩大负样本的容量，MoCo 采用了一个队列（K 高达\r\n65536）将最近编码的批次保存为负样本。这显着提高了负采样效率。</li>\r\n</ul>\r\n<p>还有一些其他辅助技术可以确保训练收敛，例如批量混洗以避免琐碎的解决方案和温度超参数\r\n<span class=\"math inline\">\\(\\tau\\)</span> 来调整规模。</p>\r\n<p>然而，MoCo\r\n采用了一种过于简单的正样本策略：一对正表示来自同一个样本，没有任何转换或增强，使得<strong>正对非常容易区分</strong>。\r\nPIRL [87] 添加了<strong>拼图增强</strong>，如第 4.1.1 节所述。 PIRL\r\n要求编码器将图像及其拼图视为相似的对，以产生pretext-invariant的表示。</p>\r\n<p>在 SimCLR [19] 中，作者通过引入 10\r\n种形式的数据增强进一步说明了<strong>硬正样本策略的重要性</strong>。这种数据增强类似于\r\nCMC [126]，它利用几种不同的视图来增强正对。</p>\r\n<p>SimCLR 遵循端到端训练框架，而不是 MoCo\r\n的动量对比，为了处理大规模负样本问题，SimCLR 选择了 N\r\n的批量大小为8196。</p>\r\n<p>N 个样本的 minibatch 被扩充为 2N 个样本$ _j(j = 1, 2, ..., 2N\r\n)$。</p>\r\n<p>对于一对正样本 <span class=\"math inline\">\\(\\hat{x}_i\\)</span> 和\r\n<span class=\"math inline\">\\(\\hat{x}_j\\)</span>\r\n（源自一个原始样本），其他 2(N - 1) 被视为负样本。成对对比损失 NT-Xent\r\n损失 [21] 定义为 <span class=\"math display\">\\[\r\nl_{i,j} = -\\log\r\n\\frac{exp(sim(\\hat{x}_i,\\hat{x}_j)/\\tau)}{\\sum_{k=1}^{2N}\r\n\\mathbb{I}_{[k\\neq i]}exp(sim(\\hat{x}_i,\\hat{x}_k)/\\tau)}\r\n\\]</span> 注意<span class=\"math inline\">\\(l_{i,j}\\)</span>​是不对称的。这里的 sim(·,·)\r\n函数是一个余弦相似度函数，可以对表示进行归一化。总损失为 <span class=\"math display\">\\[\r\n\\mathcal{L} = \\frac{1}{2N}\\sum_{k=1}^N[l_{2i-1,2i}+l_{2i,2i-1}]\r\n\\]</span> SimCLR\r\n还提供了一些其他实用技术，包括表示和对比损失之间的可学习非线性变换、更多的训练步骤和更深的神经网络。\r\n[23] 进行消融研究以表明 SimCLR 中的技术还可以进一步提高 MoCo\r\n的性能。</p>\r\n<p>InfoMin [127]\r\n对增加正样本进行了更多调查。作者声称我们应该选择那些<strong>相互信息较少的视图</strong>，以便在对比学习中获得<strong>更好的增强视图</strong>。在最佳情况下，视图应该<strong>只共享标签信息</strong>。为了产生这样的最佳视图，作者首先提出了一种无监督方法来<strong>最小化视图之间的互信息</strong>。但是，这可能会导致预测标签的信息丢失（例如纯空白视图）。因此，提出了一种半监督方法来查找仅共享标签信息的视图。与\r\nMoCo v2 相比，该技术提高了约 2%。</p>\r\n<p>BYOL [47]\r\n迈出了更激进的一步，它<strong>丢弃了自监督学习中的负采样</strong>，但比\r\nInfoMin\r\n取得了更好的结果。对于我们上面提到的对比学习方法，它们通过预测同一图像的不同视图来学习表示，并将预测问题直接投射到表示空间中。然而，直接在表示空间中进行预测可能会导致表示崩溃，因为多视图通常对彼此来说是too\r\npredictive。如果没有负样本，神经网络就很容易区分这些正视图。</p>\r\n<p>在 BYOL\r\n中，研究人员认为在这个过程中可能不需要负样本。结果表明，如果我们使用一个固定的随机初始化网络(它不会崩溃，因为它没有被训练)作为key编码器，query编码器产生的表示仍然会在训练过程中得到改善。</p>\r\n<p>如果然后我们将<strong>目标编码器</strong>设置为经过训练的<strong>query编码器</strong>并迭代此过程，我们将逐步获得更好的性能。因此，BYOL\r\n提出了一种具有指数移动平均策略的架构（图 14）来更新目标编码器，就像 MoCo\r\n所做的那样。此外，他们没有使用交叉熵损失，而是遵循回归范式，其中均方误差用作：</p>\r\n<figure>\r\n<img src=\"/2023/01/16/Self-supervised-Learning/image-20221027003350538.png\" alt=\"image-20221027003350538\">\r\n<figcaption aria-hidden=\"true\">image-20221027003350538</figcaption>\r\n</figure>\r\n<p>在 SimSiam [24]\r\n中，研究人员进一步研究了<strong>负采样的必要性</strong>，甚至是对比表示学习中的批量归一化。他们表明，BYOL\r\n中最关键的组件是停止梯度操作，它使目标表示稳定。事实证明，SimSiam\r\n的收敛速度比 MoCo、SimCLR 和 BYOL 更快，批量更小，而性能仅略有下降。</p>\r\n<p>其他一些作品的灵感来自对对比目标的理论分析。 ReLIC [88]\r\n认为对比预训练教会编码器有原因地(causally)解开图像中不变的内容（即主要对象）和风格（即环境）。为了在数据增强中更好地执行这一观察，他们建议在图像不同视图的预测\r\nlogits 之间添加一个额外的 KL-divergence\r\n正则化器。结果表明，这可以增强模型的泛化能力和鲁棒性，提高性能。</p>\r\n<p>在图学习中，图对比编码（GCC）[101]是利用实例判别作为结构信息预训练的借口任务的先驱。对于每个节点，我们通过重新启动的随机游走独立地对两个子图进行采样，并使用它们的归一化图拉普拉斯矩阵中的顶部特征向量作为节点的初始表示。然后我们使用\r\nGNN 对它们进行编码并计算 InfoNCE 损失，就像 MoCo 和 SimCLR\r\n所做的那样，其中来自同一节点（在不同子图中）的 nod 13\r\n嵌入被视为相似。结果表明，GCC 比以前的工作如 struc2vec [110]、GraphWave\r\n[40] 和 ProNE [155] 学习了更好的可转移结构知识。 GraphCL [152]\r\n研究图学习中的数据增强策略。他们提出了四种基于边缘扰动和节点丢弃的不同增强方法。它进一步表明，这些策略的适当组合可以产生更好的性能。</p>\r\n<h3 id=\"self-supervised-contrastive-pre-training-for-semisupervised-self-training\">Self-supervised\r\nContrastive Pre-training for Semisupervised Self-training</h3>\r\n<p>虽然基于对比学习的自我监督学习继续推动各种基准的界限，但标签仍然很重要，因为自我监督学习和监督学习的训练目标之间存在差距。换句话说，无论自监督学习模型如何改进，它们仍然只是强大的特征提取器，而要转移到下游任务，我们或多或少仍然需要标签。因此，为了弥合自我监督预训练和下游任务之间的差距，我们正在寻找半监督学习。</p>\r\n<p>回想一下在 ImageNet 排行榜上名列前茅的 MoCo\r\n[52]。尽管它被证明对许多其他下游视觉任务有益，但它未能改进 COCO\r\n对象检测任务。随后的一些工作 [90]、[163]\r\n研究了这个问题，并将其归因于<strong>实例识别和对象检测之间的差距</strong>。在这种情况下，虽然纯粹的自我监督预训练无法提供帮助，但基于半监督的自我训练可以对此做出很大贡献。</p>\r\n<p>首先，我们将阐明<strong>半监督学习（semi-supervised）</strong>和<strong>自我训练（self-training）</strong>的定义。半监督学习是一种机器学习方法，它在训练期间将<strong>少量标记数据与许多未标记数据相结合</strong>。各种方法源自对数据分布所做的几种不同假设，其中自我训练（或自我标记）是最古老的。在自我训练中，模型在少量标记数据上进行训练，然后在未标记数据上生成标签。只有那些<strong>具有高度置信度标签的数据</strong>才能与<strong>原始标记数据相结合</strong>去训练新模型。我们迭代此过程以找到最佳模型。</p>\r\n<p>ImageNet 上当前最先进的监督模型 [143]\r\n遵循自我训练范式，我们首先在标记的 ImageNet 图像上训练 EfficientNet\r\n模型，并将其用作教师在 300M\r\n未标记图像上生成伪标签。然后，我们基于标记和伪标记图像训练更大的\r\nEfficientNet\r\n作为学生模型。我们通过将学生放回老师来重复这个过程。在<strong>伪标签生成过程中，教师没有噪声</strong>，因此伪标签尽可能准确。然而，在<strong>学生的学习过程中，我们通过\r\nRandAugment 向学生注入了\r\ndropout、随机深度和数据增强等噪声</strong>，以便比教师更好地概括。</p>\r\n<p>鉴于半监督自我训练的成功，很自然地重新思考其与自监督方法的关系，尤其是与成功的对比预训练方法的关系。在\r\n4.2.1 节中，我们介绍了 M3S\r\n[120]，它试图将<strong>基于集群的对比预训练</strong>和下游<strong>半监督学习</strong>结合起来。对于计算机视觉任务，Zoph\r\n等人。 [163] 研究 MoCo\r\n<strong>预训练</strong>和<strong>自我训练</strong>方法，其中教师首先在下游数据集（例如\r\nCOCO）上进行训练，然后在未标记的数据（例如\r\nImageNet）上生成伪标签，最后让学生学习联合在下游数据集上的真实标签和未标记数据上的伪标签。他们惊讶地发现，预训练的表现会受到负面影响，而自我训练仍然受益于强大的数据增强。此外，更多的标记数据会降低预训练的价值，而半监督自我训练总是会提高。他们还发现预训练和自我训练的改进是相互正交的，即从不同的角度对性能做出贡献。联合预训练和自我训练的模型是最好的。</p>\r\n<p>陈等人[20] 的 SimCLR v2 支持上述结论，表明<strong>仅使用 10% 的原始\r\nImageNet 标签</strong>，ResNet-50\r\n可以通过<strong>联合预训练和自训练超过有监督的标签</strong>。他们提出了一个\r\n3 步框架：</p>\r\n<ul>\r\n<li>像 SimCLR v1 一样进行自我监督预训练，并进行一些小的架构修改和更深的\r\nResNet。</li>\r\n<li>只用原始 ImageNet 标签的 1% 或 10% 微调最后几层。</li>\r\n<li>使用微调网络作为教师，在未标记的数据上产生标签，以训练较小的学生\r\nResNet-50。</li>\r\n</ul>\r\n<p>将自我监督对比预训练和半监督自我训练相结合的成功为我们打开了未来数据高效深度学习范式的视野。预计会有更多工作来研究它们的潜在机制。</p>\r\n<h3 id=\"pros-and-cons-1\">Pros and Cons</h3>\r\n<p>因为对比学习假设下游应用是<strong>分类</strong>，与生成模型相比，它只使用编码器并丢弃架构中的解码器。因此，对比模型通常是<strong>轻量级</strong>的，并且<strong>在判别性下游应用中表现更好</strong>。</p>\r\n<p>对比学习与度量学习密切相关，这是一门研究已久的学科。然而，自监督对比学习仍然是一个新兴领域，还有许多问题有待解决，包括：</p>\r\n<ul>\r\n<li><p>Scale to natural language pre-training</p>\r\n<p>尽管它在计算机视觉方面取得了成功，但对比预训练并没有在 NLP\r\n基准测试中呈现出令人信服的结果。 NLP 中的大多数对比学习现在都在于 BERT\r\n的监督微调，例如改进 BERT 的句子级表示 [109]、信息检索\r\n[65]。很少有算法被提出在<strong>预训练阶段应用对比学习</strong>。由于大多数语言理解任务都是分类，因此对比语言预训练方法应该比当前的生成语言模型更好。</p></li>\r\n<li><p>Sampling efficiency</p>\r\n<p>对于大多数对比学习来说，负抽样是必须的，但这个过程通常很棘手、有偏见且耗时。\r\nBYOL [47] 和 SimSiam [24] 是对比学习摆脱负样本的先驱，但它可以改进。\r\n同时我们还不清楚负抽样在对比学习中的作用。</p></li>\r\n<li><p>Data augmentation</p>\r\n<p>研究人员已经证明，数据增强可以提高对比学习的表现，但关于它为什么以及如何帮助的理论仍然很模糊。这阻碍了它在<strong>数据是离散和抽象的其他领域的应用，例如\r\nNLP 和图学习。</strong></p></li>\r\n</ul>\r\n<h2 id=\"contrastive-self-supervised-representation-learning\">Contrastive\r\nSelf-supervised Representation Learning</h2>\r\n<h3 id=\"relationship-with-supervised-learning\">Relationship with\r\nSupervised Learning</h3>\r\n<p>自监督学习遵循监督学习模式。经验表明用于预训练任务的对比学习对下游的分类任务特别有效。我们想知道对比性预训练如何使监督学习受益，尤其是关于自我监督学习是否可以比监督学习学习更多，至少在准确性方面。</p>\r\n<figure>\r\n<img src=\"/2023/01/16/Self-supervised-Learning/image-20221027093910961.png\" alt=\"image-20221027093910961\">\r\n<figcaption aria-hidden=\"true\">image-20221027093910961</figcaption>\r\n</figure>\r\n<blockquote>\r\n<p>关于监督预训练vs从头开始监督训练的三个可能假设。</p>\r\n</blockquote>\r\n<p>关于预训练任务对监督学习的提升作用，这里有三个假设：</p>\r\n<ul>\r\n<li>总是带来提升</li>\r\n<li>以更少的标签达到更高的准确度，但稳定到与基线相同的准确度</li>\r\n<li>在精度达到稳定水平之前收敛到基线性能</li>\r\n</ul>\r\n<p>他们通过渲染对可以提供任意多标签的合成 COCO\r\n进行实验，发现自监督预训练遵循 (c) 中的模式。</p>\r\n<p>表明自监督学习不能比监督学习学习更多，但可以用很少的标签做到这一点。</p>\r\n<p>尽管自监督学习不能帮助提高准确性，但它可以在许多其他方面学习更多，例如模型的<strong>鲁棒性和稳定性</strong>。亨德利克斯等人。\r\n[54]\r\n发现自我监督训练的神经网络对对抗性示例、标签损坏和常见输入损坏具有很强的鲁棒性。更重要的是，它极大地有利于对<strong>困难的、接近分布的异常值进行分布外检测（out-of-distribution\r\ndetection on difficult, near-distribution\r\noutliers）</strong>，以至于它超过了完全监督方法的性能。</p>\r\n<blockquote>\r\n<p>在开放世界中分类是验证模型安全性的重要方式，也是一个真正能够商用落地的模型不可避免要面对的问题。传统的分类模型都是在一个封闭的世界中进行训练，即假设测试数据和训练数据都来自同样的分布（称作“分布内”，in-distribution）。例如我们利用一组猫狗照片训练一个猫狗分类器。然而，部署的模型在实际使用中总是会遇到一些不属于封闭世界类别的图片，例如老虎。或者也会遇到一些和训练图片视觉上大相径庭的照片，例如卡通猫。模型应当如何去处理这些不属于训练分布的图片（即分布外样本，out-of-distribution），是开放世界领域所关注的问题。</p>\r\n<ul>\r\n<li>OD: Outlier Detection, “离群检测”</li>\r\n<li>AD: Anomaly Detection, “异常检测”</li>\r\n<li>ND: Novelty Detection, “新类检测”</li>\r\n<li>OSR: Open Set Recognition, “开集识别”</li>\r\n<li>OOD Detection: Out-of-Distribution Detection, “分布外检测”</li>\r\n</ul>\r\n</blockquote>\r\n<h3 id=\"understand-contrastive-loss\">Understand Contrastive Loss</h3>\r\n<p>在 [139] 中，Wang\r\n等人。对对比损失函数进行有趣的理论分析，并将其分为两个术语： <span class=\"math display\">\\[\r\n\\mathcal{L}_{contrast} = \\mathbb{E}[-\\log\r\n\\frac{e^{f^T_xf_y/\\tau}}{e^{f^T_xf_y/\\tau} + \\sum_i\r\ne^{f^T_xf_{y_i^-}/\\tau}} ]\r\n\\\\\r\n=\\mathbb{E}[-f^T_xf_y/\\tau] + \\mathbb{E}[log(e^{f^T_xf_y/\\tau} + \\sum_i\r\ne^{f^T_xf_{y_i^-}/\\tau})]\r\n\\]</span>\r\n其中，第一项称为alignment，第二项称为uniformity。其中第一项旨在“对齐”，第二项旨在给定标准化条件下球体上样本向量的“均匀性”。</p>\r\n<p>实验表明，这两个术语与下游任务有很大的一致性。此外，作者探索直接优化alignment和uniformity损失：<img src=\"/2023/01/16/Self-supervised-Learning/image-20221027101547987.png\" alt=\"image-20221027101547987\"></p>\r\n<p>他们在广泛的场景中进行实验，包括在计算机视觉或自然语言处理任务中使用\r\nCNN 或\r\nRNN，并发现<strong>直接优化始终优于对比损失</strong>。此外，对齐和均匀性对于良好的表示<strong>都是必要的</strong>。当这两个损失的权重之一太大时，表示会崩溃。</p>\r\n<p>然而，alignment和uniformity是否必然以上两个损失的形式存在疑问，因为在\r\nBYOL [47]\r\n中，作者展示了一个没有直接负采样但优于所有先前对比学习预训练的框架。这向我们表明，我们仍然可以通过指数移动平均、批量归一化、正则化和随机初始化等其他技术来实现uniformity。</p>\r\n<h3 id=\"generalization\">Generalization</h3>\r\n<p>似乎很直观，最小化上述损失函数应该可以使表示更好地捕捉不同实体之间的“相似性”，但尚不清楚为什么学习的表示也应该在下游任务（例如线性分类任务）上带来更好的性能。</p>\r\n<p>直观地说，自监督表示学习框架必须捕获<strong>未标记数据中的特征</strong>以及<strong>与下游任务中隐含的语义信息的相似性</strong>。\r\n[5] 提出了一个概念框架来分析平均分类任务的对比学习。</p>\r\n<p>对比学习假设相似的数据对<span class=\"math inline\">\\((x,x^+)\\)</span>来自一个分布<span class=\"math inline\">\\(\\mathcal{D}_{sim}\\)</span>，而负样本<span class=\"math inline\">\\((x_1^-,x_2^-,...,x_k^-)\\)</span>来自一个可能与<span class=\"math inline\">\\(x\\)</span>无关的分布<span class=\"math inline\">\\(\\mathcal{D}_{neg}\\)</span></p>\r\n<p>在语义相似的点是从同一个潜在类中采样的假设下，无监督损失可以表示为：</p>\r\n<figure>\r\n<img src=\"/2023/01/16/Self-supervised-Learning/image-20221027102903911.png\" alt=\"image-20221027102903911\">\r\n<figcaption aria-hidden=\"true\">image-20221027102903911</figcaption>\r\n</figure>\r\n<p>自监督学习目的是发现一个在所用编码器的容量范围内最小化经验无监督损失的<span class=\"math inline\">\\(\\hat{f} \\in \\arg\r\n\\min_f\\hat{L}_{un}(f)\\)</span>。</p>\r\n<p>由于负点是从数据集中以相同方式独立采样的，因此可以根据从中抽取负样本的潜在类别将<span class=\"math inline\">\\(\\mathcal{L}_{un}\\)</span>分解为 <span class=\"math inline\">\\(\\tau\\mathcal{L}_{un}^=\\)</span> 和 <span class=\"math inline\">\\((1 - \\tau )\\mathcal{L}_{un}^\\neq\\)</span>。</p>\r\n<p>类内偏差 <span class=\"math inline\">\\(s(f ) \\geq c′(\\mathcal{L}_{un}(f\r\n) − 1)\\)</span> 控制了 <span class=\"math inline\">\\(\\mathcal{L}_{un}(f\r\n)\\)</span>\r\n并暗示由负采样策略引起的意外损失是与我们的优化目标相矛盾的。</p>\r\n<p>在只有 1 个负样本的情况下，证明优化无监督损失有利于下游分类任务：\r\n<span class=\"math display\">\\[\r\n\\mathcal{L}_{sup}(\\hat{f})\\leq\\mathcal{L}_{sup}^\\mu(\\hat{f})\\leq\\mathcal{L}_{un}^\\neq(f)\r\n+ \\beta s(f) + \\eta Gen_M\r\n\\]</span> 概率至少为 <span class=\"math inline\">\\(1 -\r\n\\delta\\)</span>，<span class=\"math inline\">\\(f\\)</span>\r\n是编码器可以捕获的特征映射函数，<span class=\"math inline\">\\(Gen_M\\)</span> 是泛化误差。当采样对 <span class=\"math inline\">\\(M \\rightarrow inf\\)</span> 和潜在类的数目 <span class=\"math inline\">\\(|C| \\rightarrow inf, Gen_M\\)</span> 和 <span class=\"math inline\">\\(\\delta \\rightarrow\r\n0\\)</span>。如果编码器足够强大并且使用足够多的样本进行训练，那么 <span class=\"math inline\">\\(\\mathcal{L}_{un}^\\neq\\)</span> 和 <span class=\"math inline\">\\(\\beta_s(f)\\)</span> 低的学习函数 <span class=\"math inline\">\\(f\\)</span> 将在监督任务上具有良好的性能（低<span class=\"math inline\">\\(\\mathcal{L}_{sup}(\\hat{f}))\\)</span>。</p>\r\n<p>对比学习也有局限性。事实上，对比学习并不总是选择最好的监督表示函数\r\n<span class=\"math inline\">\\(f\\)</span>。最小化无监督损失以获得低 <span class=\"math inline\">\\(\\mathcal{L}_{sup}( \\hat{f} )\\)</span> 并不意味着\r\n<span class=\"math inline\">\\(\\hat{f}≈ \\widetilde{f}= \\arg \\min_f\r\n\\mathcal{L}_{sup}\\)</span> 因为高 <span class=\"math inline\">\\(\\mathcal{L}_{un}^\\neq\\)</span> 和高 <span class=\"math inline\">\\(s(f)\\)</span> 并不意味着高 <span class=\"math inline\">\\(\\mathcal{L}_{sup}\\)</span>，导致算法失败。</p>\r\n<p>在平均分类器损失 <span class=\"math inline\">\\(\\mathcal{L}_{sup}^\\mu\\)</span>的条件下进一步探索\r\n<span class=\"math inline\">\\(\\mathcal{L}_{sup}( f )\\)</span> 和<span class=\"math inline\">\\(\\mathcal{L}_{sup}( \\hat{f}\r\n)\\)</span>之间的关系，其中 <span class=\"math inline\">\\(\\mu\\)</span>\r\n表示一个标签 <span class=\"math inline\">\\(c\\)</span> 只对应一个嵌入向量\r\n<span class=\"math inline\">\\(\\mu_c := \\mathbb{E}_{x∼D_c}\\sim [f\r\n(x)]\\)</span>.如果存在一个在强意义上具有类内集中度的函数 <span class=\"math inline\">\\(f\\)</span>，并且可以用均值分类器将具有高边距（平均）的潜在类分开，则\r\n<span class=\"math inline\">\\(\\mathcal{L}_{sup}^\\mu(\\hat{f})\\)</span>\r\n将较低。如果 <span class=\"math inline\">\\(f (X)\\)</span>\r\n对于每个类在每个方向上都是$ ^2 - sub - Gaussian$ 并且具有最大范数 <span class=\"math inline\">\\(\\textbf{R} =\\max_{x\\ in\\mathcal{X}} ||f\r\n(x)||\\)</span>，则 <span class=\"math inline\">\\(\\mathcal{L}_{sup}^\\mu (\r\n\\hat{f})\\)</span> 可以由 $_{sup}^(f) $控制。</p>\r\n","tags":["Self-Supervised Learning"]},{"title":"VariationalInference","url":"/2023/01/16/VariationalInference/","content":"<h1 id=\"variational-inference\">Variational Inference</h1>\r\n<blockquote>\r\n<p>本文主要介绍变分推断的相关数学基础，并和与其功能类似的吉布斯采样进行深入的比较和总结。</p>\r\n<span id=\"more\"></span>\r\n</blockquote>\r\n<h2 id=\"background\">Background</h2>\r\n<p>在贝叶斯模型中，隐变量帮助掌握数据的分布。 <span class=\"math display\">\\[\r\np(\\textbf{z},\\textbf{x}) = p(\\textbf{z})p(\\textbf{x}|\\textbf{z})\r\n\\\\\r\np(\\textbf{z}|\\textbf{x}) =\r\n\\frac{p(\\textbf{z},\\textbf{x})}{p(\\textbf{x})}\r\n\\]</span> 其中，贝叶斯模型中的<strong>推理</strong>指的是<span class=\"math inline\">\\(p(\\textbf{z}|\\textbf{x})\\)</span>。</p>\r\n<p>假设观测数据有随机变量<span class=\"math inline\">\\(y\\in\r\n\\mathcal{Y}\\)</span>表示，模型由随机变量<span class=\"math inline\">\\(x\\in\r\n\\mathcal{X}\\)</span>表示，贝叶斯学习通过贝叶斯定理计算给定数据条件下的模型的<strong>后验概率</strong>，并选择后验概率最大的模型。</p>\r\n<p>后验概率 <span class=\"math display\">\\[\r\np(x|y) =\r\n\\frac{p(x)p(y|x)}{\\int_{\\mathcal{X}}p(y|x&#39;)p(x&#39;)dx&#39;}\r\n\\]</span> 贝叶斯经常需要三种积分运算：</p>\r\n<ul>\r\n<li><p>规范化：后验概率计算中需要的： <span class=\"math display\">\\[\r\n\\int_{\\mathcal{X}}p(y|x&#39;)p(x&#39;)dx&#39;\r\n\\]</span></p></li>\r\n<li><p>边缘化：如果有隐变量<span class=\"math inline\">\\(z\\in\\mathcal{Z}\\)</span>，后验概率的计算需要边缘化计算\r\n<span class=\"math display\">\\[\r\np(x|y)=\\int_{\\mathcal{Z}}p(x,z|y)dz\r\n\\]</span></p></li>\r\n<li><p>数学期望：如果有一个函数<span class=\"math inline\">\\(f(x)\\)</span>，可以计算该函数关于后验概率分布的数学期望\r\n<span class=\"math display\">\\[\r\nE_{p(x)}[f(x)] = \\int_{\\mathcal{X}}f(x)p(x|y)dx\r\n\\]</span></p></li>\r\n</ul>\r\n<p>当观测数据和模型都很复杂的时候，以上积分变得困难，需要对其进行估计，主要由两种近似的计算方法：</p>\r\n<ul>\r\n<li>MCMC(Markov Chain Monte Carlo Sampling)</li>\r\n<li>VI(Variational Inference)</li>\r\n</ul>\r\n<p>MCMC目前作为贝叶斯统计中的重要的工具，已经被广泛地研究和使用。但是，当数据集很大或模型非常复杂时，如果我们需要比MCMC更快速地估计近似条件时，就需要VI来提供一种替代方案。</p>\r\n<p>不是使用采样，VI背后的主要思想是使用<strong>最优化</strong>：</p>\r\n<ul>\r\n<li><p>假设一族近似密度分布<span class=\"math inline\">\\(\\mathscr{Q}\\)</span>，这是一个隐变量上概率密度的集合</p></li>\r\n<li><p>我们尝试从这族密度中找到最小化和后验分布的KL散度的密度： <span class=\"math display\">\\[\r\nq^*(\\textbf{z}) = \\arg \\min_{q(\\textbf{z})\\in\r\n\\mathscr{Q}}KL(q(\\textbf{z})||p(\\textbf{z}|\\textbf{x}))\r\n\\]</span></p></li>\r\n<li><p>我们用密度族中的优化成员 <span class=\"math inline\">\\(q^∗(\\cdot)\\)</span> 来近似后验</p></li>\r\n</ul>\r\n<p>因此，VI将推理问题转换成了优化问题，并且<span class=\"math inline\">\\(\\mathscr{Q}\\)</span>的范围管理了优化的复杂度。VI背后的关键点之一就是选择能够捕获一个接近后验概率密度的足够灵活的<span class=\"math inline\">\\(\\mathscr{Q}\\)</span>，同时它又足够简单进行高效的优化。</p>\r\n<p>MCMC 和变分推理是解决同一问题的不同方法。</p>\r\n<ul>\r\n<li>MCMC 算法对马尔可夫链进行采样；变分算法解决优化问题。</li>\r\n<li>MCMC 算法用来自链的样本近似后验；变分算法用优化的结果逼近后验。</li>\r\n</ul>\r\n<p>那两种方法什么时候适用呢？</p>\r\n<p>MCMC\r\n方法往往比变分推理的计算密集度更高，但它们也提供了从目标密度产生（渐近）精确样本的保证。</p>\r\n<p>变分推理不享有这样的保证——它只能找到接近目标的密度——但往往比 MCMC\r\n更快。因为它依赖于优化，所以变分推理很容易利用随机优化和分布式优化（尽管一些\r\nMCMC 方法也可以利用这些创新）。</p>\r\n<p>因此，变分推理适用于我们想要<strong>快速探索许多模型的大型数据集和场景</strong>；\r\nMCMC\r\n适用于<strong>较小的数据集和场景，我们很乐意为更精确的样本支付更高的计算成本。</strong></p>\r\n<blockquote>\r\n<p>例如，我们可能会在一个环境中使用 MCMC，我们花了 20\r\n年时间收集一个小而昂贵的数据集，我们确信我们的模型是合适的，并且我们需要精确的推理。</p>\r\n<p>在将文本概率模型拟合到十亿个文本文档时，我们可能会使用变分推理，并且推理将用于为大量用户提供搜索结果。在这种情况下，我们可以使用分布式计算和随机优化来扩展和加速推理，我们可以轻松探索许多不同的数据模型。</p>\r\n</blockquote>\r\n<p>数据集大小不是唯一的考虑因素。另一个因素是<strong>后验分布的几何形状</strong>。</p>\r\n<p>例如，混合模型的后验允许多个模式，每个对应于组件的标签排列。如果模型允许，吉布斯抽样是一种从此类目标分布中抽样的有效方法；它很快专注于其中一种模式。对于不能选择\r\nGibbs 采样的混合模型，变分推理可能比更通用的 MCMC 技术（例如 Hamiltonian\r\nMonte Carlo）表现更好，即使对于小型数据集也是如此。</p>\r\n<p>变分推理和 MCMC 的相对准确性仍然未知。</p>\r\n<p>我们确实知道变分推理通常会低估后验密度的方差。这是其目标函数的结果。但是，根据手头的任务，低估方差可能是可以接受的。几条实证研究表明，变分推理不一定会影响准确性；其他研究侧重于变分推理不足的地方，尤其是后验方差，并试图更接近地匹配\r\nMCMC 的推理。</p>\r\n<h2 id=\"variational-inference-1\">Variational inference</h2>\r\n<p>VI的目标是估计给定观测变量后隐变量的条件密度。</p>\r\n<h3 id=\"the-problem-of-approximate-inference\">The problem of approximate\r\ninference</h3>\r\n<p>令<span class=\"math inline\">\\(\\textbf{x} =\r\nx_{1:n}\\)</span>是观测变量的集合，而<span class=\"math inline\">\\(\\textbf{z} =\r\nz_{1:m}\\)</span>是隐变量的集合，其联合概率是<span class=\"math inline\">\\(p(\\textbf{z},\\textbf{x})\\)</span>。这里我们没有在符号中写出常量，如超参数。</p>\r\n<p>推理问题是给定观测变量来计算隐变量的条件密度，<span class=\"math inline\">\\(p(\\textbf{z}|\\textbf{x})\\)</span>。此条件可用于生成潜在变量的点或区间估计，形成新数据的预测密度等。\r\n<span class=\"math display\">\\[\r\np(\\textbf{z}|\\textbf{x}) =\r\n\\frac{p(\\textbf{z},\\textbf{x})}{p(\\textbf{x})}\r\n\\]</span>\r\n分母包含观察的<strong>边际密度</strong>，也称为<strong>evidence</strong>。我们通过从<strong>联合密度</strong>中<strong>边缘化隐变量</strong>来计算它：\r\n<span class=\"math display\">\\[\r\np(\\textbf{x}) = \\int p(\\textbf{z},\\textbf{x})d\\textbf{z}\r\n\\]</span>\r\n对于许多模型，这种evidence积分在封闭形式中是不可用的，或者需要指数时间来计算。evidence是我们从联合计算条件密度所必须的；这就是在此类模型中进行推理很困难的原因。</p>\r\n<p>请注意，我们假设<strong>所有未知的感兴趣量都表示为隐随机变量</strong>。这包括可能控制所有数据的<strong>参数</strong>，如在贝叶斯模型中发现的，以及对单个数据点“局部”的隐变量。</p>\r\n<h3 id=\"the-evidence-lower-bound\">The evidence lower bound</h3>\r\n<p>在VI中，我们选择一个隐变量上的密度族<span class=\"math inline\">\\(\\mathscr{Q}\\)</span>。每个<span class=\"math inline\">\\(q(\\textbf{z})\\in\\mathscr{Q}\\)</span>都是精确条件密度的候选近似估计。我们的目标是：\r\n<span class=\"math display\">\\[\r\nq^*(\\textbf{z}) = \\arg\\min_{q(\\textbf{z})\\in\r\n\\mathscr{Q}}KL(q(\\textbf{z})||p(\\textbf{z}|\\textbf{x}))\r\n\\]</span> 这个候选密度族的复杂度决定了优化的复杂度。</p>\r\n<p>然而这个指标是不能计算的，因为它需要计算evidence <span class=\"math inline\">\\(\\log p(\\textbf{x})\\)</span>​。 <span class=\"math display\">\\[\r\nK L ( q ( \\textbf{z} ) | | p ( \\textbf{z} | \\textbf{x} ) )\r\n= \\mathbb{E} [ \\log q ( \\textbf{z} ) ] - \\mathbb{E} [ \\log p (\r\n\\textbf{z} | \\textbf{x} ) ]\r\n\\\\\r\n= \\mathbb{E} [ \\log q ( \\textbf{z} ) ] - \\mathbb{E} [ \\log p (\r\n\\textbf{z},\\textbf{x} ) ] + \\log p(\\textbf{x})\r\n\\]</span> 因为我们无法计算 KL，所以我们优化了一个增加一个常量后等效于 KL\r\n的替代目标， <span class=\"math display\">\\[\r\nELBO(q) = \\mathbb{E}[\\log p(\\textbf{z},\\textbf{x})] - \\mathbb{E}[\\log\r\nq(\\textbf{z})]\r\n\\]</span> 这个函数称为evidence lower bound(ELBO)。 ELBO 是负 KL 散度加上\r\nlog p(x)，它是关于 q(z) 的常数。最大化 ELBO 等效于最小化 KL 散度。</p>\r\n<p>检查 ELBO 可以直观地了解最佳变分密度。我们将 ELBO\r\n重写为数据的预期对数似然和先验 <span class=\"math inline\">\\(p(\\textbf{z})\\)</span> 和 <span class=\"math inline\">\\(q(\\textbf{z})\\)</span> 之间的 KL 散度之和 <span class=\"math display\">\\[\r\nELBO(q) = \\mathbb{E}[\\log p(\\textbf{z})]+\\mathbb{E}[\\log\r\np(\\textbf{x}|\\textbf{z})] - \\mathbb{E}[\\log q(\\textbf{z})]\r\n\\\\\r\n=\\mathbb{E}[\\log\r\np(\\textbf{x}|\\textbf{z})]-KL(q(\\textbf{z})||p(\\textbf{z}))\r\n\\]</span> 这个目标会鼓励 <span class=\"math inline\">\\(q(\\textbf{z})\\)</span> 将其质量放在哪个 z\r\n值上？</p>\r\n<ul>\r\n<li>第一项是预期的可能性；它鼓励将它们的质量置于<strong>解释观测数据的隐变量的配置</strong>上的密度。</li>\r\n<li>第二项是变密度与先验的负散度；它鼓励<strong>接近先前的密度</strong>。</li>\r\n</ul>\r\n<p>因此，变分目标反映了<strong>可能性</strong>和<strong>先验</strong>之间的通常平衡。</p>\r\n<p>ELBO 的另一个特性是它降低了（log）evidence的下限，对于任何 <span class=\"math inline\">\\(q(\\textbf{z})，\\log p(\\textbf{x}) \\geq\r\nELBO(q)\\)</span>。这解释了名称的由来。可以由下式看出： <span class=\"math display\">\\[\r\n\\log p(\\textbf{x}) = KL( q ( \\textbf{z} ) | | p ( \\textbf{z} |\r\n\\textbf{x} ) ) + ELBO(q)\r\n\\]</span>\r\n由于KL散度是非负的，所以显然看出ELBO是下限。在关于变分推理的原始文献中，这是通过\r\nJensen 不等式得出的。</p>\r\n<h4 id=\"vi-vs-em\">VI vs EM</h4>\r\n<p><span class=\"math display\">\\[\r\nELBO(q) = \\mathbb{E}[\\log p(\\textbf{z},\\textbf{x})] - \\mathbb{E}[\\log\r\nq(\\textbf{z})]\r\n\\]</span></p>\r\n<p>上式中ELBO的第一项是预期的完全对数似然，由EM算法优化。EM\r\n算法是被设计用于在具有隐变量的模型中寻找<strong>最大似然估计</strong>。它使用了当\r\n<span class=\"math inline\">\\(q(\\textbf{z}) = p(\\textbf{z} |\r\n\\textbf{x})\\)</span> 时 ELBO 等于对数似然 <span class=\"math inline\">\\(\\log\r\np(\\textbf{x})\\)</span>（即对数evidence）这一事实。 EM 在根据 <span class=\"math inline\">\\(p(\\textbf{z} | \\textbf{x})\\)</span>（E\r\n步骤）计算预期的完整对数似然度和针对模型参数（M\r\n步骤）进行优化之间交替进行。与变分推理不同，EM 假设 <span class=\"math inline\">\\(p(\\textbf{z}| \\textbf{x})\\)</span>\r\n下的期望是可计算的，并将其用于其他困难的参数估计问题。与 EM\r\n不同，变分推理不估计固定的模型参数——它通常用于贝叶斯设置，其中经典参数被视为潜在变量。变分推理适用于我们<strong>无法计算潜在变量的确切条件的模型</strong>。</p>\r\n<h3 id=\"the-mean-field-variational-family\">The mean-field variational\r\nfamily</h3>\r\n<p>平均场变分族(mean-field variational\r\nfamily)，其中隐变量是相互独立的，并且每个变量都由变分密度中的不同因素控制。平均场变分族的一个通用成员是：\r\n<span class=\"math display\">\\[\r\nq(\\textbf{z}) = \\prod_{j=1}^m q_j(z_j)\r\n\\]</span> 每个隐变量 <span class=\"math inline\">\\(z_j\\)</span>\r\n由其自身的变分因子——密度 <span class=\"math inline\">\\(q_j(z_j)\\)</span>\r\n控制。在优化中，选择合适的变分因子以最大化ELBO。</p>\r\n<p>我们强调变分族不是观测数据的模型——事实上，数据 x\r\n没有出现在上式中。相反，是 ELBO 和相应的 KL\r\n最小化问题将<strong>拟合的变分密度</strong>与<strong>数据和模型</strong>联系起来的。</p>\r\n<p>请注意，我们没有指定单个变分因子的<strong>参数形式</strong>。原则上，每个都可以采用适合于相应随机变量的任何参数形式。例如，一个连续变量可能有一个高斯因子；分类变量通常具有分类因子。有许多<strong>模型的属性</strong>决定了平均场变分因子\r\n<span class=\"math inline\">\\(q_j(z_j)\\)</span>\r\n的<strong>最佳形式</strong>。</p>\r\n<h4 id=\"expressive-but-cannot-capture-correlation\">Expressive but cannot\r\ncapture correlation</h4>\r\n<p>平均场族是富有表现力的，因为它可以捕获隐变量的任何边际密度。但是，它无法捕获它们之间的相关性。</p>\r\n<figure>\r\n<img src=\"/2023/01/16/VariationalInference/image-20221103192156434.png\" alt=\"image-20221103192156434\">\r\n<figcaption aria-hidden=\"true\">image-20221103192156434</figcaption>\r\n</figure>\r\n<blockquote>\r\n<p>将平均场近似二维高斯后验进行可视化。椭圆显示平均场分解的效果。\r\n（椭圆是高斯分布的 2σ 等值线。）</p>\r\n</blockquote>\r\n<p>我们考虑一个两维高斯分布，这种密度是高度相关的，这定义了它的细长形状。</p>\r\n<p>针对这个后验分布的最佳的平均场变分估计是两个高斯分布的乘积。尽管变分估计和初始的密度有相同的mean,它的协方差结构，通过建设，是解耦的。</p>\r\n<p>此外，近似的边际方差低估了目标的边际方差，这是平均场变分推断的常见效应。我们可以看到，从近似到后验的\r\nKL 散度如下： <span class=\"math display\">\\[\r\nK L ( q ( \\textbf{z} ) | | p ( \\textbf{z} | \\textbf{x} ) )\r\n= \\mathbb{E} [ \\log q ( \\textbf{z} ) ] - \\mathbb{E} [ \\log p (\r\n\\textbf{z} | \\textbf{x} ) ]\r\n\\]</span> 在 <span class=\"math inline\">\\(p\r\n(\\cdot)\\)</span>质量很小的区域，在 <span class=\"math inline\">\\(q\r\n(\\cdot)\\)</span>中放置质量会受到惩罚，反之则会受到较小的惩罚。在这个例子中，为了成功地匹配边际方差,环形\r\n<span class=\"math inline\">\\(q(\\cdot)\\)</span>必须扩展到 <span class=\"math inline\">\\(p (\\cdot)\\)</span>质量很小的区域。</p>\r\n<h3 id=\"coordinate-ascent-mean-field-variational-inference\">Coordinate\r\nascent mean-field variational inference</h3>\r\n<figure>\r\n<img src=\"/2023/01/16/VariationalInference/image-20221103195504051.png\" alt=\"image-20221103195504051\">\r\n<figcaption aria-hidden=\"true\">image-20221103195504051</figcaption>\r\n</figure>\r\n<blockquote>\r\n<p>TODO</p>\r\n<ul>\r\n<li>推导</li>\r\n<li>和吉布斯采样的关系</li>\r\n</ul>\r\n</blockquote>\r\n","tags":["变分"]},{"title":"广义EM的一个特例是VBEM","url":"/2023/01/16/%E5%B9%BF%E4%B9%89EM%E7%9A%84%E4%B8%80%E4%B8%AA%E7%89%B9%E4%BE%8B%E6%98%AFVBEM/","content":"<h1 id=\"em2\">EM2</h1>\r\n<blockquote>\r\n<p>这里是理解EM算法的几重境界的后续部分。对于以EM算法为线，串联重要的数学基础很重要。</p>\r\n<span id=\"more\"></span>\r\n</blockquote>\r\n<h2 id=\"广义em的一个特例是vbem\">广义EM的一个特例是VBEM</h2>\r\n<h3 id=\"lsalatent-semantic-analysis\">LSA(latent semantic analysis)</h3>\r\n<ul>\r\n<li><p>单词向量空间</p>\r\n<blockquote>\r\n<p>给定一个含有n个文本的集合<span class=\"math inline\">\\(D =\r\n{d_1,d_2,...,d_n}\\)</span>，以及在所有文本中出现的m个单词的集合<span class=\"math inline\">\\(W =\r\n{w_1,w_2,...,w_m}\\)</span>。将单词在文本中出现的数据用一个<strong>单词-文本矩阵(word-document\r\nmatrix)</strong>表示，记作X(mxn矩阵)。元素<span class=\"math inline\">\\(x_{ij}\\)</span>表示单词<span class=\"math inline\">\\(w_i\\)</span>在文本<span class=\"math inline\">\\(d_j\\)</span>中出现的频数或权值。</p>\r\n<p>单词种类多，单个文档中单词种类少，所以通常是一个稀疏矩阵。</p>\r\n<p>权值常用<strong>单词频率-逆文本频率(term frequency-inverse document\r\nfrequency)</strong>表示 <span class=\"math display\">\\[\r\nTFIDF_{ij} = \\frac{tf_{ij}}{tf_{·j}}\\log\\frac{df}{df_i},\\quad i =\r\n1,2,...,m;\\quad j = 1,2,...,n\r\n\\]</span> 其中<span class=\"math inline\">\\(tf_{ij}\\)</span>是单词<span class=\"math inline\">\\(w_i\\)</span>出现在文本<span class=\"math inline\">\\(d_j\\)</span>中的频数，<span class=\"math inline\">\\(df_i\\)</span>是所有含有单词<span class=\"math inline\">\\(w_i\\)</span>的文本数，<span class=\"math inline\">\\(df\\)</span>是文本集D全部的文档数。</p>\r\n<p>单词-文本矩阵X每一列代表一个文本，<span class=\"math inline\">\\(X =\r\n[x_1,x_2,...,x_n]\\)</span>即表示本的单词向量。</p>\r\n<p>使用内积/标准化内积（余弦）表示文本相似度。</p>\r\n</blockquote>\r\n<ul>\r\n<li>优点：简单，效率高</li>\r\n<li>缺点：无法解决<strong>一词多义</strong>及<strong>多词一义</strong>问题</li>\r\n</ul></li>\r\n<li><p>话题向量空间</p>\r\n<ul>\r\n<li><p>对一词多义和多词一义问题的解决</p>\r\n<blockquote>\r\n<p>一词多义的词可以对应多个话题，多词一义的词可以对应同一个话题</p>\r\n</blockquote></li>\r\n</ul>\r\n<blockquote>\r\n<p>假设所有文本共含有k个话题。假设每个话题由一个定义在单词集合W上的m维向量表示，称为话题向量。</p>\r\n<p>T是单词-话题矩阵(mxk)，其中<span class=\"math inline\">\\(t_{il}\\)</span>是单词<span class=\"math inline\">\\(w_i\\)</span>在话题<span class=\"math inline\">\\(t_l\\)</span>中的权值，权值越大，单词在该话题中的重要度就越大。</p>\r\n</blockquote>\r\n<blockquote>\r\n<p>单词向量空间中一个文本对应的向量<span class=\"math inline\">\\(x_j\\)</span>投影到T中可以得到话题向量空间中一个向量<span class=\"math inline\">\\(y_j\\)</span>,是一个k维向量。</p>\r\n<p>其中<span class=\"math inline\">\\(y_{lj}\\)</span>是文本<span class=\"math inline\">\\(d_j\\)</span>在话题<span class=\"math inline\">\\(t_l\\)</span>的权值，l=1,2,...,k,权值越大，该话题该文本中的重要程度就越高。</p>\r\n<p>Y称为话题-文本矩阵，<span class=\"math inline\">\\(Y =\r\n[y_1,y_2,...,y_n]\\)</span></p>\r\n</blockquote>\r\n<ul>\r\n<li><p>从单词向量空间到话题向量空间的<strong>线性变换</strong></p>\r\n<blockquote>\r\n<p>单词向量空间的文本向量<span class=\"math inline\">\\(x_j\\)</span>可以通过它在话题空间中的向量<span class=\"math inline\">\\(y_j\\)</span>近似表示，具体地由k个话题向量以<span class=\"math inline\">\\(y_j\\)</span>为系数的线性组合<strong>近似</strong>表示\r\n<span class=\"math display\">\\[\r\nx_j \\approx y_{1j}t_1 + y_{2j}t_2 + ··· + y_{kj}t_k\r\n\\]</span> 即 <span class=\"math inline\">\\(X \\approx\r\nTY\\)</span>(这里之所以是约等于，是因为话题个数k往往小于单词个数，导致其表达能力在单词之下，所以可能造成信息损失)\r\n<span class=\"math display\">\\[\r\nx_j \\Rightarrow y_j\r\n\\]</span> 即将m维的单词向量空间压缩到了k维的话题向量空间。</p>\r\n<figure>\r\n<img src=\"/2023/01/16/%E5%B9%BF%E4%B9%89EM%E7%9A%84%E4%B8%80%E4%B8%AA%E7%89%B9%E4%BE%8B%E6%98%AFVBEM/17-2.png\" alt=\"17-1\">\r\n<figcaption aria-hidden=\"true\">17-1</figcaption>\r\n</figure>\r\n</blockquote></li>\r\n</ul></li>\r\n<li><p>潜在语义分析算法</p>\r\n<ul>\r\n<li><p>矩阵奇异值分解算法</p>\r\n<ul>\r\n<li>截断奇异值分解至k维 <span class=\"math display\">\\[\r\nX \\approx U_k\\Sigma_kV_k^T\r\n\\]</span> 话题空间<span class=\"math inline\">\\(U_k\\)</span>,以及文本在话题空间的表示<span class=\"math inline\">\\((\\Sigma_kV_k^T)\\)</span></li>\r\n</ul></li>\r\n<li><p>非负矩阵分解算法</p>\r\n<blockquote>\r\n<p>若一个矩阵所有元素非负，则称该矩阵为非负矩阵，若X是非负矩阵，则记作<span class=\"math inline\">\\(X \\geq 0\\)</span></p>\r\n<p>给定一个非负矩阵X，找到两个非负矩阵W和H。使得 <span class=\"math display\">\\[\r\nX \\approx WH\r\n\\]</span>\r\nX可以看成基W和系数H的线性组合。非负矩阵分解旨在用较少的基向量，系数向量来表示较大的数据矩阵。</p>\r\n<p>非负矩阵分解有很直观的解释，话题向量和文本向量都非负，对应着“伪概率分布”，向量的线性组合表示局部叠加构成整体。</p>\r\n<p>即，单词向量是总的概率分布，有k个小的话题向量的分布组合而成。</p>\r\n</blockquote>\r\n<ul>\r\n<li><p>算法</p>\r\n<ul>\r\n<li><p>初始化</p>\r\n<blockquote>\r\n<p><span class=\"math inline\">\\(W\\geq\r\n0\\)</span>且对W的每一列数据归一化</p>\r\n<p><span class=\"math inline\">\\(H\\geq 0\\)</span></p>\r\n</blockquote></li>\r\n<li><p>迭代</p>\r\n<blockquote>\r\n<p>对迭代次数由1到t执行一下步骤</p>\r\n<ul>\r\n<li><p>更新W元素 <span class=\"math display\">\\[\r\nW_{il} = W_{il}\\frac{(XH^T)_{il}}{(WHH^T)_{il}},\\quad\r\ni=1,2,...,m;l=1,2,...,k\r\n\\]</span></p></li>\r\n<li><p>更新H元素 <span class=\"math display\">\\[\r\nH_{lj} = H_{lj}\\frac{(W^TX)_{lj}}{(W^TWH)_{lj}},\\quad\r\nl=1,2,...,k;j=1,2,...,n\r\n\\]</span></p></li>\r\n</ul>\r\n</blockquote></li>\r\n</ul></li>\r\n</ul></li>\r\n</ul></li>\r\n</ul>\r\n<h3 id=\"plsaprobabilistic-latent-semantic-analysis\">PLSA(probabilistic\r\nlatent semantic analysis)</h3>\r\n<ul>\r\n<li><p>生成模型</p>\r\n<blockquote>\r\n<p>每个文本拥有自己的话题概率分布<span class=\"math inline\">\\(P(z|d)\\)</span>，每个话题有自己的单词概率分布<span class=\"math inline\">\\(P(w|z)\\)</span>。即一个文本由其话题决定，一个话题由其单词决定</p>\r\n</blockquote>\r\n<ul>\r\n<li><p>生成模型生成文本-单词共现数据的步骤：</p>\r\n<ul>\r\n<li><p>根据P(d)，从文本（指标）集合中随机选取一个文本d，共生成N个文本；针对每个文本执行以下操作。</p></li>\r\n<li><p>在文本d给定条件下，依据P(z|d)从话题集合中随机选取一个话题z,共生成L个话题，这里L是文本长度</p></li>\r\n<li><p>在话题z给定条件下，根据P(w|z)从单词集合中随机选择一个单词w</p>\r\n<blockquote>\r\n<p>w和d是观测变量，z是隐变量</p>\r\n</blockquote></li>\r\n</ul>\r\n<blockquote>\r\n<p>从数据生成过程可知，文本-单词共现数据T的生成概率为所有单词-文本对(w,d)的生成概率的乘积\r\n<span class=\"math display\">\\[\r\nP(T) = \\prod_{(w,d)}P(w,d)^{n(w,d)}\r\n\\]</span> 这里n(w,d)表示(w,d)的出现次数单词-文本对出现的总次数是NxL。\r\n<span class=\"math display\">\\[\r\nP(w,d) = P(d)P(w|d)\r\n\\newline\r\n=P(d)\\sum_{z}P(w,z|d)\r\n\\newline\r\n=P(d)\\sum_{z}P(z|d)P(w|z)\r\n\\]</span>\r\n倒数第二步到最后一步基于一个假设：给定话题z的条件下，单词w和文本d条件独立:即给定d确定z之后，w就可以完全由z决定，而不需要再考虑d这个条件。\r\n<span class=\"math display\">\\[\r\nP(w,z|d) = P(z|d)P(w|d,z)\r\n\\newline\r\n=P(z|d)P(w|z)\r\n\\]</span></p>\r\n</blockquote>\r\n<p>生成模型属于概率有向图模型，可以用有向图表示<img src=\"/2023/01/16/%E5%B9%BF%E4%B9%89EM%E7%9A%84%E4%B8%80%E4%B8%AA%E7%89%B9%E4%BE%8B%E6%98%AFVBEM/image-20220918170426070-16738440550722.png\" alt=\"image-20220918170426070\"></p></li>\r\n</ul></li>\r\n<li><p>共现模型</p>\r\n<blockquote>\r\n<p><span class=\"math display\">\\[\r\nP(w,d) = \\sum_{z\\in Z}P(z)P(w|z)P(d|z)\r\n\\]</span></p>\r\n<p>同样假设话题z给定的情况下，单词w和文本d是条件独立的 <span class=\"math display\">\\[\r\nP(w,d|z) = P(w|z)P(d|z)\r\n\\]</span></p>\r\n</blockquote></li>\r\n</ul>\r\n<p>​ 从公式上而言共现模型和生成模型两者等价。<img src=\"/2023/01/16/%E5%B9%BF%E4%B9%89EM%E7%9A%84%E4%B8%80%E4%B8%AA%E7%89%B9%E4%BE%8B%E6%98%AFVBEM/image-20220918172005928-16738440550711.png\" alt=\"image-20220918172005928\"></p>\r\n<ul>\r\n<li><p>模型性质</p>\r\n<ul>\r\n<li>如果直接定义单词文本的贡献概率P(w,d)，模型参数个数<span class=\"math inline\">\\(O(M·N)\\)</span>。而引入隐变量之后，参数个数成为<span class=\"math inline\">\\(O(M·K+N·K)\\)</span>，其中K是话题数。现实中<span class=\"math inline\">\\(K \\ll\r\nM\\)</span>。所以，模型更加简洁，减少过拟合的可能性。<img src=\"/2023/01/16/%E5%B9%BF%E4%B9%89EM%E7%9A%84%E4%B8%80%E4%B8%AA%E7%89%B9%E4%BE%8B%E6%98%AFVBEM/image-20220918172922802-16738440550723.png\" alt=\"image-20220918172922802\"></li>\r\n</ul></li>\r\n<li><p>模型的几何解释</p>\r\n<blockquote>\r\n<p>单纯形：<a href=\"https://zh.m.wikipedia.org/wiki/几何学\">几何学</a>上，<strong>单纯形</strong>或者<strong>n-单纯形</strong>是和<a href=\"https://zh.m.wikipedia.org/wiki/三角形\">三角形</a>类似的<em>n</em>维<a href=\"https://zh.m.wikipedia.org/wiki/几何形状\">几何体</a>。精确的讲，单纯形是某个n维以上的<a href=\"https://zh.m.wikipedia.org/wiki/欧几里得空间\">欧几里得空间</a>中的（<em>n</em>+1）个<a href=\"https://zh.m.wikipedia.org/wiki/仿射变换\">仿射无关</a>（也就是没有<em>m-1</em>维<a href=\"https://zh.m.wikipedia.org/wiki/平面_(数学)\">平面</a>包含<em>m</em>+1个点；这样的点集被称为处于<a href=\"https://zh.m.wikipedia.org/wiki/一般位置\">一般位置</a>）的<a href=\"https://zh.m.wikipedia.org/wiki/点\">点</a>的集合的<a href=\"https://zh.m.wikipedia.org/wiki/凸包\">凸包</a>。</p>\r\n<p>例如，0-单纯形就是<a href=\"https://zh.m.wikipedia.org/wiki/点\">点</a>，1-单纯形就是<a href=\"https://zh.m.wikipedia.org/wiki/线段\">线段</a>，2-单纯形就是<a href=\"https://zh.m.wikipedia.org/wiki/三角形\">三角形</a>，3-单纯形就是<a href=\"https://zh.m.wikipedia.org/wiki/四面體\">四面体</a>，而4-单纯形是一个<a href=\"https://zh.m.wikipedia.org/wiki/正五胞体\">五胞体</a>（每种情况都包含内部）。</p>\r\n<p><strong>这里的关键是n维空间中的n-1单纯形是有n个顶点</strong></p>\r\n</blockquote>\r\n<p>概率分布<span class=\"math inline\">\\(P(w|d)\\)</span>表示文本d生成单词w的概率 <span class=\"math display\">\\[\r\n\\sum_{i=1}^{M}P(w_i|d) = 1,\\quad 0 \\leq P(w_i|d)\\leq1,i=1,2,...,M\r\n\\]</span> 可以由M维空间的(M-1)单纯形中的点表示。</p>\r\n<blockquote>\r\n<p>为什么可以由M维空间中的M-1单纯形表示呢？</p>\r\n<p>可以递推考虑：</p>\r\n<p>假如有2个参数，则a+b = 1</p>\r\n<p>可以由2维空间的1单纯形进行表示，所有满足条件的参数组合都在这个1单纯形上。</p>\r\n<p>则，推广到M个参数的约束，就可以由M维空间中的M-1单纯形上的点表示这样的参数组合。</p>\r\n</blockquote>\r\n<figure>\r\n<img src=\"/2023/01/16/%E5%B9%BF%E4%B9%89EM%E7%9A%84%E4%B8%80%E4%B8%AA%E7%89%B9%E4%BE%8B%E6%98%AFVBEM/image-20220918211648082-16738440550724.png\" alt=\"image-20220918211648082\">\r\n<figcaption aria-hidden=\"true\">image-20220918211648082</figcaption>\r\n</figure>\r\n<p>PLSA(生成模型)中文本概率分布有如下关系成立： <span class=\"math display\">\\[\r\nP(w|d) = \\sum_{z}P(z|d)P(w|z)\r\n\\]</span> 概率分布<span class=\"math inline\">\\(P(w|z)\\)</span>也存在于M维空间中的(M-1)单纯形中(系数和是1，向量的线性表出在一个面上，则每个向量都在这个面上)。如果有K个话题，则对应(M-1)单纯形中的K个点。以这K个点为顶点，构成一个(K-1)单纯形，称为话题单纯形，是单词单纯形的子单纯形。图中所示是K=3,M=3。当K=2时，参数向量在空间中缩为线段。K=1缩为点。</p>\r\n<p>即参数空间相对变小。而且是维数级别的缩小。</p></li>\r\n<li><p>PLSA与LSA的关系</p>\r\n<ul>\r\n<li><p>对LSA而言，单词-文本矩阵进行奇异值分解得到<span class=\"math inline\">\\(X = U\\Sigma\r\nV^T\\)</span>，其中U和V为正交矩阵，<span class=\"math inline\">\\(\\Sigma\\)</span>为非负降序对角矩阵。<img src=\"/2023/01/16/%E5%B9%BF%E4%B9%89EM%E7%9A%84%E4%B8%80%E4%B8%AA%E7%89%B9%E4%BE%8B%E6%98%AFVBEM/image-20220918213649385-16738440550725.png\" alt=\"image-20220918213649385\"></p></li>\r\n<li><p>对PLSA而言，共现模型也可以表示为三个矩阵乘积的形式： <span class=\"math display\">\\[\r\nX&#39; = U&#39;\\Sigma &#39;V&#39;^T\r\n\\newline\r\nX&#39; = [P(w,d)]_{M\\times N}\r\n\\newline\r\nU&#39;=[P(w|z)]_{M\\times K}\r\n\\newline\r\n\\Sigma&#39; = [P&#39;(z)]_{K\\times K}\r\n\\newline\r\nV&#39; = [P(d|z)]_{N\\times K}\r\n\\]</span></p></li>\r\n<li><p>两组矩阵的区别</p>\r\n<ul>\r\n<li>U’和 V'是非负的，规范化的，表示条件概率分布</li>\r\n<li>U和V是正交的，未必非负，不表示概率分布</li>\r\n</ul></li>\r\n</ul></li>\r\n<li><p>PLSA求解算法</p>\r\n<figure>\r\n<img src=\"/2023/01/16/%E5%B9%BF%E4%B9%89EM%E7%9A%84%E4%B8%80%E4%B8%AA%E7%89%B9%E4%BE%8B%E6%98%AFVBEM/image-20220918230311979-16738440550726.png\" alt=\"image-20220918230311979\">\r\n<figcaption aria-hidden=\"true\">image-20220918230311979</figcaption>\r\n</figure>\r\n<figure>\r\n<img src=\"/2023/01/16/%E5%B9%BF%E4%B9%89EM%E7%9A%84%E4%B8%80%E4%B8%AA%E7%89%B9%E4%BE%8B%E6%98%AFVBEM/image-20220918230411724-16738440550727.png\" alt=\"image-20220918230411724\">\r\n<figcaption aria-hidden=\"true\">image-20220918230411724</figcaption>\r\n</figure>\r\n<figure>\r\n<img src=\"/2023/01/16/%E5%B9%BF%E4%B9%89EM%E7%9A%84%E4%B8%80%E4%B8%AA%E7%89%B9%E4%BE%8B%E6%98%AFVBEM/image-20220918230642299-16738440550729.png\" alt=\"image-20220918230642299\">\r\n<figcaption aria-hidden=\"true\">image-20220918230642299</figcaption>\r\n</figure></li>\r\n</ul>\r\n<h3 id=\"马尔可夫链蒙特卡洛法\">马尔可夫链蒙特卡洛法</h3>\r\n<p>蒙特卡洛方法是从概率模型的随机抽样进行近似数值计算的方法。</p>\r\n<p>马尔可夫链蒙特卡洛法则是以马尔可夫链为概率模型的蒙特卡洛法。构建一个马尔可夫链，使其分布就是要抽样的分布，首先基于该马尔可夫链进行随机游走，产生样本的序列，之后使用该平稳分布的样本进行近似数值计算。</p>\r\n<ul>\r\n<li><p>蒙特卡洛法</p>\r\n<ul>\r\n<li><p>随机抽样</p>\r\n<p>接受-拒绝法一般流程：</p>\r\n<ul>\r\n<li>从[0,1]均匀分布抽样随机数<span class=\"math inline\">\\(u_0\\)</span>，然后计算<span class=\"math inline\">\\(x =\r\nF^{-1}(u_0)\\)</span>得到建议分布的随机样本<span class=\"math inline\">\\(x\\)</span>。(其中<span class=\"math inline\">\\(F(\\cdot)\\)</span>是建议分布的累积分布函数，<span class=\"math inline\">\\(F^{-1}(\\cdot)\\)</span>则是其逆函数)</li>\r\n<li>再从[0,1]均匀分布中抽样随机数<span class=\"math inline\">\\(u_1\\)</span>，和<span class=\"math inline\">\\(A(x) =\r\n\\frac{p(x)}{M\\cdot q(x)}\\)</span>比较，大于则拒绝；小于则接受。</li>\r\n<li>得到抽样<span class=\"math inline\">\\(x\\)</span>。</li>\r\n</ul></li>\r\n<li><p>数学期望估计</p>\r\n<p>对概率密度函数<span class=\"math inline\">\\(p(x)\\)</span>独立抽取n个样本<span class=\"math inline\">\\(x_1,x_2,...,x_n\\)</span>，之后计算函数<span class=\"math inline\">\\(f(x)\\)</span>的样本均值<span class=\"math inline\">\\(\\hat{f}_n\\)</span>: <span class=\"math display\">\\[\r\n\\hat{f}_n = \\frac{1}{n}\\sum_{i=1}^nf(x_i)\r\n\\]</span> 作为数学期望<span class=\"math inline\">\\(E_{p(x)}[f(x)]\\)</span>的估计值。（由大数定律可得到）</p></li>\r\n<li><p>积分计算 <span class=\"math display\">\\[\r\n\\int_{\\mathcal{X}}h(x)dx = \\int_{\\mathcal{X}}g(x)f(x)dx=E_{p(x)}[f(x)]\r\n\\]</span></p></li>\r\n</ul></li>\r\n</ul>\r\n<h3 id=\"马尔可夫链\">马尔可夫链</h3>\r\n<ul>\r\n<li><p>遍历定理</p>\r\n<p>设有马尔可夫链 <span class=\"math inline\">\\(X =\r\n\\{X_0,X_1,...,X_t,...\\}\\)</span>，状态空间为<span class=\"math inline\">\\(S\\)</span>，若马尔可夫链是不可约、非周期且正常返的，则该马尔可夫链有唯一平稳分布<span class=\"math inline\">\\(\\pi =\r\n(\\pi_1,\\pi_2,...)^T\\)</span>，并且转移概率的极限分布是马尔可夫链的平稳分布\r\n<span class=\"math display\">\\[\r\n\\lim_{t\\rightarrow\\infty}P(X_t=i|X_0=j) = \\pi_i,i=1,2,...;j=1,2,...\r\n\\]</span> 若<span class=\"math inline\">\\(f(X)\\)</span>是定义在状态空间上的函数，<span class=\"math inline\">\\(E_{\\pi}[|f(X)|]&lt;\\infty\\)</span>，则 <span class=\"math display\">\\[\r\nP\\{\\hat{f}_t\\rightarrow E_\\pi[f(X)] \\} = 1,t\\rightarrow \\infty\r\n\\\\\r\n\\hat{f}_t = \\frac{1}{t}\\sum_{s=1}^t f(x_s)\r\n\\\\\r\nE_\\pi[f(X)] = \\sum_if(i)\\pi_i\r\n\\]</span>\r\n样本均值是一次次时间步骤产生的，是时间均值。期望是同一时间状态空间不同状态和对应概率的积的和，是空间均值。遍历定理其实是证明了当时间趋于无穷的时候，时间均值就等于空间均值。</p></li>\r\n<li><p>可逆马尔可夫链</p>\r\n<p>设有马尔可夫链<span class=\"math inline\">\\(X =\r\n\\{X_0,X_1,...,X_t,...\\}\\)</span>，状态空间为<span class=\"math inline\">\\(S\\)</span>，转移概率矩阵为<span class=\"math inline\">\\(P\\)</span>，如果状态分布<span class=\"math inline\">\\(\\pi =\r\n(\\pi_1,\\pi_2,\\cdot\\cdot\\cdot)^T\\)</span>，对于任意状态<span class=\"math inline\">\\(i,j\\in S\\)</span>，对任意一个时刻<span class=\"math inline\">\\(t\\)</span>满足: <span class=\"math display\">\\[\r\nP(X_t = i|X_{t-1} = j)\\pi_j = P(X_{t-1}=j|X_t = i)\\pi_i,i,j=1,2,...\r\n\\]</span> 或简写为 <span class=\"math display\">\\[\r\np_{ij}\\pi_j = p_{ji}\\pi_i,i,j=1,2,...\r\n\\]</span> 则称此马尔可夫链<span class=\"math inline\">\\(X\\)</span>为可逆马尔科夫链，上式称为<strong>细致平衡方程</strong>。</p>\r\n<p>直观上，对一个可逆马尔可夫链而言，以该马尔可夫链的平稳分布作为初始分布，进行随机状态转移，无论是面向未来还是面向过去，任何一个时刻的状态分布都是该平稳分布。</p>\r\n<p><strong>定理</strong>：满足细致平衡方程的状态分布<span class=\"math inline\">\\(\\pi\\)</span>就是该马尔可夫链的平稳分布，即 <span class=\"math display\">\\[\r\nP\\pi = \\pi\r\n\\]</span></p></li>\r\n</ul>\r\n<h3 id=\"马尔可夫链蒙特卡罗法\">马尔可夫链蒙特卡罗法</h3>\r\n<ul>\r\n<li><p>基本思想</p>\r\n<ul>\r\n<li><p>目标</p>\r\n<ul>\r\n<li>对一个概率分布进行随机抽样</li>\r\n<li>求函数关于该概率分布的数学期望</li>\r\n</ul></li>\r\n<li><p>手段</p>\r\n<ul>\r\n<li>传统的蒙特卡罗法：接受-拒绝法，重要性抽样法</li>\r\n<li>马尔可夫链蒙特卡罗法：更适合随机变量是多元的，密度函数是非标准形式的，随机变量各分量不独立等情况</li>\r\n</ul></li>\r\n<li><p>Pipeline</p>\r\n<p>假设多元随机变量<span class=\"math inline\">\\(x \\in\r\n\\mathcal{X}\\)</span>，其概率密度函数为<span class=\"math inline\">\\(p(x)\\)</span>，<span class=\"math inline\">\\(f(x)\\)</span>为定义在<span class=\"math inline\">\\(x\\in\r\n\\mathcal{X}\\)</span>上的函数，目标是获得概率分布<span class=\"math inline\">\\(p(x)\\)</span>的样本集合，以及求函数<span class=\"math inline\">\\(f(x)\\)</span>的数学期望<span class=\"math inline\">\\(E_{p(x)}[f(x)]\\)</span>。</p>\r\n<ul>\r\n<li><p>在随机变量<span class=\"math inline\">\\(x\\)</span>的状态空间<span class=\"math inline\">\\(S\\)</span>上构造一个满足遍历定理的马尔可夫链，使其平稳分布为目标分布<span class=\"math inline\">\\(p(x)\\)</span></p>\r\n<blockquote>\r\n<p>连续变量的时候需要定义转移核函数；离散变量的时候需要定义转移矩阵。</p>\r\n<p>一个方法是定义特殊的转移核函数或转移矩阵，构建可逆马尔可夫链。这样可以保证（充分条件：可逆马尔科夫连一定有唯一平稳分布）遍历定理成立。</p>\r\n</blockquote></li>\r\n<li><p>从状态空间某一点<span class=\"math inline\">\\(x_0\\)</span>出发，用构造的马尔可夫链进行随机游走，产生样本序列<span class=\"math inline\">\\(x_0,x_1,...,x_t,...\\)</span></p></li>\r\n<li><p>应用马尔可夫链的遍历定理，确定正整数m和n，(m&lt;n)，得到样本集合<span class=\"math inline\">\\(\\{x_{m+1},x_{m+2},...,x_n\\}\\)</span>，求得函数<span class=\"math inline\">\\(f(x)\\)</span>的遍历均值 <span class=\"math display\">\\[\r\n\\hat{E}f = \\frac{1}{n-m}\\sum_{i=m+1}^nf(x_i)\r\n\\]</span> 就是马尔可夫链蒙特卡洛法的计算公式</p></li>\r\n</ul></li>\r\n</ul></li>\r\n</ul>\r\n<h3 id=\"马尔可夫链蒙特卡罗法与统计学习\">马尔可夫链蒙特卡罗法与统计学习</h3>\r\n<p>在贝叶斯学习中起着重要作用：它可以用于概率模型的学习和推理上。</p>\r\n<p>假设<strong>观测数据</strong>由随机变量<span class=\"math inline\">\\(y\\in\r\n\\mathcal{Y}\\)</span>表示，<strong>模型</strong>由随机变量<span class=\"math inline\">\\(x\\in\r\n\\mathcal{X}\\)</span>表示，贝叶斯学习通过贝叶斯定理计算给定数据条件下的模型的后验概率，并选择后验概率最大的模型。</p>\r\n<p>后验概率 <span class=\"math display\">\\[\r\np(x|y) =\r\n\\frac{p(x)p(y|x)}{\\int_{\\mathcal{X}}p(y|x&#39;)p(x&#39;)dx&#39;}\r\n\\]</span> 贝叶斯经常需要三种积分运算：</p>\r\n<ul>\r\n<li><p>规范化：后验概率计算中需要的： <span class=\"math display\">\\[\r\n\\int_{\\mathcal{X}}p(y|x&#39;)p(x&#39;)dx&#39;\r\n\\]</span></p></li>\r\n<li><p>边缘化：如果有隐变量<span class=\"math inline\">\\(z\\in\\mathcal{Z}\\)</span>，后验概率的计算需要边缘化计算\r\n<span class=\"math display\">\\[\r\np(x|y)=\\int_{\\mathcal{Z}}p(x,z|y)dz\r\n\\]</span></p></li>\r\n<li><p>数学期望：如果有一个函数<span class=\"math inline\">\\(f(x)\\)</span>，可以计算该函数关于后验概率分布的数学期望\r\n<span class=\"math display\">\\[\r\nE_{p(x)}[f(x)] = \\int_{\\mathcal{X}}f(x)p(x|y)dx\r\n\\]</span></p></li>\r\n</ul>\r\n<h3 id=\"metropolis-hastings算法\">Metropolis-Hastings算法</h3>\r\n<p>假设要抽样的概率分布为<span class=\"math inline\">\\(p(x)\\)</span>。Metropolis-Hastings算法采用的转移核为<span class=\"math inline\">\\(p(x,x&#39;)\\)</span>的马尔可夫链： <span class=\"math display\">\\[\r\np(x,x&#39;) = q(x,x&#39;)\\alpha(x,x&#39;)\r\n\\]</span> 其中<span class=\"math inline\">\\(q(x,x&#39;)\\)</span>和<span class=\"math inline\">\\(\\alpha(x,x&#39;)\\)</span>分别被称为建议分布和接受分布。</p>\r\n<ul>\r\n<li><p>建议分布</p>\r\n<ul>\r\n<li>是另一个马尔可夫链的转移核</li>\r\n<li>是不可约的，即概率值恒不为0</li>\r\n<li>是一个容易抽样的分布</li>\r\n<li>常见的两种形式\r\n<ul>\r\n<li>假设建议分布是对称的：q(x,x') = q(x',x)\r\n<ul>\r\n<li>比如选择条件概率分布<span class=\"math inline\">\\(p(x&#39;|x)\\)</span>，<strong>定义为</strong>以<span class=\"math inline\">\\(x\\)</span>为均值的多元正态分布，协方差矩阵是常数矩阵（在知道当前的x后，就可以从正态分布中抽取下一步的x'）</li>\r\n</ul></li>\r\n<li>独立抽样。假设q(x,x')与当前状态x无关，即q(x,x') = q(x')</li>\r\n</ul></li>\r\n</ul></li>\r\n<li><p>接受分布</p>\r\n<ul>\r\n<li>公式定义 <span class=\"math display\">\\[\r\n\\alpha(x,x&#39;) = min\\{1,\\frac{p(x&#39;)q(x&#39;,x)}{p(x)q(x,x&#39;)}\\}\r\n\\]</span></li>\r\n</ul></li>\r\n<li><p>满条件分布</p>\r\n<ul>\r\n<li><p>定义</p>\r\n<p>多元联合概率分布<span class=\"math inline\">\\(p(x)=p(x_1,x_2,...,x_k)\\)</span>，其中<span class=\"math inline\">\\(x =\r\n(x_1,x_2,...,x_k)^T\\)</span>是k维随机变量。如果条件概率分布<span class=\"math inline\">\\(p(x_I|x_{-I})\\)</span>中所有k个变量全部出现，其中<span class=\"math inline\">\\(x_I = \\{x_i,i\\in I\\},x_{-I}=\\{x_i,i\\notin\r\nI\\},I\\subset K = \\{1,2,...,k\\}\\)</span></p></li>\r\n<li><p>性质</p>\r\n<p>对任意的<span class=\"math inline\">\\(x\\in\r\n\\mathcal{X}\\)</span>和任意的<span class=\"math inline\">\\(I \\subset\r\nK\\)</span>，有 <span class=\"math display\">\\[\r\np(x_I|x_{-I}) = \\frac{p(x)}{\\int p(x)dx_I} \\propto p(x)\r\n\\]</span> 而且对任意的<span class=\"math inline\">\\(x,x&#39;\\in\r\n\\mathcal{X}\\)</span>和任意的<span class=\"math inline\">\\(I \\subset\r\nK\\)</span>，有 <span class=\"math display\">\\[\r\n\\frac{p(x_I&#39;|x_{-I}&#39;)}{p(x_I|x_{-I})} = \\frac{p(x&#39;)}{p(x)}\r\n\\]</span>\r\n利用这个性质，可以通过满条件分布概率的比来计算联合概率的比，计算更加简单。</p></li>\r\n</ul></li>\r\n<li><p>Pipeline</p>\r\n<ul>\r\n<li>输入：抽样的目标分布的密度函数<span class=\"math inline\">\\(p(x)\\)</span>，函数<span class=\"math inline\">\\(f(x)\\)</span></li>\r\n<li>输出：<span class=\"math inline\">\\(p(x)\\)</span>的随机样本<span class=\"math inline\">\\(x_{m+1},x_{m+2},...,x_n\\)</span>，函数样本均值<span class=\"math inline\">\\(f_{mn}\\)</span></li>\r\n<li>参数：收敛步数m，迭代步数n</li>\r\n</ul>\r\n<p>(1)任选初始值<span class=\"math inline\">\\(x_0\\)</span></p>\r\n<p>(2)对i=1,2,...,n循环执行</p>\r\n<ul>\r\n<li>设状态<span class=\"math inline\">\\(x_{i-1} =\r\nx\\)</span>，按照<strong>建议分布</strong><span class=\"math inline\">\\(q(x,x&#39;)\\)</span>随机抽取一个候选状态<span class=\"math inline\">\\(x&#39;\\)</span></li>\r\n<li>计算接受概率<span class=\"math inline\">\\(\\alpha(x,x&#39;)\\)</span></li>\r\n<li>从区间(0,1)中按均匀分布抽取<span class=\"math inline\">\\(u\\)</span>\r\n<ul>\r\n<li>若<span class=\"math inline\">\\(u\\leq\r\n\\alpha(x,x&#39;)\\)</span>，则状态<span class=\"math inline\">\\(x_i=x&#39;\\)</span></li>\r\n<li>否则，<span class=\"math inline\">\\(x_i = x\\)</span></li>\r\n</ul></li>\r\n</ul>\r\n<p>得到<strong>样本集合</strong>并计算<strong>函数样本均值</strong>并返回·</p></li>\r\n<li><p>单分量Metropolis-Hastings算法</p>\r\n<p>在Metropolis-Hastings算法中，通常需要对多元变量分布进行抽样，有时对多元变量分布进行抽样是困难的。</p>\r\n<p>可以对多元变量的每一变量的条件分布一次进行抽样，从而实现对整个多元变量的一次抽样。</p></li>\r\n</ul>\r\n<h4 id=\"lda\">LDA</h4>\r\n<blockquote>\r\n<p>系列回顾：PLSA</p>\r\n<ul>\r\n<li><p>生成模型 or 判别模型</p>\r\n<p>预测模型的公式是<span class=\"math inline\">\\(P(y|x)\\)</span>。即给定输入，输出给定输入的概率分布。</p>\r\n<ul>\r\n<li><p>生成方法原理上由数据学习<strong>联合概率分布</strong><span class=\"math inline\">\\(P(X,Y)\\)</span>，然后求出条件概率分布作为预测的模型，即生成模型：\r\n<span class=\"math display\">\\[\r\nP(Y|X) = \\frac{P(X,Y)}{P(X)}\r\n\\]</span>\r\n之所以称之为生成方法，是因为模型表示了给定输入X产生输出Y的生成关系。</p>\r\n<p>以HMM(隐马尔可夫模型)为例：</p>\r\n<p><strong>构成</strong></p>\r\n<ul>\r\n<li><span class=\"math inline\">\\(\\pi\\)</span>:出事状态概率向量</li>\r\n<li><span class=\"math inline\">\\(A\\)</span>:状态转移概率矩阵</li>\r\n<li><span class=\"math inline\">\\(B\\)</span>:观测概率矩阵</li>\r\n</ul>\r\n<p><span class=\"math display\">\\[\r\n\\lambda = (A,B,\\pi)\r\n\\]</span></p>\r\n<p><strong>基本假设</strong></p>\r\n<ul>\r\n<li>齐次马尔可夫假设：t时刻的状态只依赖前一时刻的状态</li>\r\n<li>观测独立性假设：任意时刻的观测只依赖该时刻马尔可夫链的状态</li>\r\n</ul>\r\n<p><strong>三个基本问题</strong></p>\r\n<ul>\r\n<li>概率计算问题：给定模型<span class=\"math inline\">\\(\\lambda =\r\n(A,B,\\pi)\\)</span>和观测序列<span class=\"math inline\">\\(O=(o_1,o_2,...,o_T)\\)</span>,计算在模型<span class=\"math inline\">\\(\\lambda\\)</span>下序列<span class=\"math inline\">\\(O\\)</span>出现的概率</li>\r\n<li>学习问题：已知观测序列，估计模型<span class=\"math inline\">\\(\\lambda\r\n= (A,B,\\pi)\\)</span>的参数，使得该模型下观测序列出现的概率<span class=\"math inline\">\\(P(O|\\lambda)\\)</span>最大，即极大似然估计方法估计参数</li>\r\n<li>预测问题（解码问题）：已知模型和观测序列，求解最有可能的对应的状态序列<span class=\"math inline\">\\(P(I|O)\\)</span></li>\r\n</ul>\r\n<p><strong>概率计算</strong></p>\r\n<ul>\r\n<li><p>直接计算</p>\r\n<p>状态序列<span class=\"math inline\">\\(I\\)</span>的概率是：<span class=\"math inline\">\\(P(I|\\lambda)\\)</span></p>\r\n<p>对固定的状态序列<span class=\"math inline\">\\(I\\)</span>，观测序列<span class=\"math inline\">\\(O\\)</span>的概率为：<span class=\"math inline\">\\(P(O|I,\\lambda)\\)</span></p>\r\n<p>则<span class=\"math inline\">\\(O\\)</span>和<span class=\"math inline\">\\(I\\)</span>同时出现的联合概率为：<span class=\"math inline\">\\(P(O,I|\\lambda) =\r\nP(O|I,\\lambda)P(I|\\lambda)\\)</span></p>\r\n<p>对所有可能的状态序列<span class=\"math inline\">\\(I\\)</span>求和，得到观测序列<span class=\"math inline\">\\(O\\)</span>的概率为： <span class=\"math display\">\\[\r\nP(O|\\lambda) = \\sum_{I}P(O|I,\\lambda)P(I|\\lambda)\r\n\\]</span>\r\n由此，可以很明显地看出，HMM拟合观测序列（x）和状态序列（y）的联合分布。</p>\r\n<p>学习时，我们数据只有观测序列（x），最大化<span class=\"math inline\">\\(P(O|\\lambda)\\)</span>然后使用EM算法进行参数估计。</p>\r\n<p>得到模型之后，状态序列y由马尔科夫模型生成，然后生成观测序列x。</p></li>\r\n<li><p>前向算法</p></li>\r\n<li><p>后向算法</p></li>\r\n</ul>\r\n<p><strong>学习算法</strong></p>\r\n<ul>\r\n<li>监督学习方法：直接大数定律，极大似然估计</li>\r\n<li>Baum-Welch算法（EM算法）\r\n<ul>\r\n<li>写出Q函数</li>\r\n<li>利用拉格朗日乘子法求解极值点</li>\r\n</ul></li>\r\n</ul>\r\n<p><strong>预测算法</strong></p>\r\n<ul>\r\n<li><p>近似算法</p>\r\n<ul>\r\n<li>每个时刻t选择最可能出现的状态，然后得到状态序列，作为预测结果</li>\r\n<li>缺点：不能保证预测的状态序列整体是最优的</li>\r\n</ul></li>\r\n<li><p>维特比算法</p>\r\n<ul>\r\n<li>动态规划求解最优路径</li>\r\n</ul>\r\n<blockquote>\r\n<p>https://www.zhihu.com/question/20136144</p>\r\n</blockquote></li>\r\n</ul></li>\r\n<li><p>判别模型：直接学习<span class=\"math inline\">\\(P(Y|X)\\)</span>作为预测的模型。典型的判别模型包括k近邻法，感知机，逻辑斯蒂回归模型，最大熵模型，支持向量机，提升方法和条件随机场等。</p></li>\r\n</ul>\r\n<p>因此，PLSA模型显然也是一种生成模型。文本-单词共现数据T的生成概率为：<span class=\"math inline\">\\(\\prod \\limits_{(w,d)}P(w,d)^{n(w,d)}\\)</span></p>\r\n<p>而每个单词对<span class=\"math inline\">\\((w,d)\\)</span>的生成概率由以下公式决定： <span class=\"math display\">\\[\r\nP(w,d) = P(d)P(w|d)=P(d)\\sum_{z}P(w,z|d)=P(d)\\sum_{z}P(z|d)P(w|z)\r\n\\]</span> 共现模型的公式更能体现其作为生成模型的特点： <span class=\"math display\">\\[\r\nP(w,d) =\\sum_zP(x,d,z) = \\sum_zP(z)P(w,d|z)\r\n\\]</span>\r\n该学习过程已经需要计算X（文本-单词对）和Y（主题）的联合概率分布了。在推断过程中根据X计算Y的过程中就可以使用生成模型的方式计算P(Y|X)（即对文档进行主题聚类）。也可以根据学习到的模型生成文本-单词对。</p></li>\r\n</ul>\r\n</blockquote>\r\n<p>那么有了PLSA后，为何会提出LDA呢？</p>\r\n<p>pLSA中，主题的概率分布P(c|d)和词在主题下的概率分布P(w|c)既然是概率分布，那么就必须要有样本进行统计才能得到这些概率分布。更具体的讲，主题模型就是为了做这个事情的，训练已获得的数据样本，得到这些参数，那么一个pLSA模型便得到了，但是这个时候问题就来了：这些参数是建立在训练样本上得到的。这是个大问题啊！你怎么能确保新加入的数据同样符合这些参数呢？你能不能别这么草率鲁莽？但是频率学派就有这么任性，他们认为参数是存在并且是确定的，\r\n只是我们未知而已，并且正是因为未知，我们才去训练pLSA的，训练之后得到的参数同样适合于新加入的数据，因为他们相信参数是确定的，既然适合于训练数据，那么也同样适合于新加入的数据了。</p>\r\n<p>​\r\n但是真实情况却不是这样，尤其是训练样本量比较少的情况下的时候，这个时候首先就不符合大数定律的条件（这里插一句大数定律和中心极限定律，在无数次<a href=\"https://www.zhihu.com/search?q=独立同分布&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A38969800%7D\">独立同分布</a>的随机事件中，事件的频率趋于一个稳定的概率值，这是大数定律；而同样的无数次独立同分布的随机事件中，事件的分布趋近于一个稳定的正态分布，而这个正太分布的期望值正是大数定律里面的概率值。所以，<a href=\"https://www.zhihu.com/search?q=中心极限定理&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A38969800%7D\">中心极限定理</a>比大数定律揭示的现象更深刻，同时成立的条件当然也要相对来说苛刻一些。\r\n非数学系出身，不对请直接喷），所以频率并不能很好的近似于概率，所以得到的参数肯定不好。我们都知道，概率的获取必须以拥有大量可重复性实验为前提，但是这里的主题模型训练显然并不能在每个场景下都有大量的训练数据。所以，当训练数据量偏小的时候，pLSA就无可避免的陷入了过拟合的泥潭里了。为了解决这个问题，LDA给这些参数都加入了一个先验知识，就是当数据量小的时候，我人为的给你一些专家性的指导，你这个参数应该这样不应该那样。比如你要统计一个地区的<a href=\"https://www.zhihu.com/search?q=人口年龄分布&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A38969800%7D\">人口年龄分布</a>，假如你手上有的训练数据是一所大学的<a href=\"https://www.zhihu.com/search?q=人口数据&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A38969800%7D\">人口数据</a>，统计出来的结果肯定是年轻人占比绝大多数，这个时候你训练出来的模型肯定是有问题的，但是我现在加入一些先验知识进去，专家认为这个地区中老年人口怎么占比这么少？不行，我得给你修正修正，这个时候得到的结果就会好很多。所以LDA相比pLSA就优在这里，它对这些参数加入了一些先验的分布进去。</p>\r\n<p>但是，当训练样本量足够大，pLSA的效果是可以等同于LDA的，因为过拟合的原因就是训练数据量太少，当把数据量提上去之后，过拟合现象会有明显的改观。</p>\r\n<blockquote>\r\n<p>作者：weizier\r\n链接：https://www.zhihu.com/question/23642556/answer/38969800</p>\r\n</blockquote>\r\n<p>当隐分布没有限制的时候，广义EM算法就是EM算法。但是当隐分布本身有限制的时候，比如存在一个先验分布的限制。（以PLSA→LDA为例）</p>\r\n<figure>\r\n<img src=\"/2023/01/16/%E5%B9%BF%E4%B9%89EM%E7%9A%84%E4%B8%80%E4%B8%AA%E7%89%B9%E4%BE%8B%E6%98%AFVBEM/v2-a868b18611aa43395d534f94ebcd8221_720w.jpg\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure>\r\n<p>由于加入了先验分布的限制，导致广义EM算法中的第一步E无法达到无限制最优，所以KL距离无法为0<img src=\"/2023/01/16/%E5%B9%BF%E4%B9%89EM%E7%9A%84%E4%B8%80%E4%B8%AA%E7%89%B9%E4%BE%8B%E6%98%AFVBEM/v2-2c76444860b4fbfad17b8978c18f1cda_720w.jpg\" alt=\"img\"></p>\r\n<p>也求是说，当隐分布没有限制的时候，<span class=\"math inline\">\\(p(x|y,\\theta^{(t)})\\)</span>是可以计算的，KL距离可以为0，即在E步的时候可以取到最优解。但是当隐分布有限制，比如LDA中引入了先验分布，这时候<span class=\"math inline\">\\(p(x|y,\\theta^{(t)})\\)</span>难以计算，就需要用变分的思想，在限制的范围内找到最接近的<span class=\"math inline\">\\(q(x)\\)</span>来代替<span class=\"math inline\">\\(p(x|y,\\theta^{(t)})\\)</span>的计算，近似在E步中实现最优。</p>\r\n<p>EM算法的目标从证据转变为了证据下限（ELBO),且假设变分分布<span class=\"math inline\">\\(q(x)\\)</span>的分量相互独立。则自由能被改写为：<img src=\"/2023/01/16/%E5%B9%BF%E4%B9%89EM%E7%9A%84%E4%B8%80%E4%B8%AA%E7%89%B9%E4%BE%8B%E6%98%AFVBEM/v2-f9f283ed1adc265c6b54b5b7fd1dcbc1_720w.jpg\" alt=\"img\"></p>\r\n<p>这样就得到VBEM算法：</p>\r\n<figure>\r\n<img src=\"/2023/01/16/%E5%B9%BF%E4%B9%89EM%E7%9A%84%E4%B8%80%E4%B8%AA%E7%89%B9%E4%BE%8B%E6%98%AFVBEM/v2-d3ae8be76ce5f2e4f5bc7b85a926b0de_720w.jpg\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure>\r\n<figure>\r\n<img src=\"/2023/01/16/%E5%B9%BF%E4%B9%89EM%E7%9A%84%E4%B8%80%E4%B8%AA%E7%89%B9%E4%BE%8B%E6%98%AFVBEM/v2-d2fe741002136a34980ee5188a5f4a74_720w.jpg\" alt=\"img\">\r\n<figcaption aria-hidden=\"true\">img</figcaption>\r\n</figure>\r\n<p>当然，对于某个难以计算的分布进行估计除了使用变分的思想，还可以使用吉布斯采样进行估计。下面也会进一步讨论。</p>\r\n<h2 id=\"广义em的另一个特例是ws算法\">广义EM的另一个特例是WS算法</h2>\r\n<h3 id=\"bp算法\">BP算法</h3>\r\n<h3 id=\"wake-sleep算法\">Wake-Sleep算法</h3>\r\n<h2 id=\"广义em的再一个特例是gibbs-sampling\">广义EM的再一个特例是Gibbs\r\nSampling</h2>\r\n<h3 id=\"gibbs-sampling\">Gibbs Sampling</h3>\r\n<p>吉布斯抽样是单分量Metropolis-Hastings算法的特殊情况。前者抽样会在样本点见移动，但可能会由于被拒绝而有停留；后者会在样本点之间持续移动。前者适合于满条件概率分布不容易计算的情况，使用更容易抽样的条件概率分布做建议分布。后者适合于满条件概率分布容易计算的情况。</p>\r\n<p>定义建议分布是当前变量<span class=\"math inline\">\\(x_j,j=1,2,...,k\\)</span>的满条件概率分布 <span class=\"math display\">\\[\r\nq(x,x&#39;) = p(x&#39;_j|x_{-j})\r\n\\]</span>\r\n这时，接受概率很容易得到是1。因此，转移核函数也是满条件概率分布： <span class=\"math display\">\\[\r\np(x,x&#39;) = p(x&#39;_j|x_{-j})\r\n\\]</span></p>\r\n<ul>\r\n<li><p>输入：目标概率分布的密度函数<span class=\"math inline\">\\(p(x)\\)</span>，函数<span class=\"math inline\">\\(f(x)\\)</span></p></li>\r\n<li><p>输出：<span class=\"math inline\">\\(p(x)\\)</span>的随机样本<span class=\"math inline\">\\(x_{m+1},x_{m+2},...,x_n\\)</span>，函数样本均值<span class=\"math inline\">\\(f_{mn}\\)</span></p></li>\r\n<li><p>参数：收敛步数m，迭代步数n</p></li>\r\n<li><p>具体步骤：</p>\r\n<figure>\r\n<img src=\"/2023/01/16/%E5%B9%BF%E4%B9%89EM%E7%9A%84%E4%B8%80%E4%B8%AA%E7%89%B9%E4%BE%8B%E6%98%AFVBEM/image-20221026163129064-16738440550728.png\" alt=\"image-20221026163129064\">\r\n<figcaption aria-hidden=\"true\">image-20221026163129064</figcaption>\r\n</figure></li>\r\n<li><p>抽样计算</p>\r\n<p>可以利用概率分布的性质提高抽样的效率。</p>\r\n<figure>\r\n<img src=\"/2023/01/16/%E5%B9%BF%E4%B9%89EM%E7%9A%84%E4%B8%80%E4%B8%AA%E7%89%B9%E4%BE%8B%E6%98%AFVBEM/image-20221026164342352-167384405507310.png\" alt=\"image-20221026164342352\">\r\n<figcaption aria-hidden=\"true\">image-20221026164342352</figcaption>\r\n</figure>\r\n<p>也就是说，直接算看上去是和所有变量都有关的（都出现在了式子左边部分）但是实际上通过式子右边部分计算，只需要用到部分变量即可进行计算，更加简单。</p></li>\r\n</ul>\r\n<h2 id=\"ws算法是vae和gan组合的简化版\">WS算法是VAE和GAN组合的简化版</h2>\r\n<h3 id=\"vae\">VAE</h3>\r\n<h3 id=\"gan\">GAN</h3>\r\n<h2 id=\"kl距离的统一\">KL距离的统一</h2>\r\n<h3 id=\"熵\">熵</h3>\r\n<blockquote>\r\n<p>信息论中，熵是接受的每条消息中<strong>包含的信息的平均值</strong>。又被称为信息熵、信源熵、平均自信息量。可以被理解为<strong>不确定性的度量</strong>，熵越大，信源的分布越随机</p>\r\n<p>1948年，由克劳德·爱尔伍德·香农将热力学中的熵引入信息论，所以也叫做：香农熵</p>\r\n</blockquote>\r\n<p>在信息论中，<strong>熵</strong>是信息量的期望。 <span class=\"math display\">\\[\r\nH(x) =\\sum_{x}p(x)h(x) - \\sum_{x}p(x)\\log p(x)\r\n\\]</span> 其中，<span class=\"math inline\">\\(h(x)\\)</span>代表的是信息量的大小。考虑这个函数需要满足条件：<strong>概率大的事件对应小的存储空间，说人话，就是成反比</strong></p>\r\n<blockquote>\r\n<p>可以考虑信息的编码（哈夫曼树和哈夫曼编码），最优的结构就是将常出现的信息放在数的根部，然后不常见的不断分支下去。概率越大，信息量越小这个结论本身就是使编码最优得到的天然结果。</p>\r\n</blockquote>\r\n<p>脑海中第一反应出来满足这个条件最直观是<strong>反比例函数</strong>，<span class=\"math inline\">\\(\\frac{1}{p(x)}\\)</span>。</p>\r\n<p>之后我们发现这个公式中有个除法非常讨厌，我们想着去掉它，脑海中第一反应出来的满足这个条件的一定是<strong>取对数</strong>，至于为什么取对数，那说道就很多，取对数是指数的<strong>逆操作</strong>，</p>\r\n<ul>\r\n<li>对数操作可以让原本不符合正态分布的模型符合正态分布，比如随着模型自变量的增加，因变量的方差也增大的模型取对数后会更加稳定</li>\r\n<li>取对数操作可以rescale（原谅我，这里思前想后还是感觉一个英文单词更加生动）其实本质来说都是因为第一点。说人话版本，人不喜欢乘法，对数可以把乘法变加法</li>\r\n<li>考虑独立事件<span class=\"math inline\">\\(x\\)</span>和<span class=\"math inline\">\\(y\\)</span>，显然有<span class=\"math inline\">\\(h(x,y) = h(x) +\r\nh(y)\\)</span>。对数形式也能很好满足这一要求。</li>\r\n</ul>\r\n<h3 id=\"交叉熵和相对熵\">交叉熵和相对熵</h3>\r\n<p>因为是我们用2bit模式存储，为了计算方便，这里取a = 2</p>\r\n<p>先计算刚刚有关天气问题 P=[p1,p2,p3,p4]\r\n：【阴、晴、雨、雪】的信息熵，假设我们对天气的概率一无所知，那么四种天气的发生概率为<strong>等概率（服从平均分布）</strong>，即\r\nP=[1/4,1/4,1/4,1/4]，带入公式得到 H(P)=2，存储信息需要的空间 Sn=2n</p>\r\n<p>继续思考，假设我们考虑天气的城市是一个地处中国南方雨季的城市，那么阴天和雨天的概率从<strong>经验角度（先验概率）</strong>来看大于晴天雪天，把这种分布<strong>记为</strong>\r\nQ=[1/4,1/8,1/2,1/8]，带入公式信息熵\r\nH(Q)=1.75H(Q)=1.75，存储信息需要的空间 Sn=1.75n</p>\r\n<p>直观的来考虑上面不同的两种情况，明显当<strong>事件的不确定性变小</strong>时候，我们可以改变存储策略（00\r\n雨天 01\r\n阴天），再通过编码，节省存储空间。信息熵的大小<strong>就是用来度量这个不确定大小的</strong>。</p>\r\n<figure>\r\n<img src=\"/2023/01/16/%E5%B9%BF%E4%B9%89EM%E7%9A%84%E4%B8%80%E4%B8%AA%E7%89%B9%E4%BE%8B%E6%98%AFVBEM/image-20230116155936453.png\" alt=\"image-20230116155936453\">\r\n<figcaption aria-hidden=\"true\">image-20230116155936453</figcaption>\r\n</figure>\r\n<p>假定在<strong>确定性更大的概率分布</strong>的情况下，用<strong>更不确定的存储策略</strong>来计算。则得到<strong>交叉熵</strong>：\r\n<span class=\"math display\">\\[\r\nH(p,q) = -\\sum_xp(x)\\log q(x)\r\n\\]</span> 计算交叉熵得到：</p>\r\n<figure>\r\n<img src=\"/2023/01/16/%E5%B9%BF%E4%B9%89EM%E7%9A%84%E4%B8%80%E4%B8%AA%E7%89%B9%E4%BE%8B%E6%98%AFVBEM/image-20230116160702485.png\" alt=\"image-20230116160702485\">\r\n<figcaption aria-hidden=\"true\">image-20230116160702485</figcaption>\r\n</figure>\r\n<p>上表直观的展现的<strong>交叉熵</strong>的数值表现，PQZW依次<strong>不确定性越来越低</strong>，极端情况的W不确定性为0，即<strong>是确定的</strong></p>\r\n<p><strong>交叉熵，用来高衡量在给定的真实分布下，使用非真实分布指定的策略消除系统的不确定性所需要付出努力的大小</strong></p>\r\n<p>总的来说，<strong>我们的目的是：让熵尽可能小，即存储空间小（消除系统的不确定的努力小）</strong></p>\r\n<p>通过上表我们发现一个规律，为了让熵小，解决方案是：<strong>是用确定性更大的概率乘以确定性更小的存储因子</strong>，比如不确定性越大的概率分布，如P概率分布，其信息熵越大；<strong>基于同一真实（确定性）分布的情况下</strong>，套用不确定性更大的存储因子，如P的存储因子，得出的交叉熵越大</p>\r\n<p>在机器学习中，即用测试结果集（样本结果集）的概率乘以训练出来的结果集存储因子，而在不断的训练过程中，我们要做的就是通过不断调整参数，降低这个值，<strong>使得模型更加的稳定，不确定性越来越小</strong>，即突出需要表征的数值的特点（白话文也就是分类的效果更好）</p>\r\n<p>有了信息熵和交叉熵后，<strong>相对熵是用来衡量两个概率分布之间的差异</strong>，记为\r\nD(P||Q)=H(P,Q)−H(P)，也称之为<strong>KL散度</strong> <span class=\"math display\">\\[\r\nD_{KL}(p||q) = \\sum_xp(x)\\log \\frac{p(x)}{q(x)}\r\n\\]</span> 当\r\nP(x)=Q(x)的时候，该值为0，深度学习过程也是一个降低该值的过程，<strong>该值越低，训练出来的概率Q越接近样本集概率P，即越准确</strong>。</p>\r\n"}]