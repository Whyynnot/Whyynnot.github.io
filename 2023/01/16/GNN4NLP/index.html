<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="Natural Language Processing Conversation Modeling on Reddit using a Graph-Structured LSTM. TACL 2018. paper Vicky Zayats, Mari Ostendorf.  Learning Graphical State Transitions. ICLR 2017. paper Daniel">
<meta property="og:type" content="article">
<meta property="og:title" content="GNN4NLP">
<meta property="og:url" content="http://example.com/2023/01/16/GNN4NLP/index.html">
<meta property="og:site_name" content="Whyynnot">
<meta property="og:description" content="Natural Language Processing Conversation Modeling on Reddit using a Graph-Structured LSTM. TACL 2018. paper Vicky Zayats, Mari Ostendorf.  Learning Graphical State Transitions. ICLR 2017. paper Daniel">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221009143334468-167384297065818.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/v2-1e430ba69797cf8242d2885f99b350bd_b-167384301715396.jpg">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221009160825337-167384297065819.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/v2-4d7bf05128e4ba55df16a9fd61913a8e_b-167384304011199.jpg">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/v2-1bfc2035f4d2a95d0af6ea0288cd8680_b-1673843042737102.jpg">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/1042406-20161227180625461-1385841797-1673843046978105.jpg">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221009232432760-167384297065920.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221009232455005-167384297065921.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221010154638572-167384297065922.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221010164908386-167384297065923.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221010170329407-167384297066024.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221010143237008-167384297066025.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221011132228704-167384297066126.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221011144220776-167384297066127.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221011144520825-167384297066128.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221011144758370-167384297066129.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221011150648322-167384297066130.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221011150702333-167384297066131.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221011161701031-167384297066132.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221011195033784-167384297066133.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221011195716309-167384297066134.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221011200344382-167384297066135.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221011200414926-167384297066236.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221011200817824-167384297066237.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221011221409703-167384297066238.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221011235610664-167384297066239.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221012145843950-167384297066240.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221012154439952-167384297066341.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221012165608095-167384297066342.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221012171329066-167384297066343.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221012194112426-167384297066344.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221012194657161-167384297066345.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221012200836949-167384297066346.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221012201009369-167384297066347.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221012220208418-167384297066348.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221012231514618-167384297066449.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221012231716198-167384297066450.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221012235316597-167384297066451.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221012235530298-167384297066452.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221012235630483-167384297066453.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221013000115875-167384297066454.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221013000529937-167384297066455.png">
<meta property="og:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221013000931882-167384297066456.png">
<meta property="article:published_time" content="2023-01-16T04:02:00.000Z">
<meta property="article:modified_time" content="2023-01-16T04:24:48.017Z">
<meta property="article:author" content="Haoran Li">
<meta property="article:tag" content="GNN in NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/01/16/GNN4NLP/image-20221009143334468-167384297065818.png">

<link rel="canonical" href="http://example.com/2023/01/16/GNN4NLP/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>GNN4NLP | Whyynnot</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Whyynnot</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/16/GNN4NLP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Haoran Li">
      <meta itemprop="description" content="Blog of Whyynnot">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Whyynnot">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          GNN4NLP
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-01-16 12:02:00 / 修改时间：12:24:48" itemprop="dateCreated datePublished" datetime="2023-01-16T12:02:00+08:00">2023-01-16</time>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2023/01/16/GNN4NLP/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2023/01/16/GNN4NLP/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Natural-Language-Processing"><a href="#Natural-Language-Processing" class="headerlink" title="Natural Language Processing"></a>Natural Language Processing</h1><ol>
<li><p><strong>Conversation Modeling on Reddit using a Graph-Structured LSTM.</strong> TACL 2018. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1704.02080">paper</a></p>
<p><em>Vicky Zayats, Mari Ostendorf.</em></p>
</li>
<li><p><strong>Learning Graphical State Transitions.</strong> ICLR 2017. <a target="_blank" rel="noopener" href="https://openreview.net/forum?id=HJ0NvFzxl">paper</a></p>
<p><em>Daniel D. Johnson.</em></p>
</li>
<li><p><strong>Multiple Events Extraction via Attention-based Graph Information Aggregation.</strong> EMNLP 2018. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1809.09078.pdf">paper</a></p>
<p><em>Xiao Liu, Zhunchen Luo, Heyan Huang.</em></p>
</li>
<li><p><strong>Recurrent Relational Networks.</strong> NeurIPS 2018. <a target="_blank" rel="noopener" href="http://papers.nips.cc/paper/7597-recurrent-relational-networks.pdf">paper</a></p>
<p><em>Rasmus Palm, Ulrich Paquet, Ole Winther.</em></p>
</li>
<li><p><strong>Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks.</strong> ACL 2015. <a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/P15-1150">paper</a></p>
<p><em>Kai Sheng Tai, Richard Socher, Christopher D. Manning.</em></p>
</li>
<li><p><strong>Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling.</strong> EMNLP 2017. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.04826">paper</a></p>
<p><em>Diego Marcheggiani, Ivan Titov.</em></p>
</li>
<li><p><strong>Graph Convolutional Networks with Argument-Aware Pooling for Event Detection.</strong> AAAI 2018. <a target="_blank" rel="noopener" href="http://ix.cs.uoregon.edu/~thien/pubs/graphConv.pdf">paper</a></p>
<p><em>Thien Huu Nguyen, Ralph Grishman.</em></p>
</li>
<li><p><strong>Exploiting Semantics in Neural Machine Translation with Graph Convolutional Networks.</strong> NAACL 2018. <a target="_blank" rel="noopener" href="http://www.aclweb.org/anthology/N18-2078">paper</a></p>
<p><em>Diego Marcheggiani, Joost Bastings, Ivan Titov.</em></p>
</li>
<li><p><strong>Exploring Graph-structured Passage Representation for Multi-hop Reading Comprehension with Graph Neural Networks.</strong> 2018. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.02040">paper</a></p>
<p><em>Linfeng Song, Zhiguo Wang, Mo Yu, Yue Zhang, Radu Florian, Daniel Gildea.</em></p>
</li>
<li><p><strong>Graph Convolution over Pruned Dependency Trees Improves Relation Extraction.</strong> EMNLP 2018. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.10185">paper</a></p>
<p><em>Yuhao Zhang, Peng Qi, Christopher D. Manning.</em></p>
</li>
<li><p><strong>N-ary relation extraction using graph state LSTM.</strong> EMNLP 18. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.09101">paper</a></p>
<p><em>Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea.</em></p>
</li>
<li><p><strong>A Graph-to-Sequence Model for AMR-to-Text Generation.</strong> ACL 2018. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1805.02473">paper</a></p>
<p><em>Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea.</em></p>
</li>
<li><p><strong>Graph-to-Sequence Learning using Gated Graph Neural Networks.</strong> ACL 2018. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1806.09835.pdf">paper</a></p>
<p><em>Daniel Beck, Gholamreza Haffari, Trevor Cohn.</em></p>
</li>
<li><p><strong>Cross-Sentence N-ary Relation Extraction with Graph LSTMs.</strong> TACL. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1708.03743">paper</a></p>
<p><em>Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, Wen-tau Yih.</em></p>
</li>
<li><p><strong>Sentence-State LSTM for Text Representation.</strong> ACL 2018. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1805.02474">paper</a></p>
<p><em>Yue Zhang, Qi Liu, Linfeng Song.</em></p>
</li>
<li><p><strong>End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures.</strong> ACL 2016. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1601.00770">paper</a></p>
<p><em>Makoto Miwa, Mohit Bansal.</em></p>
</li>
<li><p><strong>Graph Convolutional Encoders for Syntax-aware Neural Machine Translation.</strong> EMNLP 2017. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1704.04675">paper</a></p>
<p><em>Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, Khalil Sima’an.</em></p>
</li>
<li><p><strong>Semi-supervised User Geolocation via Graph Convolutional Networks.</strong> ACL 2018. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1804.08049.pdf">paper</a></p>
<p><em>Afshin Rahimi, Trevor Cohn, Timothy Baldwin.</em></p>
</li>
<li><p><strong>Modeling Semantics with Gated Graph Neural Networks for Knowledge Base Question Answering.</strong> COLING 2018. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1808.04126.pdf">paper</a></p>
<p><em>Daniil Sorokin, Iryna Gurevych.</em></p>
</li>
<li><p><strong>Graph Convolutional Networks for Text Classification.</strong> AAAI 2019. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1809.05679.pdf">paper</a></p>
<p><em>Liang Yao, Chengsheng Mao, Yuan Luo.</em></p>
</li>
</ol>
<details open style="box-sizing: border-box; display: block; margin-top: 0px; margin-bottom: 16px;"><summary style="box-sizing: border-box; display: list-item; cursor: pointer;">more</summary><ol start="21" dir="auto" style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 16px;"><li style="box-sizing: border-box;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Constructing Narrative Event Evolutionary Graph for Script Event Prediction.</strong><span>&nbsp;</span>IJCAI 2018.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/pdf/1805.05081.pdf" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Zhongyang Li, Xiao Ding, Ting Liu.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Incorporating Syntactic and Semantic Information in Word Embeddings using Graph Convolutional Networks.</strong><span>&nbsp;</span>ACL 2019.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/pdf/1809.04283" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Shikhar Vashishth, Manik Bhandari, Prateek Yadav, Piyush Rai, Chiranjib Bhattacharyya, Partha Talukdar</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">PaperRobot: Incremental Draft Generation of Scientific Ideas.</strong><span>&nbsp;</span>ACL 2019.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/pdf/1905.07870" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Qingyun Wang, Lifu Huang, Zhiying Jiang, Kevin Knight, Heng Ji, Mohit Bansal, Yi Luan.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Inter-sentence Relation Extraction with Document-level Graph Convolutional Neural Network.</strong><span>&nbsp;</span>ACL 2019.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/pdf/1906.04684" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Sunil Kumar Sahu, Fenia Christopoulou, Makoto Miwa, Sophia Ananiadou.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Textbook Question Answering with Multi-modal Context Graph Understanding and Self-supervised Open-set Comprehension.</strong><span>&nbsp;</span>ACL 2019.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/pdf/1811.00232" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Daesik Kim, Seonhoon Kim, Nojun Kwak.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs.</strong><span>&nbsp;</span>ACL 2019.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/pdf/1905.07374" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Ming Tu, Guangtao Wang, Jing Huang, Yun Tang, Xiaodong He, Bowen Zhou.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Dynamically Fused Graph Network for Multi-hop Reasoning.</strong><span>&nbsp;</span>ACL 2019.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/pdf/1905.06933" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Yunxuan Xiao, Yanru Qu, Lin Qiu, Hao Zhou, Lei Li, Weinan Zhang, Yong Yu.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Cognitive Graph for Multi-Hop Reading Comprehension at Scale.</strong><span>&nbsp;</span>ACL 2019.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/pdf/1905.05460" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, Jie Tang.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Joint Type Inference on Entities and Relations via Graph Convolutional Networks.</strong><span>&nbsp;</span>ACL 2019.<span>&nbsp;</span><a target="_blank" href="http://www.czsun.site/publications/joint_entrel_gcn.pdf" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Changzhi Sun, Yeyun Gong, Yuanbin Wu, Ming Gong, Daxing Jiang, Man Lan, Shiliang Sun1, Nan Duan.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Attention Guided Graph Convolutional Networks for Relation Extraction.</strong><span>&nbsp;</span>ACL 2019.<span>&nbsp;</span><a target="_blank" href="http://www.statnlp.org/wp-content/uploads/2019/06/Attention_Guided_Graph_Convolutional_Networks_for_Relation_Extraction.pdf" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Zhijiang Guo, Yan Zhang, Wei Lu.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">GraphRel: Modeling Text as Relational Graphs for Joint Entity and Relation Extraction.</strong><span>&nbsp;</span>ACL 2019.<span>&nbsp;</span><a target="_blank" href="https://tsujuifu.github.io/pubs/acl19_graph-rel.pdf" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Tsu-Jui Fu, Peng-Hsuan Li, Wei-Yun Ma.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Graph Neural Networks with Generated Parameters for Relation Extraction.</strong><span>&nbsp;</span>ACL 2019.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/pdf/1902.00756" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Hao Zhu, Yankai Lin, Zhiyuan Liu, Jie Fu, Tat-seng Chua, Maosong Sun.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Generating Logical Forms from Graph Representations of Text and Entities.</strong><span>&nbsp;</span>ACL 2019.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/pdf/1905.08407" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Peter Shaw, Philip Massey, Angelica Chen, Francesco Piccinno, Yasemin Altun.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Matching Article Pairs with Graphical Decomposition and Convolutions.</strong><span>&nbsp;</span>ACL 2019.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/pdf/1802.07459" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Bang Liu, Di Niu, Haojie Wei, Jinghong Lin, Yancheng He, Kunfeng Lai, Yu Xu.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Representing Schema Structure with Graph Neural Networks for Text-to-SQL Parsing.</strong><span>&nbsp;</span>ACL 2019.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/pdf/1905.06241" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Ben Bogin, Matt Gardner, Jonathan Berant.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Coherent Comment Generation for Chinese Articles with a Graph-to-Sequence Model.</strong><span>&nbsp;</span>ACL 2019.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/pdf/1906.01231" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Wei Li, Jingjing Xu, Yancheng He, Shengli Yan, Yunfang Wu, Xu sun.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">GEAR: Graph-based Evidence Aggregating and Reasoning for Fact Verification.</strong><span>&nbsp;</span>ACL 2019.<span>&nbsp;</span><a target="_blank" href="https://www.aclweb.org/anthology/P19-1085" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Jie Zhou, Xu Han, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, Maosong Sun.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Look Again at the Syntax: Relational Graph Convolutional Network for Gendered Ambiguous Pronoun Resolution.</strong><span>&nbsp;</span>ACL 2019.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/pdf/1905.08868.pdf" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Yinchuan Xu, Junlin Yang.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Structured Neural Summarization.</strong><span>&nbsp;</span>ICLR 2019.<span>&nbsp;</span><a target="_blank" href="https://openreview.net/pdf?id=H1ersoRqtm" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Patrick Fernandes, Miltiadis Allamanis, Marc Brockschmidt.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Long-tail Relation Extraction via Knowledge Graph Embeddings and Graph Convolution Networks.</strong><span>&nbsp;</span>NAACL 2019.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/pdf/1903.01306.pdf" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Ningyu Zhang, Shumin Deng, Zhanlin Sun, Guanying Wang, Xi Chen, Wei Zhang, Huajun Chen.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Text Generation from Knowledge Graphs with Graph Transformers.</strong><span>&nbsp;</span>NAACL 2019.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/pdf/1904.02342.pdf" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, Hannaneh Hajishirzi.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Question Answering by Reasoning Across Documents with Graph Convolutional Networks.</strong><span>&nbsp;</span>NAACL 2019.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/pdf/1808.09920.pdf" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Nicola De Cao, Wilker Aziz, Ivan Titov.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">BAG: Bi-directional Attention Entity Graph Convolutional Network for Multi-hop Reasoning Question Answering.</strong><span>&nbsp;</span>NAACL 2019.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/pdf/1904.04969.pdf" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Yu Cao, Meng Fang, Dacheng Tao.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">GraphIE: A Graph-Based Framework for Information Extraction.</strong><span>&nbsp;</span>NAACL 2019.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/pdf/1810.13083.pdf" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Yujie Qian, Enrico Santus, Zhijing Jin, Jiang Guo, Regina Barzilay.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Graph Convolution for Multimodal Information Extraction from Visually Rich Documents.</strong><span>&nbsp;</span>NAACL 2019.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/pdf/1903.11279.pdf" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Xiaojing Liu, Feiyu Gao, Qiong Zhang, Huasha Zhao.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Structural Neural Encoders for AMR-to-text Generation.</strong><span>&nbsp;</span>NAACL 2019.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/pdf/1903.11410.pdf" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Marco Damonte, Shay B. Cohen.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Abusive Language Detection with Graph Convolutional Networks.</strong><span>&nbsp;</span>NAACL 2019.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/pdf/1904.04073.pdf" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Pushkar Mishra, Marco Del Tredici, Helen Yannakoudakis, Ekaterina Shutova.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Learning Graph Pooling and Hybrid Convolutional Operations for Text Representations.</strong><span>&nbsp;</span>WWW 2019.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/pdf/1901.06965.pdf" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Hongyang Gao, Yongjun Chen, Shuiwang Ji.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Graph­‐based Transformer with Cross-candidate Verification for Semantic Parsing.</strong><span>&nbsp;</span>AAAI 2020.<span>&nbsp;</span><a target="_blank" rel="noopener" href="https://github.com/thunlp/GNNPapers/blob/master" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Bo Shao, Yeyun Gong, Weizhen Qi, Guihong Cao, Jianshu Ji, Xiaola Lin.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Efficient Multi-Person Pose Estimation with Provable Guarantees.</strong><span>&nbsp;</span>AAAI 2020.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/abs/1711.07794" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Shaofei Wang, Konrad Paul Kording, Julian Yarkony.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Graph Transformer for Graph-to-Sequence Learning.</strong><span>&nbsp;</span>AAAI 2020.<span>&nbsp;</span><a target="_blank" href="https://arxiv.org/abs/1911.07470" rel="nofollow noopener" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Deng Cai, Wai Lam.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Multi-­‐label Patent Categorization with Non-­‐local Attention-­‐based Graph Convolutional Network.</strong><span>&nbsp;</span>AAAI 2020.<span>&nbsp;</span><a target="_blank" rel="noopener" href="https://github.com/thunlp/GNNPapers/blob/master" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Pingjie Tang, Meng Jiang, Bryan (Ning) Xia, Jed Pitera, Jeff Welser, Nitesh Chawla.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Multi-task Learning for Metaphor Detection with Graph Convolutional Neural Networks and Word Sense Disambiguation.</strong><span>&nbsp;</span>AAAI 2020.<span>&nbsp;</span><a target="_blank" rel="noopener" href="https://github.com/thunlp/GNNPapers/blob/master" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Duong Minh Le, My Thai and Thien Huu Nguyen.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">Schema-Guided Multi-Domain Dialogue State Tracking with Graph Attention Neural Networks.</strong><span>&nbsp;</span>AAAI 2020.<span>&nbsp;</span><a target="_blank" rel="noopener" href="https://github.com/thunlp/GNNPapers/blob/master" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Lu Chen, Boer Lv, Chi Wang, Su Zhu, Bowen Tan, Kai Yu.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">GraphER: Token-Centric Entity Resolution with Graph Convolutional Neural Networks.</strong><span>&nbsp;</span>AAAI 2020.<span>&nbsp;</span><a target="_blank" rel="noopener" href="https://github.com/thunlp/GNNPapers/blob/master" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Bing Li, Wei Wang, Yifang Sun, Linhan Zhang, Muhammad Asif Ali, Yi Wang.</em></p></li><li style="box-sizing: border-box; margin-top: 0.25em;"><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><strong style="box-sizing: border-box; font-weight: 600;">CFGNN:Cross Flow Graph Neural Networks for Question Answering on Complex Tables.</strong><span>&nbsp;</span>AAAI 2020.<span>&nbsp;</span><a target="_blank" rel="noopener" href="https://github.com/thunlp/GNNPapers/blob/master" style="box-sizing: border-box; background-color: transparent; color: var(--color-accent-fg); text-decoration: none;">paper</a></p><p dir="auto" style="box-sizing: border-box; margin-top: 16px; margin-bottom: 16px;"><em style="box-sizing: border-box;">Xuanyu Zhang.</em></p></li></ol></details>

<h1 id="Graph-Neural-Networks-for-Natural-Language-Processing-A-Survey"><a href="#Graph-Neural-Networks-for-Natural-Language-Processing-A-Survey" class="headerlink" title="Graph Neural Networks for Natural Language Processing: A Survey"></a>Graph Neural Networks for Natural Language Processing: A Survey</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li><p>There is a rich variety of NLP problems that can be best expressed with a graph structure.</p>
<ul>
<li>the <strong>sentence structure information</strong> in text sequence(i.e. syntactic parsing trees like dependency and constituency parsing trees) can be exploited to augment original sequence data by incorporating the task-specific knowledge.</li>
<li>the <strong>semantic information</strong> in sequence data (i.e. semantic parsing graphs like Abstract Meaning Representation graphs and Information Extraction graphs) can be leveraged to enhance original sequence data as well.</li>
</ul>
</li>
<li><p>Therefore, these graph-structured data can encode complicated pairwise relationships between entity tokens for learning more informative representations.</p>
</li>
<li><p>Deep learning techniques that were disruptive for Euclidean data (e.g, images) or sequence data (e.g, text) are not immediately applicable to graph-structured data, due to the complexity of graph data such as irregular structure and varying size of node neighbors.</p>
</li>
<li><p>As a result, this gap has driven a tide in research for deep learning on graphs, especially in development of graph neural networks (GNNs)</p>
<ul>
<li>classification tasks</li>
<li>generation tasks</li>
</ul>
</li>
<li><p>Despite the successes these existing research has achieved, deep learning on graphs for NLP still encounters many challenges, namely,</p>
<ul>
<li><p>Automatically transforming original text sequence data into highly graph-structured data.</p>
<blockquote>
<p>Automatic graph construction from the text sequence to utilize the underlying structural information is a crucial step in utilizing graph neural networks for NLP problems.</p>
</blockquote>
</li>
<li><p>Properly determining graph representation learning techniques.</p>
<blockquote>
<p>It is critical to come up with specially-designed GNNs to learn the unique characteristics of different graph-structures data such as undirected, directed, multi-relational and heterogeneous graphs.</p>
</blockquote>
</li>
<li><p>Effectively modeling complex data.</p>
<blockquote>
<p>Such challenge is important since many NLP tasks involve learning the mapping between the graph-based inputs and other highly structured output data such as sequences, trees, as well as graph data with multi-types in both nodes and edges.</p>
</blockquote>
</li>
</ul>
</li>
<li><p>The taxonomy, which systematically organizes GNNs for NLP along three axes: graph construction, graph representation learning, encoder-decoder models, and the applications.<img src="/2023/01/16/GNN4NLP/image-20221009143334468-167384297065818.png" alt="image-20221009143334468"></p>
</li>
</ul>
<h2 id="Graph-Based-Algorithms-for-Natural-Language-Processing"><a href="#Graph-Based-Algorithms-for-Natural-Language-Processing" class="headerlink" title="Graph Based Algorithms for Natural Language Processing"></a>Graph Based Algorithms for Natural Language Processing</h2><h4 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h4><h6 id="省流"><a href="#省流" class="headerlink" title="省流"></a>省流</h6><blockquote>
<p>本节我们从图的视角审视NLP的问题，并且简单地介绍了许多相对传统的基于图解决NLP问题的方法</p>
</blockquote>
<h3 id="Natural-Language-Processing-A-Graph-Perspective"><a href="#Natural-Language-Processing-A-Graph-Perspective" class="headerlink" title="Natural Language Processing: A Graph Perspective"></a>Natural Language Processing: A Graph Perspective</h3><h4 id="Three-ways-to-representing-natural-language"><a href="#Three-ways-to-representing-natural-language" class="headerlink" title="Three ways to representing natural language"></a>Three ways to representing natural language</h4><ul>
<li><p>a bag of tokens</p>
<blockquote>
<p>This view of natural language completely ignores the specific <strong>positions</strong> of tokens appearing in text, and only considers <strong>how many times a unique token appears in text</strong>. If one randomly shuffles a given text, the meaning of the text does not change at all from this perspective.</p>
</blockquote>
<ul>
<li><p>topic modeling</p>
<blockquote>
<p>It aims to model each input text as a mixture of topics where each topic can be further modeled as a mixture of words.</p>
</blockquote>
</li>
</ul>
</li>
<li><p>a sequence of tokens</p>
<blockquote>
<p>This is how human beings normally speak and write natural language.</p>
<p>this view of natural language is able to capture richer information of text, e.g., which two tokens are <strong>consecutive</strong> and how many times a word pair <strong>co-occurs</strong> in local context.</p>
</blockquote>
<ul>
<li>linear-chain CRF</li>
<li>word2vec</li>
</ul>
</li>
<li><p>a graph</p>
<blockquote>
<p>While it is probably most apparent to regard text as sequential data, in the NLP community, there is a long history of representing text as various kinds of graphs.</p>
<p>Common graph representations of text or world knowledge include dependency graphs, constituency graphs, AMR graphs, IE graphs, lexical networks, and knowledge graphs.</p>
<p>Besides, one can also construct a text graph containing <strong>multiple hierarchies of elements</strong> such as document, passage, sentence and word. </p>
<p>In comparison with the above two perspectives, this view of natural language is able to <strong>capture richer relationships</strong> among text elements.</p>
</blockquote>
</li>
</ul>
<h3 id="Graph-Based-Methods-for-Natural-Language-Processing"><a href="#Graph-Based-Methods-for-Natural-Language-Processing" class="headerlink" title="Graph Based Methods for Natural Language Processing"></a>Graph Based Methods for Natural Language Processing</h3><h6 id="省流-1"><a href="#省流-1" class="headerlink" title="省流"></a>省流</h6><blockquote>
<p>介绍各种已经成功应用于NLP的经典的基于图的算法</p>
<ul>
<li>首先介绍各种算法</li>
<li>然后讨论它们和GNNs的联系</li>
</ul>
</blockquote>
<ul>
<li><p>Random Walk Algorithms</p>
<h5 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h5><blockquote>
<ul>
<li>是一组基于图的算法，可以在图中产生随机的路径</li>
<li>从任意节点出发，根据特定的转移概率重复地随机选择邻居节点并转移</li>
<li>在随机游走中所有经过的节点就构成了一条随机路径</li>
<li>随机游走收敛后，可以得到一个图中所有节点的平稳分布<ul>
<li>可以通过概率分数排序来选择图中结构重要性最高的节点，</li>
<li>可以通过计算两个随机游走分布之间的相似性衡量两个图的相关性</li>
</ul>
</li>
</ul>
</blockquote>
<ul>
<li><p>TextRank: Bringing Order into Texts</p>
<h4 id="例子详解"><a href="#例子详解" class="headerlink" title="例子详解"></a>例子详解</h4><blockquote>
<p><strong>基本思想</strong>：利用图形递归绘制全局信息，从而决定顶点的重要程度</p>
<ul>
<li><p>text2graph</p>
<p>为了使基于图的排序算法可以应用于自然语言文本，需要将文本表示成图，并将具有语义关系的单词或其他文本实体用边相连</p>
</li>
<li><p>pipeline</p>
<ul>
<li>确定最佳定义手头任务的<strong>文本单位</strong>，并将其作为顶点添加到图形中</li>
<li>识别连接这些文本单元的<strong>关系</strong>，并使用这些关系在图中的顶点之间绘制边。 边可以是有向的或无向的，加权的或未加权的</li>
<li>迭代基于图的排序算法，直到<strong>收敛</strong></li>
<li>根据最终得分对顶点进行<strong>排序</strong>。 使用附加到每个顶点的值进行排名 / 选择决策</li>
</ul>
</li>
<li><p>核心算法</p>
<ul>
<li>通过随机游走来生成随机路径，并按照以下公式更新节点，直到收敛到稳定分布，然后排序即可<script type="math/tex; mode=display">
W S\left(V_i\right)=(1-d)+d * \sum_{V_j \in \operatorname{In}\left(V_i\right)} \frac{w_{j i}}{\sum_{V_k \in O u t\left(V_j\right)}{w_{j k}}} W S\left(V_j\right)</script></li>
</ul>
</li>
<li><p>关键词抽取</p>
<ul>
<li>将通过一定规则提取的一系列<strong>词</strong>加入节点，词之间的关系（任何关系都可以）作为边。</li>
<li>在本任务中，选择<strong>共现关系</strong>作为关系的标准。</li>
<li>具体而言，两个节点如果出现在一个<strong>窗口</strong>内，那么这两个词之间有一条边。窗口从 2 到 N（最大值）</li>
<li>在构建图形（无向未加权图）之后，将与每个顶点相关联的分数设置为初始值 1，并且根据<strong>排序算法</strong>在图上运行多次迭代，直到它<strong>收敛</strong> - 通常为 20- 30 次迭代，阈值为 0.0001</li>
<li>获得了图中每个顶点的<strong>最终分数</strong>，顶点按其分数的相反顺序进行排序，并保留排名中的前 <strong>top T</strong> 顶点进行后期处理</li>
<li>在后期处理期间，所有词汇单位被选中作为 TextRank 算法的潜在关键词在文本中标记，并显示相邻关键字的序列被折叠成多字关键字</li>
<li>效果：<img src="/2023/01/16/GNN4NLP/v2-1e430ba69797cf8242d2885f99b350bd_b-167384301715396.jpg" alt="img"></li>
</ul>
</li>
<li><p>句子提取</p>
<ul>
<li><p>节点是text中的每个句子，需要过滤(通过词性和长度来过滤)</p>
</li>
<li><p>边的权重是句子之间的Similarity(使用单词重叠度来衡量)</p>
<script type="math/tex; mode=display">
\operatorname{Similarity}\left(S_i, S_j\right)=\frac{\left|\left\{w_k \mid w_k \in S_i \& w_k \in S_j\right\}\right|}{\log \left(\left|S_i\right|\right)+\log \left(\left|S_j\right|\right)}</script></li>
<li><p>效果：<img src="/2023/01/16/GNN4NLP/image-20221009160825337-167384297065819.png" alt="image-20221009160825337"></p>
</li>
</ul>
</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>Graph Clustering Algorithms</p>
<ul>
<li><p>Spectral clustering(谱聚类)</p>
<h5 id="相关知识定义"><a href="#相关知识定义" class="headerlink" title="相关知识定义"></a>相关知识定义</h5><blockquote>
<p><strong>无向权重图</strong></p>
<p>图$G$通常表示为$G(V,E)$。其中$V$即为我们数据集里面所有的点$(v_1,v_2,…,v_n)$。</p>
<p>对$V$中的任意两个点$v<em>i$和$v_j$，可以有边连接，也可以没有。我们定义权重$w</em>{ij}$为点$v<em>i,v_j$之间的权重。由于我们是无向图，所以$w</em>{ij} = w<em>{ji}$。其中，$w</em>{ij}&gt;0$等价于两个点之间有边。$w_{ij}=0$代表没有边。</p>
<p>对图中任意一个点$v_i$,它的度$d_i$定义为和它连接的所有边的权重之和</p>
<script type="math/tex; mode=display">
d_i = \sum_{j=1}^{n}w_{ij}</script><p>利用每个点度的定义，我们得到一个nxn的矩阵$D$,它是一个对角矩阵，只有主对角线有值，对应第i行的第i个点的度数，定义如下：</p>
<script type="math/tex; mode=display">
\mathbf{D}=\left(\begin{array}{ccc}
d_1 & \ldots & \ldots \\
\ldots & d_2 & \ldots \\
\vdots & \vdots & \ddots \\
\ldots & \ldots & d_n
\end{array}\right)</script><p>利用所有店的权重值，我们可以得到图的邻接矩阵$W$,它也是一个nxn矩阵，第i行的第j个值对应我们的权重$w_{ij}$</p>
<p>此外，对点集$V$的一个子集$A \subset V$,我们定义：</p>
<script type="math/tex; mode=display">
|A|:=子集A中点的个数
\newline
vol(A):=\sum_{i \in A}d_i</script><p><strong>相似矩阵</strong></p>
<p>获取邻接矩阵的基本思想：距离远的两个点边权值较低，而距离近的两个点之间边权值较高</p>
<p>构建方法：通过样本点距离度量的相似矩阵S来获得邻接矩阵W</p>
<p>1、$\epsilon$-临近法</p>
<p>设置了一个阈值$\epsilon$,使用欧氏距离$s<em>ij$度量任意两点$x_i$和$x_j$之间的距离。即相似矩阵的$s</em>{ij} = \left|x_i-x_j\right|_2^2$,然后定义$W$元素如下：</p>
<script type="math/tex; mode=display">
w_{i j}= \begin{cases}0 & s_{i j}>\epsilon \\ \epsilon & s_{i j} \leq \epsilon\end{cases}</script><p>距离度量不够精准，一般不使用。</p>
<p>2、K临近法</p>
<p>KNN(K-Nearest Neighbor)算法:计算当前点和其他点的距离，并找到最近的K个点。这K个点中哪个种类数目最多，则将当前点判定为该种类。</p>
<p>如果使用KNN的思想，我们自然地得到只有样本距离最近的K个点之间的$w_{ij}&gt;0$。但是这样得到的$W$是不对称的，为此，我们修改为如下两种形式：</p>
<script type="math/tex; mode=display">
w_{ij}=w_{ji}= \begin{cases} 0& {x_i \notin KNN(x_j) \;and \;x_j \notin KNN(x_i)}\\ exp(-\frac{||x_i-x_j||_2^2}{2\sigma^2})& {x_i \in KNN(x_j)\; or\; x_j \in KNN(x_i}) \end{cases}
\newline
or
\newline
w_{ij}=w_{ji}= \begin{cases} 0& {x_i \notin KNN(x_j) \;or\;x_j \notin KNN(x_i)}\\ exp(-\frac{||x_i-x_j||_2^2}{2\sigma^2})& {x_i \in KNN(x_j)\; and \; x_j \in KNN(x_i}) \end{cases}</script><p>3、全连接法（最普遍）</p>
<p>所有的点之间的权重都大于0。可以选择不同的核函数来定义边权重，比如多项式核函数，高斯核函数和Sigmoid核函数。最常用的是高斯核函数RBF，此时相似矩阵和邻接矩阵相同：</p>
<script type="math/tex; mode=display">
w_{ij}=s_{ij}=exp(-\frac{||x_i-x_j||_2^2}{2\sigma^2})</script><p><strong>拉普拉斯矩阵</strong></p>
<script type="math/tex; mode=display">
L = D-W</script><p>拉普拉斯矩阵=度矩阵 - 邻接矩阵</p>
<p>好的性质：</p>
<ul>
<li><p>对称矩阵（对称矩阵只差是对称矩阵）</p>
</li>
<li><p>由于拉普拉斯矩阵是对称矩阵，所以它的所有特征值都是实数</p>
</li>
<li><p>对任意向量$f$,我们有</p>
<script type="math/tex; mode=display">
f^TLf = \frac{1}{2}\sum\limits_{i,j=1}^{n}w_{ij}(f_i-f_j)^2</script><p>这个利用拉普拉斯矩阵的定义很容易得到如下：</p>
<script type="math/tex; mode=display">
\begin{align*}\label{12}
& f^TLf \\
& = f^TDf - f^TWf \\
& = \sum\limits_{i=1}^{n}d_if_i^2 - \sum\limits_{i,j=1}^{n}w_{ij}f_if_j \\
& =\frac{1}{2}( \sum\limits_{i=1}^{n}d_if_i^2 - 2 \sum\limits_{i,j=1}^{n}w_{ij}f_if_j + \sum\limits_{j=1}^{n}d_jf_j^2) \\
& = \frac{1}{2}\sum\limits_{i,j=1}^{n}w_{ij}(f_i-f_j)^2 \\
\end{align*}</script></li>
<li><p>拉普拉斯矩阵是半正定的，且对应的n个实数特征值都大于0，即$0 = \lambda_1 \leq \lambda_2 \leq···\leq \lambda_n$，且最小的特征值是0，这个性质由性质3很容易得出。</p>
</li>
</ul>
<p>关于拉普拉斯算子与图中的拉普拉斯矩阵：</p>
<ul>
<li><p>拉普拉斯算子是自变量的非混合二阶偏导数之和</p>
<script type="math/tex; mode=display">
\Delta f = \sum_{i=1}^{n}\frac{\partial    ^2f}{\partial    x_i^2}</script><p>我们进行离散化，并且简化为增量为1，则有：</p>
<script type="math/tex; mode=display">
\Delta f = \sum_{(k,l) \in N(i,j)}(f(x_k,y_l)  - f(x_i,y_j))</script><p>其中$N(i,j)$是$(x_i,y_j)$的邻居节点的集合</p>
<p><img src="/2023/01/16/GNN4NLP/v2-4d7bf05128e4ba55df16a9fd61913a8e_b-167384304011199.jpg" alt="img"></p>
</li>
<li><p>推广到图中</p>
<script type="math/tex; mode=display">
\Delta f_i = \sum_{j \in N_i}w_{ij}(f_i-f_j)</script><p><img src="/2023/01/16/GNN4NLP/v2-1bfc2035f4d2a95d0af6ea0288cd8680_b-1673843042737102.jpg" alt="img"></p>
<p>如果j不是i的邻居，则$w_{ij}=0$。因此，上面的式子可以写成：</p>
<script type="math/tex; mode=display">
\Delta f_i = \sum_{j \in V}w_{ij}(f_i-f_j) = \sum_{j\in V}w_{ij}f_i-\sum_{j\in V}w_{ij}f_j = d_if_i -  \textbf{w}_i\textbf{f}</script><p>因此，我们得到</p>
<script type="math/tex; mode=display">
\Delta f = (\textbf{D}-\textbf{W})\textbf{f}</script><p>由此我们可以得到拉普拉斯矩阵的定义及以上几个性质。</p>
</li>
</ul>
<p><strong>无向图切图</strong></p>
<p>对无向图$G$的切图,每个子图的集合为:$A_1,A_2,…,A_k$,它们满足$A_i \cap A_j = \emptyset$ , 且有$A_1 \cup A_2 \cup … \cup A_k = V$</p>
<p>对于任意两个子图点的集合$A,B \subset V, A \cap B =  \emptyset$ , 我们定义两个子图之间的切图权重为</p>
<script type="math/tex; mode=display">
W(A, B) = \sum\limits_{i \in A, j \in B}w_{ij}</script><p>那么对k个子图点的集合：$A_1,A_2,…,A_k$,我们定义切图cut为：</p>
<script type="math/tex; mode=display">
cut(A_1,A_2,...A_k) = \frac{1}{2}\sum\limits_{i=1}^{k}W(A_i, \overline{A}_i )</script><p>其中$\overline{A}_i$为$A_i$的补集，意为除$A_i$子集外其他V的子集的并集</p>
<p>那么如何切图可以让子图内的点权重和高，子图间的权重和低呢？一个自然地想法是最小化$cut(A_1,A_2,…,A_k)$,但是可以发现，这种方案存在问题：<img src="/2023/01/16/GNN4NLP/1042406-20161227180625461-1385841797-1673843046978105.jpg" alt="img"> </p>
<p>这个最小化方案给出的最优解Smallest cut并不是全局的最优解Best cut。如何解决？——谱聚类之切图聚类</p>
<p><strong>谱聚类之切图聚类</strong></p>
<ul>
<li><p>RatioCut切图</p>
<p>为了避免上述问题发生，需要额外考虑最大化每个子图点的个数：</p>
<script type="math/tex; mode=display">
RatioCut(A_1,A_2,...A_k) = \frac{1}{2}\sum\limits_{i=1}^{k}\frac{W(A_i, \overline{A}_i )}{|A_i|}</script><p>我们引入指示向量$h_j \in {h_1, h_2,..h_k}\; j =1,2,…k$,对任意一个向量$h_j$,它是一个n维向量(n是样本数)，其中</p>
<script type="math/tex; mode=display">
h_{ij}= \begin{cases} 0& { v_i \notin A_j}\\ \frac{1}{\sqrt{|A_j|}}& { v_i \in A_j} \end{cases}</script><p>则有：</p>
<script type="math/tex; mode=display">
\begin{align} h_i^TLh_i & = \frac{1}{2}\sum\limits_{m=1}\sum\limits_{n=1}w_{mn}(h_{im}-h_{in})^2 \\& =\frac{1}{2}(\sum\limits_{m \in A_i, n \notin A_i}w_{mn}(\frac{1}{\sqrt{|A_i|}} - 0)^2 +  \sum\limits_{m \notin A_i, n \in A_i}w_{mn}(0 - \frac{1}{\sqrt{|A_i|}} )^2\\& = \frac{1}{2}(\sum\limits_{m \in A_i, n \notin A_i}w_{mn}\frac{1}{|A_i|} +  \sum\limits_{m \notin A_i, n \in A_i}w_{mn}\frac{1}{|A_i|}\\& = \frac{1}{2}(cut(A_i, \overline{A}_i) \frac{1}{|A_i|} + cut(\overline{A}_i, A_i) \frac{1}{|A_i|}) \\& =  \frac{cut(A_i, \overline{A}_i)}{|A_i|} \end{align}</script><p>上述第（1）式用了上面第四节的拉普拉斯矩阵的性质3. 第二式用到了指示向量的定义。可以看出，对于某一个子图i，它的RatioCut对应于$h_i^TLh_i$​,那么我们的k个子图呢？对应的RatioCut函数表达式为：</p>
<script type="math/tex; mode=display">
RatioCut(A_1,A_2,...A_k) = \sum\limits_{i=1}^{k}h_i^TLh_i = \sum\limits_{i=1}^{k}(H^TLH)_{ii} = tr(H^TLH)</script><p>注意到$H^TH = I$,则我们的切图优化目标为：</p>
<script type="math/tex; mode=display">
\underbrace{arg\;min}_H\; tr(H^TLH) \;\; s.t.\;H^TH=I</script><p>注意到我们 H 矩阵里面的每一个指示向量都是 n 维的，向量中每个变量的取值为 0 或者 $\frac{1}{\sqrt{|A_j|}}$，就有 $2^n$种取值，有 k 个子图的话就有 k 个指示向量，共有 $k2^n$种 H，因此找到满足上面优化目标的 H 是一个 NP 难的问题。那么是不是就没有办法了呢？</p>
<p>　　　　注意观察 $tr(H^TLH)$中每一个优化子目标 $h_i^TLh_i$, 其中 $h$是单位正交基， L 为对称矩阵，此时 $h_i^TLh_i$的最大值为 L 的最大特征值，最小值是 L 的最小特征值。如果你对主成分分析 PCA 很熟悉的话，这里很好理解。在 PCA 中，我们的目标是找到协方差矩阵（对应此处的拉普拉斯矩阵 L）的最大的特征值，而在我们的谱聚类中，我们的目标是找到目标的最小的特征值，得到对应的特征向量，此时对应二分切图效果最佳。也就是说，我们这里要用到维度规约的思想来近似去解决这个 NP 难的问题。</p>
<p>　　　　对于 $h<em>i^TLh_i$，我们的目标是找到最小的 L 的特征值，而对于 $tr(H^TLH) = \sum\limits</em>{i=1}^{k}h_i^TLh_i$，则我们的目标就是找到 k 个最小的特征值，一般来说，k 远远小于 n，也就是说，此时我们进行了维度规约，将维度从 n 降到了 k，从而近似可以解决这个 NP 难的问题。</p>
<p>　　　　通过找到 L 的最小的 k 个特征值，可以得到对应的 k 个特征向量，这 k 个特征向量组成一个 nxk 维度的矩阵，即为我们的 H。一般需要对 H 矩阵按行做标准化，即</p>
<script type="math/tex; mode=display">h_{ij}^{*}= \frac{h_{ij}}{(\sum\limits_{t=1}^kh_{it}^{2})^{1/2}}</script><p>　　　　由于我们在使用维度规约的时候损失了少量信息，导致得到的优化后的指示向量 h 对应的 H 现在不能完全指示各样本的归属，因此一般在得到 nxk 维度的矩阵 H 后还需要对每一行进行一次传统的聚类，比如使用 K-Means 聚类.</p>
<ul>
<li>6.2 Ncut 切图</li>
</ul>
<p>　　　　Ncut 切图和 RatioCut 切图很类似，但是把 Ratiocut 的分母 $|Ai|$换成 $vol(A_i)$. 由于子图样本的个数多并不一定权重就大，我们切图时基于权重也更合我们的目标，因此一般来说 Ncut 切图优于 RatioCut 切图。</p>
<script type="math/tex; mode=display">NCut(A_1,A_2,...A_k) = \frac{1}{2}\sum\limits_{i=1}^{k}\frac{W(A_i, \overline{A}_i )}{vol(A_i)}</script><p>　　　　，对应的，Ncut 切图对指示向量 $h$做了改进。注意到 RatioCut 切图的指示向量使用的是 $\frac{1}{\sqrt{|A_j|}}$标示样本归属，而 Ncut 切图使用了子图权重 $\frac{1}{\sqrt{vol(A_j)}}$来标示指示向量 h，定义如下:</p>
<script type="math/tex; mode=display">h_{ij}= \begin{cases} 0& { v_i \notin A_j}\\ \frac{1}{\sqrt{vol(A_j)}}& { v_i \in A_j} \end{cases}</script><p>　　　　那么我们对于 $h_i^TLh_i$, 有：</p>
<script type="math/tex; mode=display">\begin{align} h_i^TLh_i & = \frac{1}{2}\sum\limits_{m=1}\sum\limits_{n=1}w_{mn}(h_{im}-h_{in})^2 \\& =\frac{1}{2}(\sum\limits_{m \in A_i, n \notin A_i}w_{mn}(\frac{1}{\sqrt{vol(A_i)}} - 0)^2 +  \sum\limits_{m \notin A_i, n \in A_i}w_{mn}(0 - \frac{1}{\sqrt{vol(A_i)}} )^2\\& = \frac{1}{2}(\sum\limits_{m \in A_i, n \notin A_i}w_{mn}\frac{1}{vol(A_i)} +  \sum\limits_{m \notin A_i, n \in A_i}w_{mn}\frac{1}{vol(A_i)}\\& = \frac{1}{2}(cut(A_i, \overline{A}_i) \frac{1}{vol(A_i)} + cut(\overline{A}_i, A_i) \frac{1}{vol(A_i)}) \\& =  \frac{cut(A_i, \overline{A}_i)}{vol(A_i)} \end{align}</script><p>　　　　推导方式和 RatioCut 完全一致。也就是说，我们的优化目标仍然是</p>
<script type="math/tex; mode=display">NCut(A_1,A_2,...A_k) = \sum\limits_{i=1}^{k}h_i^TLh_i = \sum\limits_{i=1}^{k}(H^TLH)_{ii} = tr(H^TLH)</script><p>　　　　但是此时我们的 $H^TH \neq I$, 而是 $H^TDH = I$。推导如下：</p>
<script type="math/tex; mode=display">h_i^TDh_i = \sum\limits_{j=1}^{n}h_{ij}^2d_j =\frac{1}{vol(A_i)}\sum\limits_{j \in A_i}d_j= \frac{1}{vol(A_i)}vol(A_i) =1</script><p>　　　　也就是说，此时我们的优化目标最终为：</p>
<script type="math/tex; mode=display">\underbrace{arg\;min}_H\; tr(H^TLH) \;\; s.t.\;H^TDH=I</script><p>　　　　此时我们的 H 中的指示向量 $h$并不是标准正交基，所以在 RatioCut 里面的降维思想不能直接用。怎么办呢？其实只需要将指示向量矩阵 H 做一个小小的转化即可。</p>
<p>　　　　我们令 $H = D^{-1/2}F$, 则：$H^TLH = F^TD^{-1/2}LD^{-1/2}F$，$H^TDH=F^TF = I$, 也就是说优化目标变成了:</p>
<script type="math/tex; mode=display">\underbrace{arg\;min}_F\; tr(F^TD^{-1/2}LD^{-1/2}F) \;\; s.t.\;F^TF=I</script><p>　　　　可以发现这个式子和 RatioCut 基本一致，只是中间的 L 变成了 $D^{-1/2}LD^{-1/2}$。这样我们就可以继续按照 RatioCut 的思想，求出 $D^{-1/2}LD^{-1/2}$的最小的前 k 个特征值，然后求出对应的特征向量，并标准化，得到最后的特征矩阵 $F$, 最后对 $F$进行一次传统的聚类（比如 K-Means）即可。</p>
<p>　　　　一般来说， $D^{-1/2}LD^{-1/2}$相当于对拉普拉斯矩阵 $L$做了一次标准化，即 $\frac{L_{ij}}{\sqrt{d_i*d_j}}$</p>
<p><strong>谱聚类算法流程</strong></p>
<p>最常用的相似矩阵的生成方式是基于高斯核距离的全连接方式，最常用的切图方式是 Ncut。而到最后常用的聚类方法为 K-Means。下面以 Ncut 总结谱聚类算法流程。</p>
<p>输入：样本集 D=$(x_1,x_2,…,x_n)$，相似矩阵的生成方式, 降维后的维度 $k_1$, 聚类方法，聚类后的维度 $k_2$</p>
<p>输出： 簇划分 $C(c<em>1,c_2,…c</em>{k_2})$. </p>
<p>1) 根据输入的相似矩阵的生成方式构建样本的相似矩阵 S</p>
<p>2) 根据相似矩阵 S 构建邻接矩阵 W，构建度矩阵 D<br>3) 计算出拉普拉斯矩阵 L<br>4) 构建标准化后的拉普拉斯矩阵 $D^{-1/2}LD^{-1/2}$<br>5) 计算 $D^{-1/2}LD^{-1/2}$最小的 $k_1$个特征值所各自对应的特征向量 $f$</p>
<p>6) 将各自对应的特征向量 $f$组成的矩阵按行标准化，最终组成 $n \times k<em>1$维的特征矩阵 F<br>7) 对 F 中的每一行作为一个 $k_1$维的样本，共 n 个样本，用输入的聚类方法进行聚类，聚类维数为 $k_2$。<br>8) 得到簇划分 $C(c_1,c_2,…c</em>{k_2})$. </p>
<p><strong>谱聚类总结</strong></p>
<ul>
<li>优点<ul>
<li>谱聚类只需要数据之间的相似度矩阵，因此对于处理稀疏数据的聚类很有效。这点传统聚类算法比如 K-Means 很难做到</li>
<li>由于使用了降维，因此在处理高维数据聚类时的复杂度比传统聚类算法好。</li>
</ul>
</li>
<li>缺点<ul>
<li>如果最终聚类的维度非常高，则由于降维的幅度不够，谱聚类的运行速度和最后的聚类效果均不好。</li>
<li>聚类效果依赖于相似矩阵，不同的相似矩阵得到的最终聚类效果可能很不同。</li>
</ul>
</li>
</ul>
</li>
</ul>
</blockquote>
<h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><blockquote>
<p>得到文档的语义图之后，利用聚类算法就可以对文档进行聚类。</p>
</blockquote>
</li>
</ul>
</li>
<li><p>Graph Matching Algorithms</p>
<h5 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h5><blockquote>
<p>计算两个图之间的相似度</p>
</blockquote>
<ul>
<li><p>Graph Edit Distance</p>
<h4 id="例子-1"><a href="#例子-1" class="headerlink" title="例子"></a>例子</h4><blockquote>
<p><strong>基本思想</strong>：计算两个图之间的相似度</p>
<ul>
<li>它将距离计算为将一个图转换为另一个图所需的更改次数（即添加、删除、替换）。</li>
</ul>
</blockquote>
</li>
</ul>
<h4 id="解释与例子"><a href="#解释与例子" class="headerlink" title="解释与例子"></a>解释与例子</h4></li>
</ul>
<ul>
<li><p>Label Propagation Algorithms</p>
<h5 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h5><blockquote>
<p><strong>基本思想</strong>：标签传播算法 (LPAs) 是一类基于图的半监督算法，可将标签从标记的数据点传播到以前未标记的数据点。</p>
<p><strong>操作实现</strong>：</p>
<ul>
<li>基本上，LPAs 通过在图上迭代地传播和聚合标签来操作。</li>
<li>在每次迭代中，每个节点根据其相邻节点拥有的标签更改其标签。</li>
<li>结果，标签信息在图形中扩散。</li>
</ul>
</blockquote>
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><h4 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h4><blockquote>
<ul>
<li>LPA have been widely used in the network science literature for discovering <strong>community structures</strong> in complex networks.</li>
<li>word-sense disambiguation</li>
<li>sentiment analysis</li>
</ul>
<p>这些应用程序通常专注于标记数据稀缺的<strong>半监督学习</strong>设置，并利用 LPA 算法将标签从有限的标记示例传播到大量类似的未标记示例，并<strong>假设类似示例应该具有相似的标签</strong>。</p>
</blockquote>
</li>
<li><p>传统方法的限制和与GNNs的联系</p>
<ul>
<li><p><strong>限制</strong></p>
<ul>
<li>他们的表达能力有限。他们主要专注于捕获图的结构信息，但没有考虑节点和边的特征，这对于许多 NLP 应用程序也非常重要。</li>
<li>传统的基于图的算法没有统一的学习框架。不同的基于图的算法具有非常不同的属性和设置，并且仅适用于某些特定的用例</li>
</ul>
</li>
<li><p><strong>联系</strong></p>
<ul>
<li>因此需要一个学习能力更强的统一的框架</li>
<li>GNNs 作为一类特殊的神经网络，可以对任意图结构数据进行建模</li>
<li>大多数 GNNs 变体可以被视为基于消息传递的学习框架。与传统的基于消息传递的算法（如 LPAs）通过在图上传播标签进行操作不同，GNNs 通常通过通过多个神经层转换、传播和聚合节点/边的特征来操作，以便学习更好的图表示。</li>
<li>作为一种通用的基于图的学习框架，GNNs 可以应用于各种与图相关的任务，例如节点分类、链接预测和图分类。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Graph-Neural-Networks"><a href="#Graph-Neural-Networks" class="headerlink" title="Graph Neural Networks"></a>Graph Neural Networks</h2><p><img src="/2023/01/16/GNN4NLP/image-20221009232432760-167384297065920.png" alt="image-20221009232432760"></p>
<p><img src="/2023/01/16/GNN4NLP/image-20221009232455005-167384297065921.png" alt="image-20221009232455005"></p>
<h3 id="Foundations"><a href="#Foundations" class="headerlink" title="Foundations"></a>Foundations</h3><ul>
<li><p>本质上是图表示学习的模型，可以被应用到关注节点的任务以及关注图的任务中去。</p>
</li>
<li><p>它学习图中每个节点的embedding，并综合节点embeddings来产生图embedding</p>
</li>
<li><p>通常节点embedding的学习需要利用节点embedding和图结构</p>
<script type="math/tex; mode=display">
h_i(l) = f_{filter}(A,H^{(l-1)})</script><p>其中$A\in\mathbb{R}^{n\times n}$是图的邻接矩阵，$H^{(l-1)} = { h<em>1^{(l-1)},h_2^{(l-1)},…,h_n^{(l-1)} } \in \mathbb{R}^{n \times d}$表示第$l-1$GNN层的节点embeddings输入。d是$h_i^{(l-1)}$的维数。我们将上面式子中描述的过程称为$graph\ filtering$并且$f</em>{filter}(·,·)$被称为图滤波器。</p>
<ul>
<li>不同的模型只在图滤波器的选择和参数化上有区别</li>
<li>图滤波不改变图的结构，但是会提炼节点embeddings</li>
</ul>
</li>
<li><p>由于图滤波不会改变图结构，因此我们受CNNs的影响引入池化操作来产生图级的embeddings</p>
<ul>
<li>图池化将图及其节点嵌入作为输入，然后生成一个具有较少节点的较小图及其相应的新节点嵌入。<script type="math/tex; mode=display">
A',H' = f_{pool}(A,H)</script>其中$f_{pool}(·,·) \ A\in\mathbb{R}^{n\times n}$ 和$A’ \in \mathbb{R}^{n’\times n’}$是进行池化前后的邻接矩阵。$H,H’$则是池化前后的节点embeddings。在绝大多数情况下，将$n’$设置成1来获取整个图的embedding</li>
</ul>
</li>
</ul>
<h3 id="Methodologies"><a href="#Methodologies" class="headerlink" title="Methodologies"></a>Methodologies</h3><p><strong>Graph Filtering</strong></p>
<h6 id="基础数学知识"><a href="#基础数学知识" class="headerlink" title="基础数学知识"></a>基础数学知识</h6><blockquote>
<p><strong>谱图理论（Spectral Graph Theory）</strong></p>
<p>通过分析图的拉普拉斯矩阵来学习图的性质。</p>
<ul>
<li><p>Laplacian Matrix</p>
<ul>
<li>对称</li>
<li>半正定</li>
<li>0特征向量的数量等于图中联通分量的数量</li>
</ul>
</li>
<li><p>Graph Signal Processing</p>
<ul>
<li><p>graph signal</p>
<p>一个graph signal由图和一个定义在图域中的映射函数$f$构成。</p>
<script type="math/tex; mode=display">
f: V \rightarrow \mathbb{R}^{1\times d}</script><p>其中d是和每个节点相关的值得维数。不失一般性，我们将d设置为1，从而对所有的节点，我们有$ \textbf{f}$</p>
<p><img src="/2023/01/16/GNN4NLP/image-20221010154638572-167384297065922.png" alt="image-20221010154638572"></p>
<p>一个一维的图信号实例</p>
<p>如果连接节点中的值相似，则图是<strong>平滑</strong>的。一个平滑的图信号的<strong>频率（frequency）</strong>会更低，因为值在图上通过边的变化是缓慢的。</p>
<p>可以用$\textbf{f}^TL\textbf{f}$来表征信号$\textbf{f}$的平滑程度，称为图信号的smoothness/frequency。</p>
<p>图信号和经典的信号一样，都可以在<strong>时域(time domain)</strong>和<strong>频域(frequency domain)</strong>中表示。</p>
</li>
<li><p>Graph Fourier Transform</p>
<script type="math/tex; mode=display">
\hat{f} ( \xi ) = \lt f ( t ) , e x p ( - 2 \pi i t \xi ) \gt = \int _ { - \infty } ^ { \infty } f ( t ) e x p ( - 2 \pi i t\xi ) d t</script><p>经典的傅里叶变换将信号$f(t)$分解为了一系列任意实数$\xi$的复指数$exp(-2\pi i t \xi)$。其中，$\xi$可以看做是对应的指数项的频率。</p>
<p>这些指数可以看成是<strong>拉普拉斯算子</strong>的<strong>特征函数</strong>。</p>
<script type="math/tex; mode=display">
{ \nabla  }   exp ( - 2 \pi i t \xi) = \frac { \partial ^ { 2 } } { \partial t ^ { 2 } }  }exp ( - 2 \pi i t \xi) \\ 
{ = \frac { \partial } { \partial t } ( - 2 \pi i \xi ) e x p ( - 2 \pi i t\xi ) } \\ { = - ( 2 \pi i  \xi ) ^ { 2 } { e x p ( - 2\pi i t \xi) }</script><p>类似的，图傅里叶变换可以表示为：</p>
<script type="math/tex; mode=display">
\hat{\textbf{f}}[l] = <\textbf{f},\textbf{u}_l>=\sum_{i=1}^{N}\textbf{f}[i]\textbf{u}_l[i]</script><p>其中$\textbf{u}_l$是图的拉普拉斯矩阵的第$l$个特征向量。对应的特征值$\lambda_l$代表特征向量的frequency/smoothness。</p>
<p>计算得到的$\hat{\textbf{f}}$就成为了信号$\textbf{f}$的图傅里叶变换的结果。</p>
<script type="math/tex; mode=display">
\hat{\textbf{f}} = \textbf{U}^T\textbf{f}</script><p>图傅里叶变换可以看做是将输入信号<strong>分解</strong>为<strong>不同频率的图傅里叶基</strong>的过程。得到的<strong>系数</strong>$\hat{\textbf{f}}$则表示不同傅里叶基对输入信号的贡献程度。是信号在<strong>spectral domain</strong>中的表示。</p>
<p>如以下等式所示：</p>
<script type="math/tex; mode=display">
\textbf{u}_l^T\textbf{L}\textbf{u}_l=\lambda_l·\textbf{u}_l^T\textbf{u}_l=\lambda_l</script><p>特征向量对应的特征值衡量其smoothness(因为上面的式子本身就是衡量信号在图中的平滑程度).<img src="/2023/01/16/GNN4NLP/image-20221010164908386-167384297065923.png" alt="image-20221010164908386"></p>
<p>同样的，有逆图傅里叶变换过程，会将spectral representation再转换回spatial representation</p>
<script type="math/tex; mode=display">
\textbf{f} = \sum_{l=1}^N\hat{f}[l]\textbf{u}_l[i]</script><p>即$\textbf{f} = \textbf{U}\hat{\textbf{f}}$。</p>
<p><img src="/2023/01/16/GNN4NLP/image-20221010170329407-167384297066024.png" alt="image-20221010170329407"></p>
<p>图 2.5 显示了空间域和频谱域中的图形信号。具体来说，图 2.5a 显示了空间域中的图形信号，图 2.5b 显示了频谱域中的相同图形信号。在图 2.5b 中，x 轴是图傅立叶基，y 轴表示相应的图傅立叶系数。</p>
</li>
</ul>
</li>
</ul>
</blockquote>
<ul>
<li><p>Spectral-based Graph Filters</p>
<h5 id="定义-2"><a href="#定义-2" class="headerlink" title="定义"></a>定义</h5><blockquote>
<ul>
<li><p>Graph Spectral Filtering</p>
<p><img src="/2023/01/16/GNN4NLP/image-20221010143237008-167384297066025.png" alt="image-20221010143237008"></p>
<p>图谱滤波的思想是调制图信号的频率，使得其中的一些频率组成保持或增强，其他的则被移除或减弱。</p>
<p>因此，给定一个图信号$ \textbf{f}\in \mathbb{R}^{N}$,我们首先需要在信号上应用<strong>Graph Fourier Transform(GFT)</strong>来得到它的<strong>graph Fourier coefficients(图傅里叶系数)</strong>。然后，在空间域中重建信号前调整这些图傅里叶系数。</p>
<p>首先我们通过图傅里叶变换得到信号在频域中的系数</p>
<script type="math/tex; mode=display">
\hat{\textbf{f}} = \textbf{U}^T\textbf{f}</script><p>为了调节信号的频率(frequencies)，我们对图傅里叶系数进行如下滤波操作：</p>
<script type="math/tex; mode=display">
\hat{\textbf{f}}'[i] = \hat{\textbf{f}}[i]·\gamma(\lambda_i),for\quad i=1,...,N</script><p>其中$\gamma(\lambda_i)$是以$\lambda_i$为输入的，来决定对应频率成分如何调节的函数。这个过程可以被下面的矩阵形式表述：</p>
<script type="math/tex; mode=display">
\hat{\textbf{f}} ^ { \prime } = \gamma ( \Lambda ) \cdot \hat{\textbf{f}} = \gamma ( \Lambda ) \cdot \textbf{U} ^ { T } \textbf{f}</script><p>其中$\Lambda$是一个由频率构成的对角矩阵(拉普拉斯矩阵的特征值)，并且$\gamma(\Lambda)$会将函数$\gamma()$应用到对角矩阵$\Lambda$中的每一个元素上。</p>
<p><img src="/2023/01/16/GNN4NLP/image-20221011132228704-167384297066126.png" alt="image-20221011132228704"></p>
<p>得到过滤后的系数后，我们就可以用逆图傅里叶变换将信号重建会图域中：</p>
<script type="math/tex; mode=display">
\textbf{f}^{\prime} = \textbf{U}\hat{\textbf{f}}^{\prime}=\textbf{U}\cdot\gamma(\Lambda)\cdot\textbf{U}^T\textbf{f}</script><p>其中$\textbf{f}^{\prime}$是得到的过滤后的图信号。这个过滤的过程可以看做是将算子$\textbf{U}\cdot\gamma(\Lambda)\cdot\textbf{U}^\top$应用于输入的图信号。</p>
<p>便利起见，我们有时将函数$\gamma(\Lambda)$称为滤波器，因为它控制了图信号中的频率分量是如何被过滤的。比如，如果$\gamma(\lambda_i)$等于0，然后$\hat{\textbf{f}}^{\prime}[i]=0$这就意味着信号$\textbf{f}$中的频率组分$\textbf{u}_i$被移除了。</p>
</li>
<li><p>Spectral-based Graph Filter</p>
<p>如果我们想要图信号更加平滑，我们可以将$\gamma(\Lambda)$设置成低通滤波器。但是，在图神经网络中，我们不知道哪些频率是更重要的，因此我们需要学习出滤波器的具体形式。更详细地说，我们选定$\gamma(\Lambda)$为特定的函数，然后学习其中的参数。</p>
<p>最只有的方式，是对每一个特征值都有一个对应的函数</p>
<script type="math/tex; mode=display">
\gamma(\lambda_l) = \theta_l</script><p>但是，这种滤波器会有很大的局限性：</p>
<ul>
<li>参数过多，等于节点的个数。现实世界中的图的节点会过多</li>
<li>滤波器$\textbf{U}\cdot\gamma(\Lambda)\cdot\textbf{U}^\top$很可能是一个稠密矩阵，因此输出信号$\textbf{f}^{\prime}$的第i个元素会和图中的所有节点有关。即算子并非<strong>空间局部（spatially localized）</strong>的。此外，由于拉普拉斯矩阵的特征分解和稠密矩阵的矩阵乘法使得计算成本很昂贵。</li>
</ul>
<p>为了解决这个问题，一个<strong>多项式滤波算子——Poly-Filter</strong>被提出了。</p>
<script type="math/tex; mode=display">
\gamma(\lambda_l) = \sum_{k=0}^{K}\theta_k\lambda_l^k
\newline
\gamma(\Lambda) = \sum_{k=0}^{K}\theta_k\Lambda^k</script><p>此时，同阶的是共用同一个参数$\theta$的，因此参数个数是K+1，并不依赖于图节点的个数。</p>
<p>同时，滤波算子$\textbf{U}\cdot\gamma({\Lambda})\cdot\textbf{U}^\top$可以表示成拉普拉斯矩阵的多项式。这意味着：</p>
<ul>
<li>不需要再进行拉普拉斯矩阵的特征分解</li>
<li>多项式参数化滤波算子是<strong>空间局部化</strong>的，即输出 f’ 的每个元素的计算只涉及图中的少量节点。</li>
</ul>
<p>应用Poly-Filter，我们可以得到过滤后的图信号如下：<img src="/2023/01/16/GNN4NLP/image-20221011144220776-167384297066127.png" alt="image-20221011144220776"></p>
<p>其中<img src="/2023/01/16/GNN4NLP/image-20221011144520825-167384297066128.png" alt="image-20221011144520825"></p>
<p>因此，我们有：<img src="/2023/01/16/GNN4NLP/image-20221011144758370-167384297066129.png" alt="image-20221011144758370"></p>
<p>拉普拉斯矩阵的多项式都是稀疏的。同时，只有当节点$v_i$和节点$v_j$之间的最短路径长度即$dis(v_i, v_j)$小于或等于k时，$\textbf{L}^k$的第i，j个元素才非零。(可以用归纳法进行证明)</p>
<p>下面我们来关注输出的过滤后的信号的单个元素和哪些节点有关：</p>
<script type="math/tex; mode=display">
\textbf{f}^\prime[i] = \sum_{v_j\in V}(\sum_{k=0}^K\theta_k\textbf{L}_{i,j}^k)\textbf{f}[j]</script><p>根据上面的结论我们知道，只有在K跳之内的节点才能参与计算，并不是图中的所有节点都参与计算的:<img src="/2023/01/16/GNN4NLP/image-20221011150648322-167384297066130.png" alt="image-20221011150648322"><img src="/2023/01/16/GNN4NLP/image-20221011150702333-167384297066131.png" alt="image-20221011150702333"></p>
<p>其中$N^K(v_i)$代表节点K跳内的邻居节点；$dis(v_i,v_j)$代表节点之间最短路径的长度。</p>
<p>Poly-Filter是在空间局部的，因为只和最邻近的K跳节点有关；也可以看做是空间滤波器，因为可以用空间结构表出（如上面的式子所示）。</p>
<p>尽管Poly-Filter有很多优势，它也有很多局限性：</p>
<ul>
<li>多项式的基($1,x,x^2,…$)彼此是不正交的。</li>
<li>这就意味着系数彼此是相互依赖的，会导致一个节点的更新可能会导致其他节点的变动</li>
</ul>
<p>为了解决这个问题——<strong>Chebyshev Polynomial and Cheby-Filter</strong></p>
<p>切比雪夫多项式$T_k(y)$可以根据下面的递归关系产生</p>
<script type="math/tex; mode=display">
T_k(y) = 2yT_{k-1}(y)-T_{k-2}(y)</script><p>其中，$T_0(y)$=1且$T_1(y) = y$。对于$y \in [-1,1]$，这些切比雪夫多项式可以被表示成三角形式</p>
<script type="math/tex; mode=display">
T_k(y) = cos(k\ arccos(y))</script><p>而且这些切比雪夫多项式还满足如下的关系：</p>
<p><img src="/2023/01/16/GNN4NLP/image-20221011161701031-167384297066132.png" alt="image-20221011161701031"></p>
<p>其中$\delta_{l,m}$=1当且仅当l=m时成立，否则为0；该式子表明了切比雪夫多项式是相互正交的。因此，切比雪夫多项式组成了希尔伯特空间中平方可积函数的一组正交基，在$dy/\sqrt{1-y^2}$的度量下。记作$L^2([-1,1],dy/\sqrt1-y^2)$</p>
<p>由于切比雪夫多项式的域是[-1,1],为了用切比雪夫多项式来近似滤波器，我们重新缩放和移动拉普拉斯矩阵的特征值如下：</p>
<script type="math/tex; mode=display">
\widetilde{\lambda}_l = \frac{2\cdot\lambda_l}{\lambda_{max}}-1</script><p>其中$\lambda_{max}$也就是最大的特征值$\lambda_N$。这样一来，所有的特征值都被改变到[-1,1]范围内了。矩阵层面的表示为：</p>
<script type="math/tex; mode=display">
\widetilde{\Lambda} = \frac{2\cdot\Lambda}{\lambda_{max}}-I</script><p>用截断的 切比雪夫(Chebyshev)多项式参数化的 Chevy-Filter 可以表述如下：</p>
<script type="math/tex; mode=display">
\gamma(\Lambda) = \sum_{k=0}^{K}\theta_kT_k(\widetilde{\Lambda})</script><p>因此，对图信号使用Cheby-Filter的滤波过程可以表示为：<img src="/2023/01/16/GNN4NLP/image-20221011195033784-167384297066133.png" alt="image-20221011195033784"></p>
<p>通过数学归纳法容易证明</p>
<script type="math/tex; mode=display">
\textbf{U}T_k(\widetilde{\Lambda})\textbf{U}^\top = T_k(\widetilde{\textbf{L}})
\newline
\widetilde{\textbf{L}} = \frac{2\textbf{L}}{\Lambda_{max}} - \textbf{I}</script><p>此时，Cheby-Filter可以写成：<img src="/2023/01/16/GNN4NLP/image-20221011195716309-167384297066134.png" alt="image-20221011195716309"></p>
<p>因此，Cheby-Filter 仍然享有 Poly-Filter 的优点，同时在扰动下更稳定。</p>
<p><strong>GCN-Filter: Simplified Cheby-Filter Involving 1-hop Neighbors</strong></p>
<p>通过将切比雪夫多项式中的K设置成1并逼近$\lambda_{max} = 2$,GCN-Filter可以从Cheby-Filter中简化得到。</p>
<p>在这样的简化和逼近下，可以简化为：<img src="/2023/01/16/GNN4NLP/image-20221011200344382-167384297066135.png" alt="image-20221011200344382"></p>
<p>相应地，将 GCN-Filter 应用于图信号 f，我们可以得到输出信号 f’，如下所示：<img src="/2023/01/16/GNN4NLP/image-20221011200414926-167384297066236.png" alt="image-20221011200414926"></p>
<p>注意这里的拉普拉斯矩阵是规范化的，即$\textbf{L} = \textbf{I}-\textbf{D}^{-\frac{1}{2}}\textbf{A}\textbf{D}^{-\frac{1}{2}}$</p>
<p>将$\theta=\theta_0=-\theta_1$带入进行进一步的化简得到：<img src="/2023/01/16/GNN4NLP/image-20221011200817824-167384297066237.png" alt="image-20221011200817824"></p>
<p>由于矩阵$\textbf{I}+\textbf{D}^{-\frac{1}{2}}\textbf{A}\textbf{D}^{-\frac{1}{2}}$的特征值在[0,2]上，当在深度学习中反复使用这个算子的时候可能导致数值不稳定，梯度消失/梯度爆炸。为了解决这个问题，我们使用renormalization trick</p>
<script type="math/tex; mode=display">
\widetilde{\textbf{A}} = \textbf{A} + \textbf{I}
\newline
\widetilde{\textbf{D}}_{ii} = \sum_{j}\widetilde{\textbf{A}}_{i,j\cdot}
\newline
\widetilde{\textbf{D}}^{-\frac{1}{2}}\widetilde{\textbf{A}}\widetilde{\textbf{D}}^{-\frac{1}{2}}</script><blockquote>
<p>这里怎么理解呢？</p>
<p>我们将这个矩阵抽象成$A = S\Lambda S^{-1}$,那么$A^k$可以从$\Lambda^{k}$中得到</p>
<script type="math/tex; mode=display">
u_k = A^ku_0=S\Lambda^kS^{-1}u_0</script><p>我们令$S^{-1}u_0 = c$则有：<img src="/2023/01/16/GNN4NLP/image-20221011221409703-167384297066238.png" alt="image-20221011221409703"></p>
<p>当层数比较多的时候，这个式子的结果主要由最大的特征值决定。当特征值大于1的时候，会趋向于无穷；特征值小于1的时候，会趋向于0。正好对应着梯度爆炸和梯度消失两种情况。</p>
<p>而新的用来替换的方案，虽然不是恒等的，但是相当于进行了一种新的标准化。新的方案的特征值分布在[-1,1]上，相较于[0,2]发生梯度消失/梯度爆炸的几率更小。</p>
</blockquote>
<p>此时，我们得到GCN-Filter的算子如下：</p>
<script type="math/tex; mode=display">
\textbf{f}^{\prime} = \theta\widetilde{\textbf{D}}^{-\frac{1}{2}}\widetilde{\textbf{A}}\widetilde{\textbf{D}}^{-\frac{1}{2}}\textbf{f}</script><p>对于单个节点，这个过程可以看作是从它的 1 跳邻居聚合信息，其中节点本身也被认为是它的 1 跳邻居。因此，GCN-Filter 也可以看作是一个基于空间的过滤器，它在更新节点特征时只涉及直接连接的邻居。</p>
<p><strong>Graph Filters for Multi-channel Graph Signals</strong></p>
<p>上面我们讨论的都是1-channel的图信号，即每个节点的信号是一个标量。然而，实际上典型的图信号大都是multi-channel的。即每个节点都有一个特征向量。</p>
<p>一个$d<em>{in}$维度的多通道图信号可以表示为$\textbf{F} \in \mathbb{R}^{N\times d</em>{in}}$。为了将图滤波器扩展到多通道信号上，我们利用来自所有输入通道的信号来生成输出信号，如下所示：</p>
<script type="math/tex; mode=display">
\textbf{f}_{out} = \sum_{d=1}^{d_{in}}\textbf{U}\cdot\gamma_d(\Lambda)\cdot\textbf{U}^\top\textbf{F}_{:,d}</script><p>其中$\textbf{f}_{out} \in \mathbb{R}^N$是滤波器的单通道输出。因此，该过程可以被视为在每个输入通道中应用图形过滤器，然后计算它们的结果的总和。</p>
<p>就像经典的卷积神经网络一样，在大多数情况下，使用多个滤波器来过滤输入通道，输出也是多通道信号。假如我们使用$d_out$通道的滤波器，则：</p>
<script type="math/tex; mode=display">
\textbf{F}^{\prime}_{:,j} = \sum_{d=1}^{d_{in}}\textbf{U}\cdot\gamma_{j,d}(\Lambda)\cdot\textbf{U}^\top\textbf{F}_{:,d} \quad for \ j=1,...,d_{out}</script><p>具体来说，在方程式中的 GCN-Filter 的情况下。这个多通道输入和输出的过程可以简单地表示为：</p>
<script type="math/tex; mode=display">
\textbf{F}^{\prime}_{:,j} = \sum_{d=1}^{d_{in}}\theta_{j,d}\widetilde{\textbf{D}}^{-\frac{1}{2}}\widetilde{\textbf{A}}\widetilde{\textbf{D}}^{-\frac{1}{2}}\textbf{F}_{:,d} \quad for \ j=1,...,d_{out}</script><p>可以进一步写成：</p>
<script type="math/tex; mode=display">
\textbf{F}^{\prime} = \widetilde{\textbf{D}}^{-\frac{1}{2}}\widetilde{\textbf{A}}\widetilde{\textbf{D}}^{-\frac{1}{2}}\textbf{F}\Theta</script><p>其中$\Theta \in \mathbb{R}^{d<em>{in}\times d</em>{out}}$并且$\Theta[d,j]=\theta_{j,d}$是和第j个输出通道和第d个输入通道相对应的参数。</p>
<p>具体来说，对于单个节点 $v_i$，等式中的过滤过程也可以表述为以下形式：<img src="/2023/01/16/GNN4NLP/image-20221011235610664-167384297066239.png" alt="image-20221011235610664"></p>
<p>其中$\widetilde{d<em>i}=\widetilde{\textbf{D}}</em>{i,i}$并且我们使用$\textbf{F}<em>i\in \mathbb{R}^{1\times d</em>{out}}$代表的是$\textbf{F}$的第i行，即节点$v_i$的特征。上述方程式中的过程可以看作是从节点 vi 的 1 跳邻居聚合信息。</p>
</li>
</ul>
</blockquote>
</li>
<li><p>Spatial-based Graph Filters</p>
<h5 id="再次学习"><a href="#再次学习" class="headerlink" title="再次学习"></a>再次学习</h5><blockquote>
<ul>
<li><p>Filter in the very first GNN</p>
<p>对节点$v_i$而言，它对应的标签可以表示为$l_i$。在过滤的过程中，输入的图特征可以被表示为$\textbf{F}$,其中$\textbf{F}_i$是它的第i行，是和节点$v_i$相对应的特征。整个过程可以表示为：</p>
<script type="math/tex; mode=display">
\textbf{F}^{\prime}_i = \sum_{v_j \in N(v_i)}g(l_i,\textbf{F}_j,l_j)</script><p>其中g()是参数化的函数，称为local transition function，是空间局部的。这个过程对每个节点来说只使用其1跳的邻居节点。</p>
<p>注意，节点的标签信息$l_i$可以被视为<strong>初始的输入信息</strong>，在过滤过程中是<strong>固定的</strong>并被利用。</p>
</li>
<li><p>GraphSAGE-Filter</p>
<p>对一个单独的节点$v_i$，产生它的新的特征的过程可以描绘如下：<img src="/2023/01/16/GNN4NLP/image-20221012145843950-167384297066240.png" alt="image-20221012145843950"></p>
<p>其中 SAMPLE() 是将一个集合作为输入并从输入中随机采样 S 个元素作为输出的函数，AGGREGATE() 是一个组合来自相邻节点的信息的函数，其中 $f’_{N_S(v_i)}$ 表示AGGREGATE() 函数的输出，而 [·,·] 是连接操作。</p>
<p>目前，已经有很多AGGREGATE()函数被提出了：</p>
<ul>
<li><p><strong>Mean aggregator</strong>: 直接对向量逐元素取平均。这和GCN-Filter很像，两者都使用（加权）平均，但区别在于：本方案后续对待节点自身的特征时是拼接上去的，而GCN-Filter则是同等对待，也是加权平均的一部分。</p>
</li>
<li><p><strong>LSTM aggregator</strong>: 将邻居节点视作一个序列，然后使用LSTM结构进行处理，最后一个单元的输出就作为操作的结果。</p>
<p>由于节点本身不存在序列，因此采用随机序列。</p>
</li>
<li><p><strong>Pooling operator</strong>: 在进行最大池化操作前首先通过一层神经网络：</p>
<script type="math/tex; mode=display">
\textbf{f}^{\prime}_{N_S(v_i)}=max(\{ \alpha(\textbf{F}_j\Theta_{pool} ),\forall v_j \in N_S(v_i)\})</script><p>其中max()是逐元素的最大化操作；$\Theta_{pool}$代表转移矩阵，$\alpha()$是一个非线性激活函数。</p>
</li>
</ul>
<p>是空间局部性的，且聚合器是节点间共享的。</p>
</li>
<li><p>MPNN: 基于空间的图滤波器的通用框架</p>
<p>Message Passing Neural Networks (MPNN) 是一个通用的图神经网络框架。很多基于空间的图滤波器都是它的特殊情况。</p>
<p>对一个节点$v_i$而言，MPNN-Filter按照如下流程更新其特征：<img src="/2023/01/16/GNN4NLP/image-20221012154439952-167384297066341.png" alt="image-20221012154439952"></p>
<p>其中$M()$是message函数，$U()$是更新函数，$e_{(v_i,v_j)}$是可能有的边特征。</p>
<p>M()从节点的邻居节点产生需要传递给该节点的messages.</p>
<p>U()然后通过结合<strong>原始特征</strong>和<strong>来自其邻居的聚合消息</strong>来更新节点 vi 的特征。</p>
<p>如果我们将第一个式子中的<strong>求和</strong>操作替换成一个抽象的操作，这个框架可以更加通用。</p>
</li>
</ul>
</blockquote>
</li>
<li><p>Attention-based Graph Filter</p>
<h5 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h5><blockquote>
<p>Self-attention机制被引入到graph attention networks(GAT)中来建立空间图滤波器。我们将GAT中的滤波器称为<strong>GAT-Filter</strong>。</p>
<p>和GCN-Filter类似，GAT-Filter也在更新节点特征的时候使用了来自邻居节点的聚合信息。但是，GCN-Filter仅仅是基于图结构，GAT-Filter则是在执行聚合时尝试<strong>区分邻居的重要性</strong>。</p>
<ul>
<li><p>Pipeline</p>
<ul>
<li>在为节点 $v_i$ 生成新特征时，它会关注其所有邻居以生成每个邻居的重要性分数。</li>
<li>然后在聚合过程中采用这些重要性分数作为线性系数。</li>
</ul>
</li>
<li><p>节点$v_i$和邻居节点及自身的重要性计算方法：</p>
<script type="math/tex; mode=display">
e_{ij} = a(\textbf{F}_i\Theta,\textbf{F}_j\Theta)</script><p>其中$\Theta$是一个共享的参数矩阵。$a()$是一个共享的注意力函数，它是一个单层的前馈网络：</p>
<script type="math/tex; mode=display">
a(\textbf{F}_i\Theta,\textbf{F}_j\Theta) = LeakyReLU(a^\top [\textbf{F}_i\Theta,\textbf{F}_j\Theta])</script><p>其中$[\cdot,\cdot]$指的是连接操作。$a$是一个参数化的向量。而$LeakyReLU$则是一种非线性激活函数。</p>
<p>由方程式计算的分数在被用作聚合过程中的权重之前需要进行归一化，以将输出表示保持在合理的范围内。 $v_i$ 的所有邻居的归一化是通过 softmax 层执行的：<img src="/2023/01/16/GNN4NLP/image-20221012165608095-167384297066342.png" alt="image-20221012165608095"></p>
<p>$\alpha_{ij}$就是标准化后的节点$v_j$到节点$v_i$的重要性分数。</p>
<p>有了重要性分数，节点的新的表示的计算过程就可以写成：</p>
<script type="math/tex; mode=display">
\textbf{F}_i^{\prime} = \sum_{v_j\in N(v_i)\cup\{v_i\}}\alpha_{ij}\textbf{F}_j\Theta</script><p>其中$\Theta$是计算e的时候的同一个参数矩阵。</p>
<p>为了稳定自注意力机制，提出了多头注意力机制。</p>
<p>M个独立的拥有不同的$\Theta^m$和$\alpha_{ij}^m$的，按照上式形式的注意力机制被平行地使用，他们的输出之后会连接在一起来产生最终的表示：<img src="/2023/01/16/GNN4NLP/image-20221012171329066-167384297066343.png" alt="image-20221012171329066"></p>
<p>注意力机制的滤波器也是空间局部性的，并且初始版本中各个独立的注意力机制连接前要通过激活函数，这里为了方便进行省略。</p>
</li>
</ul>
</blockquote>
</li>
<li><p>Recurrent-based Graph Filter</p>
<p>使用Gated Recurrent Unit(GRU)改进GNN filter就得到了<strong>GGNN-Filter</strong>。</p>
<h5 id="学习-1"><a href="#学习-1" class="headerlink" title="学习"></a>学习</h5><blockquote>
<ul>
<li><p>GGNN-Filter是为边是有向的同时具有不同类型的图而设计的。</p>
</li>
<li><p>对边$(v_i,v_j)$而言，我们使用$tp(v_i,v_j)$来表示其类型。过滤过程可以表示为：<img src="/2023/01/16/GNN4NLP/image-20221012194112426-167384297066344.png" alt="image-20221012194112426"></p>
<p>其中$\Theta$型的变量都是需要去学习的参数。第一步是要从传入邻居节点和传出邻居节点中聚合信息。在这个聚合过程中，转移矩阵$\Theta_{tp(v_j,v_i)}^e$是被所有类型为$tp(v_i,v_j)$的边连接到节点$v_i$的节点共享的。</p>
<p>剩下的四个式子是GRU的步骤，用来更新隐藏表示。因此，这个过程也可以表示为：<img src="/2023/01/16/GNN4NLP/image-20221012194657161-167384297066345.png" alt="image-20221012194657161"></p>
</li>
</ul>
</blockquote>
</li>
</ul>
<p><strong>Graph Pooling</strong></p>
<h6 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h6><blockquote>
<ul>
<li><p>图池化层是为了获取图级表示而提出的，主要服务于基于图滤波得到的节点embeddings进行图分类、预测的graph-focused的下流任务。</p>
</li>
<li><p>图池化操作主要根据两种信息：</p>
<ul>
<li>node features</li>
<li>graph structure</li>
</ul>
</li>
<li><p>主要分为平面池化层和分级池化层</p>
<ul>
<li>平面图池在一个步骤中直接从节点嵌入生成图级表示。</li>
<li>分层图池包含几个图池层，每个池层都遵循一堆图过滤器。</li>
</ul>
</li>
<li><p>一个图池化层通常输入一个图，并返回粗化图：</p>
<script type="math/tex; mode=display">
\textbf{A}^{(op)},\textbf{F}^{(op)}=pool(\textbf{A}^{(ip)},\textbf{F}^{(ip)})</script><p>就是两件事：新的图+新的特征</p>
</li>
</ul>
</blockquote>
<h4 id="easy"><a href="#easy" class="headerlink" title="easy"></a>easy</h4><blockquote>
<p><strong>Flat Graph Pooling</strong></p>
<p>过程总结：</p>
<script type="math/tex; mode=display">
\textbf{f}_G=pool(\textbf{A}^{(ip)},\textbf{F}^{(ip)})</script><p>其中$\textbf{f}<em>G\in \mathbb{R}^{1\times d</em>{op}}$是图表示。</p>
<p>最大池化层：</p>
<script type="math/tex; mode=display">
\textbf{f}_G = max(\textbf{F}^{(ip)})
\newline
\textbf{f}_G[i] = max(\textbf{F}^{(ip)}_{:,i})</script><p>平均池化层：</p>
<script type="math/tex; mode=display">
\textbf{f}_G = ave(\textbf{F}^{(ip)})</script><p>基于注意力的平面池化操作，称为<strong>门控全局池化(gated global pooling)</strong>。衡量每个节点重要性的注意力分数用于总结节点表示以生成图形表示。具体来说，节点 vi 的注意力分数计算为：<img src="/2023/01/16/GNN4NLP/image-20221012200836949-167384297066346.png" alt="image-20221012200836949"></p>
<p>其中h是将输入映射到一个标量的前馈网络。</p>
<p>使用学习到的注意力分数，图形表示可以从节点表示中总结为：</p>
<p><img src="/2023/01/16/GNN4NLP/image-20221012201009369-167384297066347.png" alt="image-20221012201009369"></p>
<p>其中$\Theta_{ip}$是待学习的参数。</p>
<p>一些层池化操作潜入了过滤层。一个“假”节点被添加到连接到所有节点的图中（Li et al., 2015）。这个“假”节点的表示可以在过滤过程中学习。它的表示捕获整个图的信息，因为它连接到图中的所有节点。因此，“假”节点的表示可以用作下游任务的图表示。</p>
</blockquote>
<h5 id="进一步"><a href="#进一步" class="headerlink" title="进一步"></a>进一步</h5><blockquote>
<ul>
<li><p>平面池化层在汇总图表示的节点表示时通常会忽略<strong>分层图结构信息</strong>。</p>
</li>
<li><p>分层图池化层旨在通过逐步粗化图来保留分层图结构信息，直到实现图表示。</p>
</li>
<li><p>分层池化层可以根据它们粗化图的方式进行粗略分组</p>
<ul>
<li>一种分层池化层通过<strong>子采样</strong>来粗化图，即选择最重要的节点作为粗化图的节点。</li>
<li>一种不同类型的分层池化层将输入图中的<strong>节点组合</strong>在一起形成超级节点，作为粗化图的节点。</li>
</ul>
</li>
<li><p>Downsampling-based Pooling</p>
<p>为了粗化输入的图，我们需要从输入的$N_{op}$个节点中根据重要性测度来进行筛选。</p>
<p>在本类池化层中，主要由三个组成部分：</p>
<ul>
<li>developing the <strong>measure</strong> for downsampling</li>
<li>generating <strong>graph structure</strong> for the coarsened graph</li>
<li>generating <strong>node features</strong> for the coarsened graph</li>
</ul>
<p>开山之作——<strong>gPool layer</strong></p>
<p>重要性测度是从输入节点特征中学到的：</p>
<script type="math/tex; mode=display">
\textbf{y}=\frac{\textbf{F}^{(ip)}\textbf{p}}{||\textbf{p}||}</script><p>$\textbf{F}^{(ip)}\in \mathbb{R}^{N<em>{ip}\times d</em>{ip}}$是代表输入节点特征的。而$\textbf{p}\in \mathbb{R}^{d_{ip}}$是将输入特征投影到重要性分数中要学习的向量。</p>
<p>得到重要性分数y之后就可以得到前k个最重要的节点了</p>
<script type="math/tex; mode=display">
idx = rank(\textbf{y},N_{op})</script><p>粗化图的图结构可以从输入图的图结构中推导出为：</p>
<script type="math/tex; mode=display">
\textbf{A}^{(op)} = \textbf{A}^{(ip)}(idx,idx)</script><p>等式右边的部分指的是从矩阵中按行和列进行抽取。</p>
<p>同样，也可以从输入节点特征中提取节点特征。在（Gao 和 Ji，2019）中，采用门控系统来控制从输入特征到新特征的信息流。具体来说，具有较高重要性分数的选定节点可以有更多的信息流到粗化图，可以建模为：<img src="/2023/01/16/GNN4NLP/image-20221012220208418-167384297066348.png" alt="image-20221012220208418"></p>
<p>其中，$\sigma()$是将重要性分数映射到(0,1)的激活函数。而$\textbf{1}<em>{d</em>{ip}}\in \mathbb{R}^{d_{ip}}$是全1向量。请注意，y(idx) 根据 idx 中的索引从 y 中提取相应的元素，而 $F^{(ip)}(idx)$ 根据 idx 检索相应的行。</p>
<p>gPool layer中学习重要性的方法忽视了<strong>图结构信息</strong>，为了容纳图结构信息，GCN-Filter被用来学习重要性分数。</p>
<script type="math/tex; mode=display">
\textbf{y}=\alpha(GCN-Filter(\textbf{A}^{(ip)},\textbf{F}^{(ip)}))</script><p>其中$\alpha$是激活函数。请注意，y 是向量而不是矩阵。也就是说，GCN-Filter的输出通道数设置为1。这个图池化操作被命名为SAGPool。</p>
</li>
<li><p>Supernode-based Hierarchical Graph Pooling</p>
<p>基于超节点的池化方法旨在通过生成超节点来粗化输入图。具体来说，他们尝试学习将输入图中的节点分配到不同的集群中，这些集群被视为超级节点。</p>
<p>这些超节点被视为粗化图中的节点。然后生成超节点之间的边和这些超节点的特征以形成粗化图。</p>
<p>这种方法有三种关键的组分：</p>
<ul>
<li>generating supernodes as the nodes for the coarsened graph</li>
<li>generating graph structure for the coarsened graph</li>
<li>generating node features for the coarsened graph</li>
</ul>
<p><strong>diffpool</strong></p>
<p>具体来说，使用 GCN-Filter 学习从输入图中的节点到超节点的软分配矩阵：</p>
<script type="math/tex; mode=display">
\textbf{S} = softmax(GCN-Filter(\textbf{A}^{(ip)}))</script><p>其中$\textbf{S}\in \mathbb{R}^{N<em>{ip}\times N</em>{op}}$是需要被学习的分配矩阵。$\textbf{F}^{(ip)}$通常是最新的图滤波层的输出。此外，可以堆叠几个 GCN 过滤器来学习分配矩阵，尽管在上面的方程式中只使用了一个过滤器</p>
<p>分配矩阵的每一行都可以看成是一个超节点。第 i 行中的第 j 个元素表示将第 i 个节点分配给第 j 个超节点的概率。</p>
<p>具体来说，粗化图的图结构可以通过利用软分配矩阵 S 从输入图生成：<img src="/2023/01/16/GNN4NLP/image-20221012231514618-167384297066449.png" alt="image-20221012231514618"></p>
<p>同理，根据赋值矩阵 S 对输入图的节点特征进行线性组合，可以得到超节点的节点特征：<img src="/2023/01/16/GNN4NLP/image-20221012231716198-167384297066450.png" alt="image-20221012231716198"></p>
<p>其中$\textbf{F}^{(inter)}\in \mathbb{R}^{N<em>{ip}\times d</em>{op}}$是通过GCN-Filter学习到的中间特征：</p>
<script type="math/tex; mode=display">
F ^ { ( inter ) } = G C N - F i l t e r ( A ^ { ( i p ) } , F ^ { ( i p ) } )</script><p>上面式子中的GCN-Filter也是可以堆叠多个。</p>
<p><strong>EigenPooling</strong></p>
<p>EigenPooling (Ma et al., 2019b) 使用谱聚类方法生成超节点，并专注于为粗化图形成图结构和节点特征。</p>
<p>应用谱聚类算法后，得到一组不重叠的簇，也可以作为粗化图的超节点。</p>
<p>输入节点和输出节点之间的<strong>分配矩阵</strong>可以被表示为$\textbf{S} \in {0,1}^{N<em>{ip}\times N</em>{op}}$,其中一行中只有一个元素是1，其余全是0。</p>
<p>更具体地说，仅当第 i 个节点分配给第 j 个超级节点时，S[i, j] = 1。对于第 k 个超级节点，我们使用 $A(k) \in \mathbb{R}^{N^{(k)}×N^{(k)}}$ 来描述其对应集群中的图结构，其中 N(k) 是该集群中的节点数。我们将采样算子$\textbf{C}^{(k)}\in {0,1}^{N_{ip}\times N^{(k)}}$​定义为：</p>
<script type="math/tex; mode=display">
\textbf{C}^{(k)}[i,j] = 1\quad if\ and \ only \ if \  \Gamma^{(k)}(j)=v_i</script><p>其中 $Γ^{(k)}$ 表示第 k 个集群中的节点列表，$Γ^{(k)}(j) = v_i$ 表示节点 vi 对应于该集群中的第 j 个节点。使用这个采样算子，第 k 个簇的邻接矩阵可以正式定义为：<img src="/2023/01/16/GNN4NLP/image-20221012235316597-167384297066451.png" alt="image-20221012235316597"></p>
<p>接下来，我们讨论为粗化图生成图结构和节点特征的过程。为了形成超级节点之间的图结构，只考虑原始图中跨集群的连接。为了实现这个目标，我们首先为输入图生成簇内邻接矩阵，该矩阵仅由每个簇内的边组成：<img src="/2023/01/16/GNN4NLP/image-20221012235530298-167384297066452.png" alt="image-20221012235530298"></p>
<p>然后，仅由跨集群的边组成的集群间邻接矩阵可以表示为 $A<em>{ext} = A-A</em>{int}$。粗化图的邻接矩阵可以得到：<img src="/2023/01/16/GNN4NLP/image-20221012235630483-167384297066453.png" alt="image-20221012235630483"></p>
<p>采用<strong>图傅里叶变换</strong>生成节点特征。具体而言，利用每个子图（或集群）的图结构和节点特征来生成相应超节点的节点特征。接下来，我们以第 k 个集群作为说明性示例来演示该过程。令 L(k) 表示该子图的拉普拉斯矩阵，$ u^{(k)}<em>1 , . . . , u^{(k)}</em>{n^{(k)}}$ 是其对应的特征向量。该子图中节点的特征可以通过使用采样算子 $C^{(k)}$ 从 $F^{(ip) }$中提取，如下所示：</p>
<p><img src="/2023/01/16/GNN4NLP/image-20221013000115875-167384297066454.png" alt="image-20221013000115875"></p>
<p>其中$\textbf{F}<em>{ip}^{(k)} \in \mathbb{R}^{N^{(k)}\times d</em>{ip}}$是第k集簇中的节点的输入特征。</p>
<p>之后，我们使用傅里叶变换来为$\textbf{F}^{(k)}_{ip}$中的所有频道产生系数：<img src="/2023/01/16/GNN4NLP/image-20221013000529937-167384297066455.png" alt="image-20221013000529937"></p>
<p>其中$\textbf{f}^{(k)}<em>i \in \mathbb{R}^{1\times d</em>{ip}}$由所有特征通道的第 i 个图傅里叶系数组成。</p>
<p>第 k 个超节点的节点特征可以通过将这些系数连接起来形成：<img src="/2023/01/16/GNN4NLP/image-20221013000931882-167384297066456.png" alt="image-20221013000931882"></p>
<p>我们通常只利用<strong>前几个系数</strong>来生成超级节点的特征，原因有两个。</p>
<ul>
<li>首先，不同的子图可能有不同数量的节点；因此，为了确保特征的相同维度，需要丢弃一些系数。</li>
<li>其次，前几个系数通常捕获大部分重要信息，因为实际上，大多数图形信号都是平滑的。</li>
</ul>
</li>
</ul>
</blockquote>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/GNN-in-NLP/" rel="tag"># GNN in NLP</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/01/15/EM/" rel="prev" title="EM">
      <i class="fa fa-chevron-left"></i> EM
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Natural-Language-Processing"><span class="nav-number">1.</span> <span class="nav-text">Natural Language Processing</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Graph-Neural-Networks-for-Natural-Language-Processing-A-Survey"><span class="nav-number">2.</span> <span class="nav-text">Graph Neural Networks for Natural Language Processing: A Survey</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">2.1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Graph-Based-Algorithms-for-Natural-Language-Processing"><span class="nav-number">2.2.</span> <span class="nav-text">Graph Based Algorithms for Natural Language Processing</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Tips"><span class="nav-number">2.2.0.1.</span> <span class="nav-text">Tips</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%9C%81%E6%B5%81"><span class="nav-number">2.2.0.1.0.1.</span> <span class="nav-text">省流</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Natural-Language-Processing-A-Graph-Perspective"><span class="nav-number">2.2.1.</span> <span class="nav-text">Natural Language Processing: A Graph Perspective</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Three-ways-to-representing-natural-language"><span class="nav-number">2.2.1.1.</span> <span class="nav-text">Three ways to representing natural language</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Graph-Based-Methods-for-Natural-Language-Processing"><span class="nav-number">2.2.2.</span> <span class="nav-text">Graph Based Methods for Natural Language Processing</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%9C%81%E6%B5%81-1"><span class="nav-number">2.2.2.0.0.1.</span> <span class="nav-text">省流</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%A7%A3%E9%87%8A"><span class="nav-number">2.2.2.0.1.</span> <span class="nav-text">解释</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BE%8B%E5%AD%90%E8%AF%A6%E8%A7%A3"><span class="nav-number">2.2.2.1.</span> <span class="nav-text">例子详解</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E5%AE%9A%E4%B9%89"><span class="nav-number">2.2.2.1.1.</span> <span class="nav-text">相关知识定义</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BE%8B%E5%AD%90"><span class="nav-number">2.2.2.2.</span> <span class="nav-text">例子</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89"><span class="nav-number">2.2.2.2.1.</span> <span class="nav-text">定义</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BE%8B%E5%AD%90-1"><span class="nav-number">2.2.2.3.</span> <span class="nav-text">例子</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E9%87%8A%E4%B8%8E%E4%BE%8B%E5%AD%90"><span class="nav-number">2.2.2.4.</span> <span class="nav-text">解释与例子</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89-1"><span class="nav-number">2.2.2.4.1.</span> <span class="nav-text">定义</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BA%94%E7%94%A8"><span class="nav-number">2.2.2.5.</span> <span class="nav-text">应用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B"><span class="nav-number">2.2.2.6.</span> <span class="nav-text">实例</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Graph-Neural-Networks"><span class="nav-number">2.3.</span> <span class="nav-text">Graph Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Foundations"><span class="nav-number">2.3.1.</span> <span class="nav-text">Foundations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Methodologies"><span class="nav-number">2.3.2.</span> <span class="nav-text">Methodologies</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86"><span class="nav-number">2.3.2.0.0.1.</span> <span class="nav-text">基础数学知识</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89-2"><span class="nav-number">2.3.2.0.1.</span> <span class="nav-text">定义</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%86%8D%E6%AC%A1%E5%AD%A6%E4%B9%A0"><span class="nav-number">2.3.2.0.2.</span> <span class="nav-text">再次学习</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0"><span class="nav-number">2.3.2.0.3.</span> <span class="nav-text">学习</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0-1"><span class="nav-number">2.3.2.0.4.</span> <span class="nav-text">学习</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-number">2.3.2.0.4.1.</span> <span class="nav-text">简介</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#easy"><span class="nav-number">2.3.2.1.</span> <span class="nav-text">easy</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BF%9B%E4%B8%80%E6%AD%A5"><span class="nav-number">2.3.2.1.1.</span> <span class="nav-text">进一步</span></a></li></ol></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Haoran Li</p>
  <div class="site-description" itemprop="description">Blog of Whyynnot</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Haoran Li</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'c2AnFNAFnFrReTpruCM2RWMV-gzGzoHsz',
      appKey     : 'rMWSQ6KHYU6DHK01uJS0Mvmg',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
